# è°ƒç”¨é“¾è·¯åˆ†æ - 06 LLMå±‚

## 6. LLMå±‚ï¼šæ¨¡å‹è°ƒç”¨

### å…¥å£å‡½æ•°
```
ğŸ“ backend/daoyoucode/agents/llm/clients/unified.py :: UnifiedLLMClient.chat()
```

### è°ƒç”¨æµç¨‹

#### 6.1 LLMå®¢æˆ·ç«¯ç®¡ç†å™¨

**æ–‡ä»¶**: `backend/daoyoucode/agents/llm/client_manager.py`

**ä»£ç **:
```python
class LLMClientManager:
    """LLMå®¢æˆ·ç«¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.http_client = httpx.AsyncClient(
            timeout=60.0,
            limits=httpx.Limits(max_connections=10)
        )
        self.provider_configs = {}
        self.clients = {}
    
    def add_provider(self, name: str, config: Dict):
        """æ·»åŠ æä¾›å•†é…ç½®"""
        self.provider_configs[name] = config
    
    def get_client(self, model: str) -> BaseLLMClient:
        """è·å–å®¢æˆ·ç«¯ï¼ˆæ ¹æ®æ¨¡å‹åç§°ï¼‰"""
        # æŸ¥æ‰¾æ¨¡å‹æ‰€å±çš„æä¾›å•†
        provider = self._find_provider_for_model(model)
        
        # è·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯
        if provider not in self.clients:
            config = self.provider_configs[provider]
            self.clients[provider] = UnifiedLLMClient(
                http_client=self.http_client,
                api_key=config['api_key'],
                base_url=config['base_url'],
                model=model
            )
        
        return self.clients[provider]
```

**èŒè´£**:
- ç®¡ç†å¤šä¸ªLLMæä¾›å•†
- å…±äº«HTTPè¿æ¥æ± 
- æ ¹æ®æ¨¡å‹åç§°è·¯ç”±åˆ°æ­£ç¡®çš„å®¢æˆ·ç«¯

---

#### 6.2 é…ç½®åŠ è½½

**æ–‡ä»¶**: `backend/daoyoucode/agents/llm/config_loader.py`

**ä»£ç **:
```python
def auto_configure(client_manager: LLMClientManager):
    """è‡ªåŠ¨é…ç½®LLMå®¢æˆ·ç«¯"""
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    config = load_llm_config()
    
    # 2. æ³¨å†Œæä¾›å•†
    for provider_name, provider_config in config.get('providers', {}).items():
        if provider_config.get('enabled', True):
            client_manager.add_provider(provider_name, provider_config)
```

**é…ç½®æ–‡ä»¶**: `backend/config/llm_config.yaml`

**å†…å®¹**:
```yaml
providers:
  qwen:
    enabled: true
    api_key: ${DASHSCOPE_API_KEY}
    base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    models:
      - qwen-max
      - qwen-plus
      - qwen-turbo
  
  deepseek:
    enabled: false
    api_key: ${DEEPSEEK_API_KEY}
    base_url: "https://api.deepseek.com/v1"
    models:
      - deepseek-chat
      - deepseek-coder
```

---

#### 6.3 ç»Ÿä¸€LLMå®¢æˆ·ç«¯

**æ–‡ä»¶**: `backend/daoyoucode/agents/llm/clients/unified.py`

**ä»£ç **:
```python
class UnifiedLLMClient(BaseLLMClient):
    """ç»Ÿä¸€LLMå®¢æˆ·ç«¯ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰"""
    
    async def chat(self, request: LLMRequest) -> LLMResponse:
        """åŒæ­¥å¯¹è¯"""
        start_time = time.time()
        
        # æ”¯æŒå¤šè½®å¯¹è¯
        if hasattr(request, 'messages') and request.messages:
            messages = request.messages
        else:
            messages = [{"role": "user", "content": request.prompt}]
        
        try:
            # æ„å»ºè¯·æ±‚payload
            payload = {
                "model": request.model,
                "messages": messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
            }
            
            # æ·»åŠ Function Callingæ”¯æŒ
            if hasattr(request, 'functions') and request.functions:
                payload["functions"] = request.functions
                if hasattr(request, 'function_call'):
                    payload["function_call"] = request.function_call
            
            # å‘é€HTTPè¯·æ±‚
            response = await self.http_client.post(
                f"{self.base_url}/chat/completions",
                headers=self._get_headers(),
                json=payload,
                timeout=60.0
            )
            response.raise_for_status()
            data = response.json()
            
            latency = time.time() - start_time
            
            # è§£æå“åº”
            message = data["choices"][0]["message"]
            function_call = message.get("function_call")
            
            return LLMResponse(
                content=message.get("content", ""),
                model=request.model,
                tokens_used=data["usage"]["total_tokens"],
                cost=self._calculate_cost(data["usage"], request.model),
                latency=latency,
                metadata={
                    "prompt_tokens": data["usage"]["prompt_tokens"],
                    "completion_tokens": data["usage"]["completion_tokens"],
                    "function_call": function_call
                }
            )
        
        except httpx.TimeoutException as e:
            raise LLMTimeoutError(f"è¯·æ±‚è¶…æ—¶: {e}")
        except httpx.HTTPError as e:
            raise LLMConnectionError(f"è¿æ¥é”™è¯¯: {e}")
```

**å…³é”®ç‰¹æ€§**:
- OpenAIå…¼å®¹æ ¼å¼
- æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆmessageså‚æ•°ï¼‰
- æ”¯æŒFunction Calling
- è‡ªåŠ¨è®¡ç®—æˆæœ¬
- é”™è¯¯å¤„ç†

---

#### 6.4 è¯·æ±‚/å“åº”æ•°æ®ç»“æ„

**LLMRequest**:
```python
@dataclass
class LLMRequest:
    prompt: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 4000
    messages: Optional[List[Dict]] = None  # å¤šè½®å¯¹è¯
    functions: Optional[List[Dict]] = None  # Function schemas
    function_call: Optional[str] = None  # "auto" | "none"
```

**LLMResponse**:
```python
@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    cost: float
    latency: float
    metadata: Dict[str, Any] = field(default_factory=dict)
```

---

#### 6.5 Function Callingæ ¼å¼

**è¯·æ±‚æ ¼å¼**:
```json
{
  "model": "qwen-max",
  "messages": [
    {"role": "user", "content": "Agentç³»ç»Ÿæ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ"}
  ],
  "functions": [
    {
      "name": "repo_map",
      "description": "ç”Ÿæˆä»£ç ä»“åº“åœ°å›¾",
      "parameters": {
        "type": "object",
        "properties": {
          "repo_path": {
            "type": "string",
            "description": "ä»“åº“æ ¹ç›®å½•è·¯å¾„"
          },
          "max_tokens": {
            "type": "integer",
            "description": "æœ€å¤§tokenæ•°é‡",
            "default": 2000
          }
        },
        "required": ["repo_path"]
      }
    }
  ],
  "temperature": 0.7
}
```

**å“åº”æ ¼å¼ï¼ˆæœ‰function_callï¼‰**:
```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": null,
        "function_call": {
          "name": "repo_map",
          "arguments": "{\"repo_path\": \"backend\", \"max_tokens\": 2000}"
        }
      }
    }
  ],
  "usage": {
    "prompt_tokens": 1234,
    "completion_tokens": 56,
    "total_tokens": 1290
  }
}
```

**å“åº”æ ¼å¼ï¼ˆæ— function_callï¼‰**:
```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "Agentç³»ç»Ÿä¸»è¦åœ¨backend/daoyoucode/agents/ç›®å½•ä¸‹å®ç°..."
      }
    }
  ],
  "usage": {
    "prompt_tokens": 1234,
    "completion_tokens": 567,
    "total_tokens": 1801
  }
}
```

---

#### 6.6 æˆæœ¬è®¡ç®—

**ä»£ç **:
```python
# æ¨¡å‹å®šä»·ï¼ˆæ¯1000 tokensï¼Œå•ä½ï¼šå…ƒï¼‰
PRICING = {
    "qwen-max": {"input": 0.02, "output": 0.06},
    "qwen-plus": {"input": 0.004, "output": 0.012},
    "qwen-turbo": {"input": 0.002, "output": 0.006},
    "default": {"input": 0.01, "output": 0.03},
}

def _calculate_cost(self, usage: dict, model: str) -> float:
    """è®¡ç®—æˆæœ¬"""
    input_tokens = usage.get("prompt_tokens", 0)
    output_tokens = usage.get("completion_tokens", 0)
    
    pricing = self.PRICING.get(model, self.PRICING["default"])
    
    cost = (
        input_tokens * pricing["input"] +
        output_tokens * pricing["output"]
    ) / 1000
    
    return cost
```

**ç¤ºä¾‹**:
```
è¾“å…¥: 1000 tokens
è¾“å‡º: 500 tokens
æ¨¡å‹: qwen-plus

æˆæœ¬ = (1000 * 0.004 + 500 * 0.012) / 1000
     = (4 + 6) / 1000
     = 0.01 å…ƒ
```

---

### å…³é”®æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | èŒè´£ | å…³é”®ç±»/å‡½æ•° |
|------|------|------------|
| `llm/client_manager.py` | å®¢æˆ·ç«¯ç®¡ç† | `LLMClientManager` |
| `llm/config_loader.py` | é…ç½®åŠ è½½ | `auto_configure()` |
| `llm/clients/unified.py` | ç»Ÿä¸€å®¢æˆ·ç«¯ | `UnifiedLLMClient` |
| `llm/base.py` | åŸºç¡€å®šä¹‰ | `LLMRequest`, `LLMResponse` |
| `config/llm_config.yaml` | LLMé…ç½® | YAMLé…ç½® |

---

### ä¾èµ–å…³ç³»

```
client_manager.py
    â†“
â”œâ”€ config_loader.py
â”‚   â””â”€ llm_config.yaml
â”œâ”€ clients/unified.py
â”‚   â”œâ”€ httpx (HTTPå®¢æˆ·ç«¯)
â”‚   â””â”€ base.py (æ•°æ®ç»“æ„)
â””â”€ exceptions.py (å¼‚å¸¸å®šä¹‰)
```

---

### ä¸‹ä¸€æ­¥

LLMå±‚å®Œæˆåï¼Œè¿”å›åˆ° **Agentå±‚**ï¼Œæˆ–ç»§ç»­åˆ° **Memoryå±‚**

â†’ ç»§ç»­é˜…è¯» `CALL_CHAIN_07_MEMORY.md`
