"""
AgentåŸºç±»å’Œæ³¨å†Œè¡¨

Agentæ˜¯æ‰§è¡Œä»»åŠ¡çš„ä¸“å®¶
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from abc import ABC
import logging

logger = logging.getLogger(__name__)


@dataclass
class AgentConfig:
    """Agenté…ç½®"""
    name: str
    description: str
    model: str
    temperature: float = 0.7
    system_prompt: str = ""


@dataclass
class AgentResult:
    """Agentæ‰§è¡Œç»“æœ"""
    success: bool
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    tools_used: list = field(default_factory=list)
    tokens_used: int = 0
    cost: float = 0.0


class BaseAgent(ABC):
    """AgentåŸºç±»"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.name = config.name
        self.logger = logging.getLogger(f"agent.{self.name}")
        
        # ç¡®ä¿å·¥å…·æ³¨å†Œè¡¨å·²åˆå§‹åŒ–ï¼ˆåŒä¿é™©ï¼‰
        from ..tools import get_tool_registry
        self._tool_registry = get_tool_registry()
        self.logger.debug(f"å·¥å…·æ³¨å†Œè¡¨å·²å°±ç»ª: {len(self._tool_registry.list_tools())} ä¸ªå·¥å…·")
        
        # æ¥å…¥è®°å¿†æ¨¡å—ï¼ˆå•ä¾‹ï¼Œä¸ä¼šé‡å¤åŠ è½½ï¼‰
        from ..memory import get_memory_manager
        self.memory = get_memory_manager()
        
        # æ¥å…¥å·¥å…·åå¤„ç†å™¨
        from ..tools.postprocessor import get_tool_postprocessor
        self.tool_postprocessor = get_tool_postprocessor()
        self.logger.debug("å·¥å…·åå¤„ç†å™¨å·²å°±ç»ª")
        
        # ç”¨æˆ·ç”»åƒç¼“å­˜ï¼ˆæŒ‰éœ€åŠ è½½ï¼Œé¿å…æ¯è½®éƒ½è¯»å–ï¼‰
        self._user_profile_cache: Dict[str, Dict[str, Any]] = {}
        
        # ç”¨æˆ·ç”»åƒæ£€æŸ¥æ—¶é—´ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹æ£€æŸ¥ï¼‰
        # æ ¼å¼ï¼š{user_id: last_check_timestamp}
        self._profile_check_cache: Dict[str, float] = {}
    
    def get_user_profile(self, user_id: str, force_reload: bool = False) -> Optional[Dict[str, Any]]:
        """
        è·å–ç”¨æˆ·ç”»åƒï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
        
        Returns:
            ç”¨æˆ·ç”»åƒå­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if force_reload or user_id not in self._user_profile_cache:
            profile = self.memory.long_term_memory.get_user_profile(user_id)
            if profile:
                self._user_profile_cache[user_id] = profile
                self.logger.debug(f"åŠ è½½ç”¨æˆ·ç”»åƒ: {user_id}")
        
        return self._user_profile_cache.get(user_id)
    
    async def _check_and_update_profile(self, user_id: str, session_id: str):
        """
        æ£€æŸ¥å¹¶æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆå¸¦æ—¶é—´çª—å£ä¼˜åŒ–ï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        - 1å°æ—¶å†…åªæ£€æŸ¥ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹çš„æ–‡ä»¶I/Oå’Œè®¡ç®—
        - å‡å°‘90%çš„ä¸å¿…è¦æ£€æŸ¥
        
        Args:
            user_id: ç”¨æˆ·ID
            session_id: å½“å‰ä¼šè¯ID
        """
        import time
        
        try:
            # æ£€æŸ¥æ—¶é—´çª—å£ï¼ˆ1å°æ—¶ = 3600ç§’ï¼‰
            CHECK_INTERVAL = 3600
            current_time = time.time()
            last_check = self._profile_check_cache.get(user_id)
            
            if last_check and (current_time - last_check) < CHECK_INTERVAL:
                # 1å°æ—¶å†…å·²ç»æ£€æŸ¥è¿‡ï¼Œè·³è¿‡
                self.logger.debug(
                    f"è·³è¿‡ç”»åƒæ£€æŸ¥: user_id={user_id}, "
                    f"è·ä¸Šæ¬¡æ£€æŸ¥ {int(current_time - last_check)}ç§’"
                )
                return
            
            # æ›´æ–°æ£€æŸ¥æ—¶é—´
            self._profile_check_cache[user_id] = current_time
            
            # è·å–ç”¨æˆ·çš„æ€»å¯¹è¯æ•°
            tasks = self.memory.get_task_history(user_id, limit=1000)
            total_conversations = len(tasks)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            should_update = self.memory.long_term_memory.should_update_profile(
                user_id, total_conversations
            )
            
            if should_update:
                self.logger.info(f"ğŸ”„ è§¦å‘ç”¨æˆ·ç”»åƒæ›´æ–°: user_id={user_id}, conversations={total_conversations}")
                
                # å¼‚æ­¥æ›´æ–°ï¼ˆä¸é˜»å¡å½“å‰è¯·æ±‚ï¼‰
                # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ”¾åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—
                await self._update_user_profile_async(user_id)
        
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥ç”¨æˆ·ç”»åƒæ›´æ–°å¤±è´¥: {e}")
    
    async def _update_user_profile_async(self, user_id: str):
        """
        å¼‚æ­¥æ›´æ–°ç”¨æˆ·ç”»åƒ
        
        Args:
            user_id: ç”¨æˆ·ID
        """
        try:
            # æ”¶é›†ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯
            all_sessions = self.memory.get_user_sessions(user_id)
            
            if not all_sessions:
                self.logger.warning(f"ç”¨æˆ· {user_id} æ²¡æœ‰ä¼šè¯è®°å½•ï¼Œè·³è¿‡ç”»åƒæ›´æ–°")
                return
            
            # è·å–LLMå®¢æˆ·ç«¯
            from ..llm import get_client_manager
            client_manager = get_client_manager()
            llm_client = client_manager.get_client(self.config.model)
            
            # æ„å»ºç”¨æˆ·ç”»åƒ
            profile = await self.memory.long_term_memory.build_user_profile(
                user_id=user_id,
                all_sessions=all_sessions,
                llm_client=llm_client
            )
            
            # æ¸…é™¤ç¼“å­˜ï¼Œä¸‹æ¬¡è®¿é—®æ—¶ä¼šé‡æ–°åŠ è½½
            if user_id in self._user_profile_cache:
                del self._user_profile_cache[user_id]
            
            self.logger.info(
                f"âœ… ç”¨æˆ·ç”»åƒå·²æ›´æ–°: user_id={user_id}, "
                f"sessions={len(all_sessions)}, "
                f"topics={len(profile.get('common_topics', []))}"
            )
        
        except Exception as e:
            self.logger.error(f"æ›´æ–°ç”¨æˆ·ç”»åƒå¤±è´¥: {e}", exc_info=True)
    
    async def execute_stream(
        self,
        prompt_source: Dict[str, Any],
        user_input: str,
        context: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        tools: Optional[List[str]] = None,
        max_tool_iterations: int = 15  # ğŸ†• å¢åŠ åˆ° 15 æ¬¡
    ):
        """
        æµå¼æ‰§è¡Œä»»åŠ¡ï¼ˆyieldæ¯ä¸ªtokenï¼‰
        
        æ³¨æ„ï¼šæµå¼æ¨¡å¼ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œå¦‚æœæä¾›äº†toolså‚æ•°ä¼šè‡ªåŠ¨é™çº§åˆ°æ™®é€šæ¨¡å¼
        
        Args:
            prompt_source: Promptæ¥æº
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡
            llm_config: LLMé…ç½®
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆæµå¼æ¨¡å¼ä¸‹ä¼šè¢«å¿½ç•¥ï¼‰
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°ï¼ˆæµå¼æ¨¡å¼ä¸‹æ— æ•ˆï¼‰
        
        Yields:
            Dict: æµå¼äº‹ä»¶
                - {'type': 'token', 'content': str} - æ–‡æœ¬token
                - {'type': 'metadata', 'data': dict} - å…ƒæ•°æ®ï¼ˆå¼€å§‹/ç»“æŸï¼‰
                - {'type': 'error', 'error': str} - é”™è¯¯ä¿¡æ¯
        """
        if context is None:
            context = {}
        
        # æå–session_idå’Œuser_id
        session_id = context.get('session_id', 'default')
        user_id = context.get('user_id')
        if not user_id:
            from ..memory import get_current_user_id
            user_id = get_current_user_id()
        context['user_id'] = user_id
        
        # æµå¼æ¨¡å¼ä¸æ”¯æŒå·¥å…·è°ƒç”¨
        if tools:
            self.logger.warning("æµå¼æ¨¡å¼ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œè‡ªåŠ¨é™çº§åˆ°æ™®é€šæ¨¡å¼")
            result = await self.execute(
                prompt_source, user_input, context, llm_config, tools, max_tool_iterations
            )
            yield {'type': 'token', 'content': result.content}
            yield {'type': 'metadata', 'data': {'success': result.success, 'done': True}}
            return
        
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield {'type': 'metadata', 'data': {'status': 'started'}}
            
            # ========== 1. è·å–è®°å¿†ï¼ˆæ™ºèƒ½åŠ è½½ï¼‰==========
            is_followup = False
            confidence = 0.0
            if session_id != 'default':
                is_followup, confidence, reason = await self.memory.is_followup(
                    session_id, user_input
                )
            
            memory_context = await self.memory.load_context_smart(
                session_id=session_id,
                user_id=user_id,
                user_input=user_input,
                is_followup=is_followup,
                confidence=confidence
            )
            
            history = memory_context.get('history', [])
            if history:
                context['conversation_history'] = history
            
            summary = memory_context.get('summary')
            if summary:
                context['conversation_summary'] = summary
            
            prefs = self.memory.get_preferences(user_id)
            if prefs:
                context['user_preferences'] = prefs
            
            task_history = self.memory.get_task_history(user_id, limit=5)
            if task_history:
                context['recent_tasks'] = task_history
            
            # ========== 2. åŠ è½½å’Œæ¸²æŸ“Prompt ==========
            prompt = await self._load_prompt(prompt_source, context)
            full_prompt = self._render_prompt(prompt, user_input, context)
            
            # ========== 3. æµå¼è°ƒç”¨LLM ==========
            response_content = ""
            async for token in self._stream_llm(full_prompt, llm_config):
                response_content += token
                yield {'type': 'token', 'content': token}
            
            # ========== 4. ä¿å­˜åˆ°è®°å¿† ==========
            self.memory.add_conversation(
                session_id,
                user_input,
                response_content,
                metadata={'agent': self.name, 'stream': True},
                user_id=user_id
            )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
            history_after = self.memory.get_conversation_history(session_id)
            current_round = len(history_after)
            
            if self.memory.long_term_memory.should_generate_summary(session_id, current_round):
                try:
                    from ..llm import get_client_manager
                    client_manager = get_client_manager()
                    llm_client = client_manager.get_client(
                        llm_config.get('model') if llm_config else self.config.model
                    )
                    summary = await self.memory.long_term_memory.generate_summary(
                        session_id, history_after, llm_client
                    )
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            
            # ä¿å­˜ä»»åŠ¡
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],
                'result': response_content[:200],
                'success': True,
                'stream': True
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç”¨æˆ·ç”»åƒ
            await self._check_and_update_profile(user_id, session_id)
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield {'type': 'metadata', 'data': {'status': 'completed', 'done': True}}
        
        except Exception as e:
            self.logger.error(f"æµå¼æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # å¤±è´¥ä¹Ÿè®°å½•åˆ°ä»»åŠ¡å†å²
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],
                'error': str(e)[:200],
                'success': False
            })
            
            yield {'type': 'error', 'error': str(e)}
            yield {'type': 'metadata', 'data': {'status': 'failed', 'done': True}}
    
    async def execute(
        self,
        prompt_source: Dict[str, Any],
        user_input: str,
        context: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        tools: Optional[List[str]] = None,
        max_tool_iterations: int = 15,  # ğŸ†• å¢åŠ åˆ° 15 æ¬¡
        enable_streaming: bool = False  # ğŸ†• æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    ):
        """
        æ‰§è¡Œä»»åŠ¡
        
        Args:
            prompt_source: Promptæ¥æº
                - {'file': 'path/to/prompt.md'}
                - {'inline': 'prompt text'}
                - {'use_agent_default': True}
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡
            llm_config: LLMé…ç½®
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆå·¥å…·åç§°ï¼‰
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆæœ€ç»ˆå›å¤é˜¶æ®µï¼‰
        
        Returns:
            å¦‚æœ enable_streaming=True:
                AsyncGenerator[Dict[str, Any], None] - æµå¼äº‹ä»¶
                    - {'type': 'token', 'content': str} - æ–‡æœ¬token
                    - {'type': 'result', 'result': AgentResult} - æœ€ç»ˆç»“æœ
            å¦åˆ™:
                AgentResult - æ‰§è¡Œç»“æœ
        """
        if context is None:
            context = {}
        
        # æå–session_idå’Œuser_id
        session_id = context.get('session_id', 'default')
        
        # è·å–user_idï¼ˆä¼˜å…ˆçº§ï¼šcontext > user_manager > session_idï¼‰
        user_id = context.get('user_id')
        if not user_id:
            # ä»ç”¨æˆ·ç®¡ç†å™¨è·å–
            from ..memory import get_current_user_id
            user_id = get_current_user_id()
        
        # ç¡®ä¿user_idåœ¨contextä¸­ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
        context['user_id'] = user_id
        
        tools_used = []
        
        try:
            # ========== 1. è·å–è®°å¿†ï¼ˆæ™ºèƒ½åŠ è½½ï¼‰==========
            
            # 1.1 åˆ¤æ–­æ˜¯å¦ä¸ºè¿½é—®
            is_followup = False
            confidence = 0.0
            if session_id != 'default':
                is_followup, confidence, reason = await self.memory.is_followup(
                    session_id, user_input
                )
                self.logger.debug(f"è¿½é—®åˆ¤æ–­: {is_followup} (ç½®ä¿¡åº¦: {confidence:.2f}, åŸå› : {reason})")
            
            # 1.2 æ™ºèƒ½åŠ è½½å¯¹è¯å†å²ï¼ˆLLMå±‚è®°å¿†ï¼‰
            memory_context = await self.memory.load_context_smart(
                session_id=session_id,
                user_id=user_id,
                user_input=user_input,
                is_followup=is_followup,
                confidence=confidence
            )
            
            # æå–åŠ è½½çš„å†å²
            history = memory_context.get('history', [])
            if history:
                context['conversation_history'] = history
                self.logger.info(
                    f"ğŸ“š æ™ºèƒ½åŠ è½½: ç­–ç•¥={memory_context['strategy']}, "
                    f"å†å²={len(history)}è½®, æˆæœ¬={memory_context['cost']}, "
                    f"ç­›é€‰={'æ˜¯' if memory_context.get('filtered') else 'å¦'}"
                )
            
            # æå–æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            summary = memory_context.get('summary')
            if summary:
                context['conversation_summary'] = summary
                self.logger.info(f"ğŸ“ åŠ è½½æ‘˜è¦: {len(summary)}å­—ç¬¦")
            
            # 1.3 ç”¨æˆ·åå¥½ï¼ˆAgentå±‚è®°å¿†ï¼Œè½»é‡çº§ï¼‰
            prefs = self.memory.get_preferences(user_id)
            if prefs:
                context['user_preferences'] = prefs
                self.logger.debug(f"åŠ è½½äº†ç”¨æˆ·åå¥½: {list(prefs.keys())}")
            
            # 1.4 ä»»åŠ¡å†å²ï¼ˆAgentå±‚è®°å¿†ï¼Œæœ€è¿‘5ä¸ªï¼‰
            task_history = self.memory.get_task_history(user_id, limit=5)
            if task_history:
                context['recent_tasks'] = task_history
                self.logger.debug(f"åŠ è½½äº† {len(task_history)} ä¸ªæœ€è¿‘ä»»åŠ¡")
            
            # ========== 2. åŠ è½½Prompt ==========
            prompt = await self._load_prompt(prompt_source, context)
            
            # ========== 3. æ¸²æŸ“Prompt ==========
            full_prompt = self._render_prompt(prompt, user_input, context)
            
            # æ·»åŠ å·¥å…·ä½¿ç”¨è§„åˆ™ï¼ˆå¯é…ç½®ï¼šcontext['tool_rules'] è¦†ç›–é»˜è®¤ï¼ŒSkill/ç¼–æ’å™¨å¯ä¼ å…¥ï¼‰
            if tools:
                default_tool_rules = """âš ï¸ å·¥å…·ä½¿ç”¨è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š

1. è·¯å¾„å‚æ•°ä½¿ç”¨ '.' è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•
   - âœ… æ­£ç¡®ï¼šrepo_map(repo_path=".")
   - âŒ é”™è¯¯ï¼šrepo_map(repo_path="./your-repo-path")
   - âŒ é”™è¯¯ï¼šrepo_map(repo_path="/path/to/repo")

2. æ–‡ä»¶è·¯å¾„ï¼šç›¸å¯¹**é¡¹ç›®æ ¹**ï¼ˆå½“å‰å·¥ä½œç›®å½•å³é¡¹ç›®æ ¹ï¼‰
   - æœ¬ä»“åº“åç«¯ä»£ç åœ¨ backend/daoyoucode/ ä¸‹ï¼Œç¼–æ’å™¨åœ¨ backend/daoyoucode/agents/orchestrators/
   - âœ… æ­£ç¡®ï¼šread_file(file_path="backend/daoyoucode/agents/orchestrators/multi_agent.py")
   - âŒ é”™è¯¯ï¼šread_file(file_path="daoyoucode/orchestrators/multi_agent.py")  ï¼ˆä¼šè§£æåˆ°é”™è¯¯è·¯å¾„ï¼‰

3. æœç´¢ç›®å½•ä½¿ç”¨ '.' æˆ–çœç•¥
   - âœ… æ­£ç¡®ï¼štext_search(query="example", directory=".")
   - âŒ é”™è¯¯ï¼štext_search(query="example", directory="./src")

4. ç»†ç²’åº¦ç¼–è¾‘ä¸éªŒè¯
   - å¯ç”¨ apply_patch(diff="...") æäº¤ unified diffï¼Œä¾¿äºå®¡è®¡å’Œå›æ»šã€‚
   - ç¼–è¾‘åå»ºè®®è°ƒç”¨ run_lint æˆ– run_test éªŒè¯ï¼Œå†æ ¹æ®è¾“å‡ºä¿®å¤ã€‚

5. å•æ–‡ä»¶ç¬¦å·ï¼ˆAST æ·±åº¦ï¼‰
   - å·²çŸ¥æ–‡ä»¶æ—¶å¯è°ƒç”¨ get_file_symbols(file_path) è·å–ç±»/å‡½æ•°/æ–¹æ³•åŠè¡Œå·ã€‚

6. ä¸è¦é‡å¤è°ƒç”¨
   - åŒä¸€è½®å¯¹è¯ä¸­ä¸è¦ç”¨ç›¸åŒå‚æ•°é‡å¤è°ƒç”¨åŒä¸€å·¥å…·ï¼ˆå¦‚å¤šæ¬¡ repo_map(repo_path=".")ï¼‰ï¼›è‹¥å·²è·å¾—è¯¥å·¥å…·ç»“æœï¼Œè¯·ç›´æ¥åŸºäºç»“æœå›ç­”ã€‚

è®°ä½ï¼šå½“å‰å·¥ä½œç›®å½•=é¡¹ç›®æ ¹ï¼›ä¸è¦ç”¨ daoyoucode/ ä½œä¸ºè·¯å¾„å‰ç¼€ï¼Œåç«¯ä»£ç åœ¨ backend/daoyoucode/ ä¸‹ã€‚

---

"""
                tool_rules = context.get('tool_rules') or default_tool_rules
                full_prompt = (tool_rules if isinstance(tool_rules, str) else default_tool_rules) + full_prompt
            
            # ========== 4. è°ƒç”¨LLM ==========
            if tools:
                # æ„å»ºåˆå§‹æ¶ˆæ¯ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰
                initial_messages = []
                # å†å²è½®æ•°å¯é…ç½®ï¼šcontext['max_history_rounds']ï¼Œé»˜è®¤ 5
                max_history_rounds = context.get('max_history_rounds', 5)
                if history:
                    # å¦‚æœå†å²è¶…è¿‡é™åˆ¶ï¼Œåªä¿ç•™æœ€è¿‘çš„Nè½®
                    if len(history) > max_history_rounds:
                        truncated_count = len(history) - max_history_rounds
                        history = history[-max_history_rounds:]
                        self.logger.info(
                            f"ğŸ“‰ å·¥å…·è°ƒç”¨å†å²æˆªæ–­: ä¿ç•™æœ€è¿‘{max_history_rounds}è½®, "
                            f"æˆªæ–­{truncated_count}è½® (èŠ‚çœtoken)"
                        )
                    
                    for h in history:
                        initial_messages.append({
                            "role": "user",
                            "content": h.get('user', '')
                        })
                        initial_messages.append({
                            "role": "assistant",
                            "content": h.get('ai', '')
                        })
                
                # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
                initial_messages.append({
                    "role": "user",
                    "content": full_prompt
                })
                
                result = await self._call_llm_with_tools(
                    initial_messages,  # ä¼ é€’åŒ…å«å†å²çš„æ¶ˆæ¯åˆ—è¡¨
                    tools,
                    llm_config,
                    max_tool_iterations,
                    context=context,  # ä¼ é€’ context
                    history=history,   # ä¼ é€’ history
                    enable_streaming=enable_streaming  # ğŸ†• ä¼ é€’æµå¼æ ‡å¿—
                )
                
                # æ£€æŸ¥æ˜¯å¦è¿”å›ç”Ÿæˆå™¨ï¼ˆæµå¼è¾“å‡ºï¼‰
                import inspect
                if inspect.isasyncgen(result):
                    # æµå¼è¾“å‡ºæ¨¡å¼
                    self.logger.info("ğŸŒŠ è¿›å…¥æµå¼è¾“å‡ºæ¨¡å¼")
                    
                    async def stream_with_memory():
                        response_content = ""
                        final_tools_used = []
                        
                        # é€ä¸ª yield token
                        async for event in result:
                            if event['type'] == 'token':
                                response_content += event['content']
                                yield event
                            elif event['type'] == 'metadata':
                                final_tools_used = event.get('tools_used', [])
                        
                        # æµå¼è¾“å‡ºå®Œæˆåï¼Œä¿å­˜åˆ°è®°å¿†
                        self.memory.add_conversation(
                            session_id,
                            user_input,
                            response_content,
                            metadata={'agent': self.name, 'stream': True},
                            user_id=user_id
                        )
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
                        history_after = self.memory.get_conversation_history(session_id)
                        current_round = len(history_after)
                        
                        if self.memory.long_term_memory.should_generate_summary(session_id, current_round):
                            try:
                                from ..llm import get_client_manager
                                client_manager = get_client_manager()
                                llm_client = client_manager.get_client(
                                    llm_config.get('model') if llm_config else self.config.model
                                )
                                summary = await self.memory.long_term_memory.generate_summary(
                                    session_id, history_after, llm_client
                                )
                            except Exception as e:
                                self.logger.warning(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                        
                        # ä¿å­˜ä»»åŠ¡
                        self.memory.add_task(user_id, {
                            'agent': self.name,
                            'input': user_input[:200],
                            'result': response_content[:200],
                            'success': True,
                            'tools_used': final_tools_used,
                            'stream': True
                        })
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç”¨æˆ·ç”»åƒ
                        await self._check_and_update_profile(user_id, session_id)
                        
                        # å‘é€æœ€ç»ˆç»“æœ
                        yield {
                            'type': 'result',
                            'result': AgentResult(
                                success=True,
                                content=response_content,
                                metadata={'agent': self.name, 'stream': True},
                                tools_used=final_tools_used
                            )
                        }
                    
                    return stream_with_memory()
                else:
                    # éæµå¼æ¨¡å¼ï¼Œresult æ˜¯ tuple
                    response, tools_used = result
            else:
                response = await self._call_llm(full_prompt, llm_config)
                tools_used = []
                tools_used = []
            
            # ========== 5. ä¿å­˜åˆ°è®°å¿† ==========
            
            # 5.1 ä¿å­˜å¯¹è¯ï¼ˆLLMå±‚è®°å¿†ï¼‰
            self.memory.add_conversation(
                session_id,
                user_input,
                response,
                metadata={'agent': self.name},
                user_id=user_id  # ä¼ é€’user_idä»¥ç»´æŠ¤æ˜ å°„
            )
            
            # 5.2 æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
            history_after = self.memory.get_conversation_history(session_id)
            current_round = len(history_after)
            
            if self.memory.long_term_memory.should_generate_summary(session_id, current_round):
                self.logger.info(f"ğŸ”„ è§¦å‘æ‘˜è¦ç”Ÿæˆ: session={session_id}, round={current_round}")
                try:
                    # è·å–LLMå®¢æˆ·ç«¯
                    from ..llm import get_client_manager
                    client_manager = get_client_manager()
                    llm_client = client_manager.get_client(
                        llm_config.get('model') if llm_config else self.config.model
                    )
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary = await self.memory.long_term_memory.generate_summary(
                        session_id, history_after, llm_client
                    )
                    self.logger.info(f"âœ… æ‘˜è¦å·²ç”Ÿæˆ: {len(summary)}å­—ç¬¦")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            
            # 5.3 ä¿å­˜ä»»åŠ¡ï¼ˆAgentå±‚è®°å¿†ï¼‰
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],  # é™åˆ¶é•¿åº¦
                'result': response[:200],   # é™åˆ¶é•¿åº¦
                'success': True,
                'tools_used': tools_used
            })
            
            # 5.4 å­¦ä¹ ç”¨æˆ·åå¥½ï¼ˆAgentå±‚è®°å¿†ï¼‰
            # ä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·ç»å¸¸é—®Pythoné—®é¢˜ï¼Œè®°ä½è¿™ä¸ªåå¥½
            if 'python' in user_input.lower():
                self.memory.remember_preference(user_id, 'preferred_language', 'python')
            elif 'javascript' in user_input.lower() or 'js' in user_input.lower():
                self.memory.remember_preference(user_id, 'preferred_language', 'javascript')
            
            # 5.5 æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç”¨æˆ·ç”»åƒ
            await self._check_and_update_profile(user_id, session_id)
            
            return AgentResult(
                success=True,
                content=response,
                metadata={'agent': self.name},
                tools_used=tools_used
            )
        
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
            from ..llm.exceptions import LLMTimeoutError
            if isinstance(e, LLMTimeoutError):
                # è¶…æ—¶é”™è¯¯ï¼Œæä¾›å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
                from .timeout_recovery import get_user_friendly_timeout_message
                error_message = get_user_friendly_timeout_message(3)  # å‡è®¾å·²é‡è¯•3æ¬¡
                
                self.logger.warning(f"âš ï¸ LLM è¯·æ±‚è¶…æ—¶: {error_message}")
            else:
                error_message = str(e)
            
            # å¤±è´¥ä¹Ÿè®°å½•åˆ°ä»»åŠ¡å†å²
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],
                'error': str(e)[:200],
                'success': False
            })
            
            return AgentResult(
                success=False,
                content="",
                error=error_message,
                tools_used=tools_used
            )
    
    async def _load_prompt(
        self,
        prompt_source: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """åŠ è½½Prompt"""
        if 'file' in prompt_source:
            return await self._load_prompt_from_file(prompt_source['file'])
        elif 'inline' in prompt_source:
            return prompt_source['inline']
        elif prompt_source.get('use_agent_default'):
            return self.config.system_prompt
        else:
            raise ValueError(f"Invalid prompt source: {prompt_source}")
    
    async def _load_prompt_from_file(self, file_path: str) -> str:
        """ä»æ–‡ä»¶åŠ è½½Prompt"""
        from pathlib import Path
        
        path = Path(file_path)
        if not path.is_absolute():
            path = Path.cwd() / path
        
        if not path.exists():
            raise FileNotFoundError(f"Prompt file not found: {file_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _render_prompt(
        self,
        prompt: str,
        user_input: str,
        context: Dict[str, Any]
    ) -> str:
        """æ¸²æŸ“Promptï¼ˆæ”¯æŒJinja2æ¨¡æ¿ï¼‰"""
        try:
            from jinja2 import Template
            template = Template(prompt)
            return template.render(user_input=user_input, **context)
        except Exception as e:
            self.logger.warning(f"Promptæ¸²æŸ“å¤±è´¥: {e}")
            return prompt.replace('{{user_input}}', user_input)
    
    async def _call_llm(
        self,
        prompt: str,
        llm_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """è°ƒç”¨LLM"""
        from ..llm import get_client_manager
        
        client_manager = get_client_manager()
        
        # åˆå¹¶é…ç½®
        model = (llm_config or {}).get('model', self.config.model)
        temperature = (llm_config or {}).get('temperature', self.config.temperature)
        
        # è·å–å®¢æˆ·ç«¯
        client = client_manager.get_client(model=model)
        
        # æ„å»ºè¯·æ±‚
        from ..llm.base import LLMRequest
        request = LLMRequest(
            prompt=prompt,
            model=model,
            temperature=temperature
        )
        
        # è°ƒç”¨
        llm_response = await client.chat(request)
        
        return llm_response.content
    
    async def _stream_llm(
        self,
        prompt: str,
        llm_config: Optional[Dict[str, Any]] = None
    ):
        """
        æµå¼è°ƒç”¨LLM
        
        Args:
            prompt: æç¤ºè¯
            llm_config: LLMé…ç½®
        
        Yields:
            str: æ¯ä¸ªtoken
        """
        from ..llm import get_client_manager
        
        client_manager = get_client_manager()
        
        # åˆå¹¶é…ç½®
        model = (llm_config or {}).get('model', self.config.model)
        temperature = (llm_config or {}).get('temperature', self.config.temperature)
        
        # è·å–å®¢æˆ·ç«¯
        client = client_manager.get_client(model=model)
        
        # æ„å»ºè¯·æ±‚
        from ..llm.base import LLMRequest
        request = LLMRequest(
            prompt=prompt,
            model=model,
            temperature=temperature,
            stream=True
        )
        
        # æµå¼è°ƒç”¨
        async for token in client.stream_chat(request):
            yield token
    
    async def _call_llm_with_tools(
        self,
        initial_messages: List[Dict[str, Any]],  # æ”¹ä¸ºæ¥å—æ¶ˆæ¯åˆ—è¡¨
        tool_names: List[str],
        llm_config: Optional[Dict[str, Any]] = None,
        max_iterations: int = 15,  # ğŸ†• å¢åŠ åˆ° 15 æ¬¡
        context: Optional[Dict[str, Any]] = None,  # æ·»åŠ  context å‚æ•°
        history: Optional[List[Dict[str, Any]]] = None,  # æ·»åŠ  history å‚æ•°
        enable_streaming: bool = True  # ğŸ†• æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    ):
        """
        è°ƒç”¨LLMå¹¶æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        
        Args:
            initial_messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²å¯¹è¯å’Œå½“å‰è¾“å…¥ï¼‰
            tool_names: å¯ç”¨å·¥å…·åç§°åˆ—è¡¨
            llm_config: LLMé…ç½®
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆç”¨äºåå¤„ç†ï¼‰
            history: å¯¹è¯å†å²ï¼ˆç”¨äºåå¤„ç†ï¼‰
            enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆæœ€ç»ˆå›å¤é˜¶æ®µï¼‰
        
        Returns:
            å¦‚æœ enable_streaming=True ä¸”æœ€ç»ˆå›å¤æ— å·¥å…·è°ƒç”¨:
                AsyncGenerator[Dict[str, Any], None] - æµå¼äº‹ä»¶
                    - {'type': 'token', 'content': str} - æ–‡æœ¬token
                    - {'type': 'metadata', 'tools_used': List[str]} - å…ƒæ•°æ®
            å¦åˆ™:
                tuple[str, List[str]] - (æœ€ç»ˆå“åº”, ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨)
        """
        import json
        import time  # æ·»åŠ  time å¯¼å…¥
        
        # ä½¿ç”¨å·²åˆå§‹åŒ–çš„å·¥å…·æ³¨å†Œè¡¨
        tool_registry = self._tool_registry
        tools_used = []
        
        # è°ƒè¯•ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·
        available_tools = tool_registry.list_tools()
        self.logger.info(f"å¯ç”¨å·¥å…·æ•°é‡: {len(available_tools)}")
        self.logger.debug(f"å¯ç”¨å·¥å…·åˆ—è¡¨: {', '.join(sorted(available_tools))}")
        
        # è·å–å·¥å…·çš„Function schemas
        function_schemas = tool_registry.get_function_schemas(tool_names)
        
        if not function_schemas:
            # æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œç›´æ¥è°ƒç”¨
            # ä»initial_messagesä¸­æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            last_user_message = ""
            for msg in reversed(initial_messages):
                if msg['role'] == 'user':
                    last_user_message = msg['content']
                    break
            response = await self._call_llm(last_user_message, llm_config)
            return response, []
        
        # ä½¿ç”¨åˆå§‹æ¶ˆæ¯ä½œä¸ºèµ·ç‚¹
        messages = initial_messages.copy()
        # åŒè½®å»é‡ï¼šç›¸åŒ (å·¥å…·å, å‚æ•°) åœ¨æœ¬è½®å·²æ‰§è¡Œè¿‡åˆ™ç›´æ¥å¤ç”¨ç»“æœï¼Œé¿å…æ¨¡å‹é‡å¤è°ƒç”¨ï¼ˆå¦‚è¿ç»­ 5 æ¬¡ repo_mapï¼‰
        same_call_cache = {}
        
        # å·¥å…·è°ƒç”¨å¾ªç¯
        for iteration in range(max_iterations):
            self.logger.info(f"å·¥å…·è°ƒç”¨è¿­ä»£ {iteration + 1}/{max_iterations}")
            
            # è°ƒç”¨LLMï¼ˆå¸¦å·¥å…·ï¼‰
            response = await self._call_llm_with_functions(
                messages,
                function_schemas,
                llm_config
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰function_call
            function_call = response.get('metadata', {}).get('function_call')
            
            if not function_call:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç»ˆå›å¤
                # å¦‚æœå¯ç”¨æµå¼ä¸”æ˜¯ç¬¬ä¸€è½®ï¼ˆæ²¡æœ‰å·¥å…·è°ƒç”¨è¿‡ï¼‰ï¼Œä½¿ç”¨æµå¼è¾“å‡º
                if enable_streaming and iteration == 0:
                    # ç¬¬ä¸€è½®å°±æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµå¼è¾“å‡º
                    self.logger.info("ğŸŒŠ ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
                    
                    async def stream_generator():
                        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸º prompt
                        last_user_message = ""
                        for msg in reversed(messages):
                            if msg.get('role') == 'user':
                                last_user_message = msg.get('content', '')
                                break
                        
                        # æµå¼è¾“å‡º
                        async for token in self._stream_llm(last_user_message, llm_config):
                            yield {'type': 'token', 'content': token}
                        
                        # å‘é€å…ƒæ•°æ®
                        yield {'type': 'metadata', 'tools_used': tools_used}
                    
                    return stream_generator()
                
                elif enable_streaming and iteration > 0:
                    # æœ‰å·¥å…·è°ƒç”¨åçš„æœ€ç»ˆå›å¤
                    # æ³¨æ„ï¼šresponse å·²ç»åŒ…å«äº† LLM çš„å›å¤ï¼Œä½†æ˜¯éæµå¼çš„
                    # æˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªå›å¤è½¬æ¢ä¸ºæµå¼è¾“å‡º
                    self.logger.info(f"ğŸŒŠ è½¬æ¢ä¸ºæµå¼è¾“å‡ºï¼ˆå·¥å…·è°ƒç”¨åï¼Œè¿­ä»£{iteration+1}æ¬¡ï¼‰")
                    
                    async def stream_generator():
                        # å°†å·²æœ‰çš„ response å†…å®¹é€å­—ç¬¦ yield
                        content = response.get('content', '')
                        
                        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆé€è¯è¾“å‡ºï¼‰
                        import re
                        # æŒ‰è¯åˆ†å‰²ï¼ˆä¸­æ–‡æŒ‰å­—ï¼Œè‹±æ–‡æŒ‰è¯ï¼‰
                        tokens = []
                        current = ""
                        for char in content:
                            current += char
                            # ä¸­æ–‡å­—ç¬¦æˆ–ç©ºæ ¼/æ ‡ç‚¹åè¾“å‡º
                            if ord(char) > 127 or char in ' \n.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š':
                                if current:
                                    tokens.append(current)
                                    current = ""
                        if current:
                            tokens.append(current)
                        
                        # é€ä¸ª yield
                        for token in tokens:
                            yield {'type': 'token', 'content': token}
                        
                        # å‘é€å…ƒæ•°æ®
                        yield {'type': 'metadata', 'tools_used': tools_used}
                    
                    return stream_generator()
                
                else:
                    # ä¸å¯ç”¨æµå¼ï¼Œè¿”å›å®Œæ•´å“åº”
                    return response.get('content', ''), tools_used
            
            # è§£æå·¥å…·è°ƒç”¨
            tool_name = function_call['name']
            
            # ğŸ†• å®‰å…¨è§£æ JSONï¼Œå¤„ç†ç©ºå­—ç¬¦ä¸²å’Œæ ¼å¼é”™è¯¯
            try:
                args_str = function_call['arguments'].strip()
                
                # å°è¯•æå–JSONéƒ¨åˆ†ï¼ˆå¤„ç†LLMæ·»åŠ é¢å¤–æ–‡æœ¬çš„æƒ…å†µï¼‰
                if args_str.startswith('{'):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                    brace_count = 0
                    json_end = -1
                    for i, char in enumerate(args_str):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_end = i + 1
                                break
                    
                    if json_end > 0:
                        args_str = args_str[:json_end]
                
                tool_args = json.loads(args_str)
            except json.JSONDecodeError as e:
                self.logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
                self.logger.error(f"åŸå§‹å†…å®¹: '{function_call['arguments']}'")
                
                # å°è¯•ä¿®å¤å¸¸è§é—®é¢˜
                args_str = function_call['arguments'].strip()
                
                if not args_str or args_str == '':
                    # ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨ç©ºå­—å…¸
                    self.logger.warning("âš ï¸ Function arguments ä¸ºç©ºï¼Œä½¿ç”¨ç©ºå­—å…¸")
                    tool_args = {}
                else:
                    # æ— æ³•ä¿®å¤ï¼Œè·³è¿‡è¿™æ¬¡å·¥å…·è°ƒç”¨
                    self.logger.error(f"âš ï¸ æ— æ³•è§£æ function argumentsï¼Œè·³è¿‡å·¥å…·è°ƒç”¨: {tool_name}")
                    
                    # æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                    messages.append({
                        "role": "assistant",
                        "content": None,
                        "function_call": function_call
                    })
                    messages.append({
                        "role": "function",
                        "name": tool_name,
                        "content": f"Error: æ— æ³•è§£æå·¥å…·å‚æ•°ã€‚è¯·æ£€æŸ¥å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚"
                    })
                    
                    # ç»§ç»­ä¸‹ä¸€æ¬¡è¿­ä»£
                    continue
            
            # åŒè½®å»é‡ï¼šè‹¥æœ¬è½®å›å·²ç”¨ç›¸åŒå‚æ•°è°ƒç”¨è¿‡è¯¥å·¥å…·ï¼Œç›´æ¥å¤ç”¨ä¸Šæ¬¡ç»“æœå¹¶æç¤ºæ¨¡å‹åŸºäºç»“æœå›ç­”
            try:
                args_key = json.dumps(tool_args, sort_keys=True)
            except Exception:
                args_key = str(tool_args)
            cache_key = (tool_name, args_key)
            if cache_key in same_call_cache:
                self.logger.info(f"åŒè½®å»é‡: {tool_name} ä¸ä¸Šæ¬¡å‚æ•°ç›¸åŒï¼Œå¤ç”¨ç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œ")
                tools_used.append(tool_name)
                from ..ui import get_tool_display
                display = get_tool_display()
                display.show_tool_start(tool_name, tool_args)
                display.show_success(tool_name, 0)  # æ˜¾ç¤ºå®Œæˆï¼Œé¿å… UI æ‚¬ç©º
                tool_result_str = same_call_cache[cache_key] + "\n\n[ç³»ç»Ÿæç¤ºï¼šä¸Šæ–‡ä¸ºæœ¬è½®å›è°ƒç›¸åŒå‚æ•°çš„ç»“æœï¼Œè¯·ç›´æ¥åŸºäºè¯¥ç»“æœå›ç­”ï¼Œä¸è¦å†æ¬¡è°ƒç”¨åŒä¸€å·¥å…·ã€‚]"
                messages.append({"role": "assistant", "content": None, "function_call": function_call})
                messages.append({"role": "function", "name": tool_name, "content": tool_result_str})
                continue
            
            self.logger.info(f"è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
            tools_used.append(tool_name)
            
            # ä½¿ç”¨ç¾è§‚çš„UIæ˜¾ç¤º
            from ..ui import get_tool_display
            display = get_tool_display()
            
            # æ˜¾ç¤ºå·¥å…·å¼€å§‹
            display.show_tool_start(tool_name, tool_args)
            
            # æ‰§è¡Œå·¥å…·ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
            start_time = time.time()
            try:
                with display.show_progress(tool_name) as progress:
                    task = progress.add_task(f"æ­£åœ¨æ‰§è¡Œ {tool_name}...", total=100)
                    
                    # æ¨¡æ‹Ÿè¿›åº¦
                    progress.update(task, advance=30)
                    tool_result = await tool_registry.execute_tool(tool_name, **tool_args)
                    progress.update(task, advance=70)
                
                duration = time.time() - start_time
                display.show_success(tool_name, duration)
                
                # ========== æ™ºèƒ½åå¤„ç† ==========
                if tool_result.success:
                    # æå–ç”¨æˆ·é—®é¢˜ï¼ˆä»messagesä¸­æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
                    user_query = ""
                    for msg in reversed(messages):
                        if msg.get('role') == 'user':
                            user_query = msg.get('content', '')
                            break
                    
                    # åå¤„ç†
                    if user_query and context:  # ç¡®ä¿ context å­˜åœ¨
                        tool_result = await self.tool_postprocessor.process(
                            tool_name=tool_name,
                            result=tool_result,
                            user_query=user_query,
                            context={
                                'session_id': context.get('session_id') if context else None,
                                'conversation_history': history if history else [],
                            }
                        )
                
                # æå–å®é™…å†…å®¹
                if tool_result.success:
                    tool_result_str = str(tool_result.content) if tool_result.content else "å·¥å…·æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰è¿”å›å†…å®¹"
                    same_call_cache[cache_key] = tool_result_str
                    # æ˜¾ç¤ºç»“æœé¢„è§ˆï¼ˆå¯é€‰ï¼‰
                    # display.show_result_preview(tool_result_str, max_lines=3)
                else:
                    tool_result_str = f"Error: {tool_result.error}"
                    display.show_warning(tool_name, f"å·¥å…·è¿”å›é”™è¯¯: {tool_result.error}")
            except Exception as e:
                duration = time.time() - start_time
                display.show_error(tool_name, e, duration)
                
                tool_result_str = f"Error: {str(e)}"
                self.logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "assistant",
                "content": None,
                "function_call": function_call
            })
            messages.append({
                "role": "function",
                "name": tool_name,
                "content": tool_result_str
            })
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„å“åº”
        self.logger.warning(f"è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°: {max_iterations}")
        
        if enable_streaming:
            # æµå¼è¾“å‡ºæœ€åçš„å“åº”
            self.logger.info("ğŸŒŠ ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰")
            
            async def stream_generator():
                from ..llm import get_client_manager
                from ..llm.base import LLMRequest
                
                client_manager = get_client_manager()
                model = (llm_config or {}).get('model', self.config.model)
                temperature = (llm_config or {}).get('temperature', self.config.temperature)
                client = client_manager.get_client(model=model)
                
                request = LLMRequest(
                    prompt="",
                    model=model,
                    temperature=temperature,
                    stream=True
                )
                request.messages = messages
                
                async for token in client.stream_chat(request):
                    yield {'type': 'token', 'content': token}
                
                yield {'type': 'metadata', 'tools_used': tools_used}
            
            return stream_generator()
        else:
            final_response = await self._call_llm_with_functions(
                messages,
                [],  # ä¸å†æä¾›å·¥å…·
                llm_config
            )
            
            return final_response.get('content', ''), tools_used
    
    async def _call_llm_with_functions(
        self,
        messages: List[Dict[str, Any]],
        functions: List[Dict[str, Any]],
        llm_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMï¼ˆæ”¯æŒFunction Callingï¼‰
        
        Args:
            messages: æ¶ˆæ¯å†å²
            functions: Function schemas
            llm_config: LLMé…ç½®
        
        Returns:
            å“åº”å­—å…¸
        """
        from ..llm import get_client_manager
        from ..llm.base import LLMRequest
        
        client_manager = get_client_manager()
        
        # åˆå¹¶é…ç½®
        model = (llm_config or {}).get('model', self.config.model)
        temperature = (llm_config or {}).get('temperature', self.config.temperature)
        
        # è·å–å®¢æˆ·ç«¯
        client = client_manager.get_client(model=model)
        
        # æ„å»ºè¯·æ±‚ - ä¼ é€’å®Œæ•´çš„æ¶ˆæ¯å†å²
        request = LLMRequest(
            prompt="",  # å½“æœ‰messagesæ—¶ï¼Œpromptå¯ä»¥ä¸ºç©º
            model=model,
            temperature=temperature
        )
        
        # æ·»åŠ å®Œæ•´çš„æ¶ˆæ¯å†å²
        request.messages = messages
        
        # æ·»åŠ functions
        if functions:
            request.functions = functions
        
        # è°ƒç”¨
        response = await client.chat(request)
        
        return {
            'content': response.content,
            'metadata': response.metadata
        }


class AgentRegistry:
    """Agentæ³¨å†Œè¡¨"""
    
    def __init__(self):
        self._agents: Dict[str, BaseAgent] = {}
    
    def register(self, agent: BaseAgent):
        """æ³¨å†ŒAgent"""
        self._agents[agent.name] = agent
        logger.info(f"å·²æ³¨å†ŒAgent: {agent.name}")
    
    def get_agent(self, name: str) -> Optional[BaseAgent]:
        """è·å–Agent"""
        return self._agents.get(name)
    
    def list_agents(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰Agent"""
        return list(self._agents.keys())


# å…¨å±€æ³¨å†Œè¡¨
_agent_registry = AgentRegistry()


def get_agent_registry() -> AgentRegistry:
    """è·å–Agentæ³¨å†Œè¡¨"""
    return _agent_registry


def register_agent(agent: BaseAgent):
    """æ³¨å†ŒAgent"""
    _agent_registry.register(agent)
