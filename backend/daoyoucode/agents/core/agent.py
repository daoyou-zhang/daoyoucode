"""
AgentåŸºç±»å’Œæ³¨å†Œè¡¨

Agentæ˜¯æ‰§è¡Œä»»åŠ¡çš„ä¸“å®¶
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from abc import ABC
import logging

logger = logging.getLogger(__name__)


@dataclass
class AgentConfig:
    """Agenté…ç½®"""
    name: str
    description: str
    model: str
    temperature: float = 0.7
    system_prompt: str = ""


@dataclass
class AgentResult:
    """Agentæ‰§è¡Œç»“æœ"""
    success: bool
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None
    tools_used: list = field(default_factory=list)
    tokens_used: int = 0
    cost: float = 0.0


class BaseAgent(ABC):
    """AgentåŸºç±»"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.name = config.name
        self.logger = logging.getLogger(f"agent.{self.name}")
        
        # æ¥å…¥è®°å¿†æ¨¡å—ï¼ˆå•ä¾‹ï¼Œä¸ä¼šé‡å¤åŠ è½½ï¼‰
        from ..memory import get_memory_manager
        self.memory = get_memory_manager()
    
    async def execute(
        self,
        prompt_source: Dict[str, Any],
        user_input: str,
        context: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        tools: Optional[List[str]] = None,
        max_tool_iterations: int = 5
    ) -> AgentResult:
        """
        æ‰§è¡Œä»»åŠ¡
        
        Args:
            prompt_source: Promptæ¥æº
                - {'file': 'path/to/prompt.md'}
                - {'inline': 'prompt text'}
                - {'use_agent_default': True}
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡
            llm_config: LLMé…ç½®
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆå·¥å…·åç§°ï¼‰
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
        """
        if context is None:
            context = {}
        
        # æå–session_idå’Œuser_id
        session_id = context.get('session_id', 'default')
        user_id = context.get('user_id', session_id)
        
        tools_used = []
        
        try:
            # ========== 1. è·å–è®°å¿† ==========
            
            # 1.1 å¯¹è¯å†å²ï¼ˆLLMå±‚è®°å¿†ï¼‰
            history = self.memory.get_conversation_history(session_id, limit=3)
            if history:
                context['conversation_history'] = history
                self.logger.debug(f"åŠ è½½äº† {len(history)} è½®å¯¹è¯å†å²")
            
            # 1.2 ç”¨æˆ·åå¥½ï¼ˆAgentå±‚è®°å¿†ï¼‰
            prefs = self.memory.get_preferences(user_id)
            if prefs:
                context['user_preferences'] = prefs
                self.logger.debug(f"åŠ è½½äº†ç”¨æˆ·åå¥½: {list(prefs.keys())}")
            
            # 1.3 ä»»åŠ¡å†å²ï¼ˆAgentå±‚è®°å¿†ï¼‰
            task_history = self.memory.get_task_history(user_id, limit=5)
            if task_history:
                context['recent_tasks'] = task_history
                self.logger.debug(f"åŠ è½½äº† {len(task_history)} ä¸ªæœ€è¿‘ä»»åŠ¡")
            
            # ========== 2. åŠ è½½Prompt ==========
            prompt = await self._load_prompt(prompt_source, context)
            
            # ========== 3. æ¸²æŸ“Prompt ==========
            full_prompt = self._render_prompt(prompt, user_input, context)
            
            # ========== 4. è°ƒç”¨LLM ==========
            if tools:
                response, tools_used = await self._call_llm_with_tools(
                    full_prompt,
                    tools,
                    llm_config,
                    max_tool_iterations
                )
            else:
                response = await self._call_llm(full_prompt, llm_config)
            
            # ========== 5. ä¿å­˜åˆ°è®°å¿† ==========
            
            # 5.1 ä¿å­˜å¯¹è¯ï¼ˆLLMå±‚è®°å¿†ï¼‰
            self.memory.add_conversation(
                session_id,
                user_input,
                response,
                metadata={'agent': self.name}
            )
            
            # 5.2 ä¿å­˜ä»»åŠ¡ï¼ˆAgentå±‚è®°å¿†ï¼‰
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],  # é™åˆ¶é•¿åº¦
                'result': response[:200],   # é™åˆ¶é•¿åº¦
                'success': True,
                'tools_used': tools_used
            })
            
            # 5.3 å­¦ä¹ ç”¨æˆ·åå¥½ï¼ˆAgentå±‚è®°å¿†ï¼‰
            # ä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·ç»å¸¸é—®Pythoné—®é¢˜ï¼Œè®°ä½è¿™ä¸ªåå¥½
            if 'python' in user_input.lower():
                self.memory.remember_preference(user_id, 'preferred_language', 'python')
            elif 'javascript' in user_input.lower() or 'js' in user_input.lower():
                self.memory.remember_preference(user_id, 'preferred_language', 'javascript')
            
            return AgentResult(
                success=True,
                content=response,
                metadata={'agent': self.name},
                tools_used=tools_used
            )
        
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # å¤±è´¥ä¹Ÿè®°å½•åˆ°ä»»åŠ¡å†å²
            self.memory.add_task(user_id, {
                'agent': self.name,
                'input': user_input[:200],
                'error': str(e)[:200],
                'success': False
            })
            
            return AgentResult(
                success=False,
                content="",
                error=str(e),
                tools_used=tools_used
            )
    
    async def _load_prompt(
        self,
        prompt_source: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """åŠ è½½Prompt"""
        if 'file' in prompt_source:
            return await self._load_prompt_from_file(prompt_source['file'])
        elif 'inline' in prompt_source:
            return prompt_source['inline']
        elif prompt_source.get('use_agent_default'):
            return self.config.system_prompt
        else:
            raise ValueError(f"Invalid prompt source: {prompt_source}")
    
    async def _load_prompt_from_file(self, file_path: str) -> str:
        """ä»æ–‡ä»¶åŠ è½½Prompt"""
        from pathlib import Path
        
        path = Path(file_path)
        if not path.is_absolute():
            path = Path.cwd() / path
        
        if not path.exists():
            raise FileNotFoundError(f"Prompt file not found: {file_path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _render_prompt(
        self,
        prompt: str,
        user_input: str,
        context: Dict[str, Any]
    ) -> str:
        """æ¸²æŸ“Promptï¼ˆæ”¯æŒJinja2æ¨¡æ¿ï¼‰"""
        try:
            from jinja2 import Template
            template = Template(prompt)
            return template.render(user_input=user_input, **context)
        except Exception as e:
            self.logger.warning(f"Promptæ¸²æŸ“å¤±è´¥: {e}")
            return prompt.replace('{{user_input}}', user_input)
    
    async def _call_llm(
        self,
        prompt: str,
        llm_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """è°ƒç”¨LLM"""
        from ..llm import get_client_manager
        
        client_manager = get_client_manager()
        
        # åˆå¹¶é…ç½®
        model = (llm_config or {}).get('model', self.config.model)
        temperature = (llm_config or {}).get('temperature', self.config.temperature)
        
        # è·å–å®¢æˆ·ç«¯
        client = client_manager.get_client(model=model)
        
        # æ„å»ºè¯·æ±‚
        from ..llm.base import LLMRequest
        request = LLMRequest(
            prompt=prompt,
            model=model,
            temperature=temperature
        )
        
        # è°ƒç”¨
        llm_response = await client.chat(request)
        
        return llm_response.content
    
    async def _call_llm_with_tools(
        self,
        prompt: str,
        tool_names: List[str],
        llm_config: Optional[Dict[str, Any]] = None,
        max_iterations: int = 5
    ) -> tuple[str, List[str]]:
        """
        è°ƒç”¨LLMå¹¶æ”¯æŒå·¥å…·è°ƒç”¨
        
        Args:
            prompt: æç¤ºè¯
            tool_names: å¯ç”¨å·¥å…·åç§°åˆ—è¡¨
            llm_config: LLMé…ç½®
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        
        Returns:
            (æœ€ç»ˆå“åº”, ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨)
        """
        from ...tools import get_tool_registry
        import json
        
        tool_registry = get_tool_registry()
        tools_used = []
        
        # è°ƒè¯•ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·
        available_tools = tool_registry.list_tools()
        self.logger.info(f"å¯ç”¨å·¥å…·æ•°é‡: {len(available_tools)}")
        self.logger.debug(f"å¯ç”¨å·¥å…·åˆ—è¡¨: {', '.join(sorted(available_tools))}")
        
        # è·å–å·¥å…·çš„Function schemas
        function_schemas = tool_registry.get_function_schemas(tool_names)
        
        if not function_schemas:
            # æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œç›´æ¥è°ƒç”¨
            response = await self._call_llm(prompt, llm_config)
            return response, []
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = [{"role": "user", "content": prompt}]
        
        # å·¥å…·è°ƒç”¨å¾ªç¯
        for iteration in range(max_iterations):
            self.logger.info(f"å·¥å…·è°ƒç”¨è¿­ä»£ {iteration + 1}/{max_iterations}")
            
            # è°ƒç”¨LLMï¼ˆå¸¦å·¥å…·ï¼‰
            response = await self._call_llm_with_functions(
                messages,
                function_schemas,
                llm_config
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰function_call
            function_call = response.get('metadata', {}).get('function_call')
            
            if not function_call:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå“åº”
                return response.get('content', ''), tools_used
            
            # è§£æå·¥å…·è°ƒç”¨
            tool_name = function_call['name']
            tool_args = json.loads(function_call['arguments'])
            
            self.logger.info(f"è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
            print(f"\nğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")  # æ·»åŠ æ§åˆ¶å°è¾“å‡º
            print(f"   å‚æ•°: {tool_args}")
            tools_used.append(tool_name)
            
            # æ‰§è¡Œå·¥å…·
            try:
                print(f"   â³ æ­£åœ¨æ‰§è¡Œ...")  # æ·»åŠ è¿›åº¦æç¤º
                tool_result = await tool_registry.execute_tool(tool_name, **tool_args)
                print(f"   âœ“ æ‰§è¡Œå®Œæˆ")  # æ·»åŠ å®Œæˆæç¤º
                tool_result_str = str(tool_result)
            except Exception as e:
                print(f"   âœ— æ‰§è¡Œå¤±è´¥: {e}")  # æ·»åŠ å¤±è´¥æç¤º
                tool_result_str = f"Error: {str(e)}"
                self.logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            
            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "assistant",
                "content": None,
                "function_call": function_call
            })
            messages.append({
                "role": "function",
                "name": tool_name,
                "content": tool_result_str
            })
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„å“åº”
        self.logger.warning(f"è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°: {max_iterations}")
        final_response = await self._call_llm_with_functions(
            messages,
            [],  # ä¸å†æä¾›å·¥å…·
            llm_config
        )
        
        return final_response.get('content', ''), tools_used
    
    async def _call_llm_with_functions(
        self,
        messages: List[Dict[str, Any]],
        functions: List[Dict[str, Any]],
        llm_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨LLMï¼ˆæ”¯æŒFunction Callingï¼‰
        
        Args:
            messages: æ¶ˆæ¯å†å²
            functions: Function schemas
            llm_config: LLMé…ç½®
        
        Returns:
            å“åº”å­—å…¸
        """
        from ..llm import get_client_manager
        from ..llm.base import LLMRequest
        
        client_manager = get_client_manager()
        
        # åˆå¹¶é…ç½®
        model = (llm_config or {}).get('model', self.config.model)
        temperature = (llm_config or {}).get('temperature', self.config.temperature)
        
        # è·å–å®¢æˆ·ç«¯
        client = client_manager.get_client(model=model)
        
        # æ„å»ºè¯·æ±‚ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦æ”¯æŒmessagesï¼‰
        # è¿™é‡Œæš‚æ—¶ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        for msg in reversed(messages):
            if msg['role'] == 'user':
                user_message = msg['content']
                break
        
        request = LLMRequest(
            prompt=user_message,
            model=model,
            temperature=temperature
        )
        
        # æ·»åŠ functions
        if functions:
            request.functions = functions
        
        # è°ƒç”¨
        response = await client.chat(request)
        
        return {
            'content': response.content,
            'metadata': response.metadata
        }


class AgentRegistry:
    """Agentæ³¨å†Œè¡¨"""
    
    def __init__(self):
        self._agents: Dict[str, BaseAgent] = {}
    
    def register(self, agent: BaseAgent):
        """æ³¨å†ŒAgent"""
        self._agents[agent.name] = agent
        logger.info(f"å·²æ³¨å†ŒAgent: {agent.name}")
    
    def get_agent(self, name: str) -> Optional[BaseAgent]:
        """è·å–Agent"""
        return self._agents.get(name)
    
    def list_agents(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰Agent"""
        return list(self._agents.keys())


# å…¨å±€æ³¨å†Œè¡¨
_agent_registry = AgentRegistry()


def get_agent_registry() -> AgentRegistry:
    """è·å–Agentæ³¨å†Œè¡¨"""
    return _agent_registry


def register_agent(agent: BaseAgent):
    """æ³¨å†ŒAgent"""
    _agent_registry.register(agent)
