"""
è¶…æ—¶æ¢å¤ç­–ç•¥

ä¸“é—¨å¤„ç† LLM è¯·æ±‚è¶…æ—¶çš„æ¢å¤æœºåˆ¶
"""

import logging
from typing import Optional, Dict, Any, Callable
import asyncio
from dataclasses import dataclass

from ..llm.exceptions import LLMTimeoutError

logger = logging.getLogger(__name__)


@dataclass
class TimeoutRecoveryConfig:
    """è¶…æ—¶æ¢å¤é…ç½®"""
    max_retries: int = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    initial_timeout: float = 1800.0  # åˆå§‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- 30åˆ†é’Ÿï¼Œæ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨å’Œå¤§è§„æ¨¡æ–‡ä»¶æ“ä½œ
    timeout_multiplier: float = 1.2  # æ¯æ¬¡é‡è¯•è¶…æ—¶æ—¶é—´å€æ•°ï¼ˆé™ä½å€æ•°ï¼Œå› ä¸ºåŸºç¡€æ—¶é—´å·²ç»å¾ˆé•¿ï¼‰
    max_timeout: float = 3600.0  # æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- 1å°æ—¶
    retry_delay: float = 2.0  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    enable_prompt_simplification: bool = True  # æ˜¯å¦å¯ç”¨ prompt ç®€åŒ–
    enable_fallback_model: bool = True  # æ˜¯å¦å¯ç”¨å¤‡ç”¨æ¨¡å‹


class TimeoutRecoveryStrategy:
    """è¶…æ—¶æ¢å¤ç­–ç•¥"""
    
    def __init__(self, config: Optional[TimeoutRecoveryConfig] = None):
        self.config = config or TimeoutRecoveryConfig()
        self.retry_count = 0
        self.current_timeout = self.config.initial_timeout
    
    async def execute_with_timeout_recovery(
        self,
        func: Callable,
        *args,
        context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Any:
        """
        å¸¦è¶…æ—¶æ¢å¤çš„æ‰§è¡Œ
        
        ç­–ç•¥ï¼š
        1. ç¬¬ä¸€æ¬¡ï¼šæ­£å¸¸æ‰§è¡Œ
        2. ç¬¬äºŒæ¬¡ï¼šå¢åŠ è¶…æ—¶æ—¶é—´
        3. ç¬¬ä¸‰æ¬¡ï¼šç®€åŒ– prompt + å¢åŠ è¶…æ—¶
        4. ç¬¬å››æ¬¡ï¼šä½¿ç”¨å¤‡ç”¨æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            context: ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« promptã€model ç­‰ï¼‰
            *args, **kwargs: ä¼ é€’ç»™ func çš„å‚æ•°
        
        Returns:
            æ‰§è¡Œç»“æœ
        
        Raises:
            LLMTimeoutError: æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        """
        last_error = None
        original_prompt = None
        original_model = None
        
        # ä¿å­˜åŸå§‹å‚æ•°
        if context:
            original_prompt = context.get('prompt')
            original_model = context.get('model')
        
        while self.retry_count < self.config.max_retries:
            try:
                attempt = self.retry_count + 1
                logger.info(f"ğŸ”„ è¶…æ—¶æ¢å¤å°è¯• {attempt}/{self.config.max_retries}")
                
                # åº”ç”¨æ¢å¤ç­–ç•¥
                self._apply_recovery_strategy(attempt, context, original_prompt, original_model)
                
                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)
                
                logger.info(f"âœ… æ‰§è¡ŒæˆåŠŸï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰")
                return result
            
            except LLMTimeoutError as e:
                last_error = e
                self.retry_count += 1
                
                logger.warning(
                    f"âš ï¸ è¶…æ—¶é”™è¯¯ï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰: {e}\n"
                    f"   å½“å‰è¶…æ—¶è®¾ç½®: {self.current_timeout}ç§’"
                )
                
                if self.retry_count < self.config.max_retries:
                    logger.info(f"â³ ç­‰å¾… {self.config.retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(self.config.retry_delay)
                    continue
                
                # é‡è¯•æ¬¡æ•°ç”¨å®Œ
                break
            
            except Exception as e:
                # éè¶…æ—¶é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                logger.error(f"âŒ éè¶…æ—¶é”™è¯¯: {e}")
                raise
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(
            f"âŒ è¶…æ—¶æ¢å¤å¤±è´¥ï¼Œå·²é‡è¯• {self.config.max_retries} æ¬¡\n"
            f"   æœ€åé”™è¯¯: {last_error}"
        )
        
        raise LLMTimeoutError(
            f"è¯·æ±‚è¶…æ—¶ï¼Œå·²é‡è¯• {self.config.max_retries} æ¬¡ä»ç„¶å¤±è´¥ã€‚"
            f"å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) ç®€åŒ–é—®é¢˜ 3) ç¨åé‡è¯•"
        )
    
    def _apply_recovery_strategy(
        self,
        attempt: int,
        context: Optional[Dict[str, Any]],
        original_prompt: Optional[str],
        original_model: Optional[str]
    ):
        """
        åº”ç”¨æ¢å¤ç­–ç•¥
        
        Args:
            attempt: å½“å‰å°è¯•æ¬¡æ•°ï¼ˆ1-basedï¼‰
            context: ä¸Šä¸‹æ–‡
            original_prompt: åŸå§‹ prompt
            original_model: åŸå§‹æ¨¡å‹
        """
        if not context:
            return
        
        # ç­–ç•¥1: å¢åŠ è¶…æ—¶æ—¶é—´ï¼ˆæ‰€æœ‰é‡è¯•éƒ½åº”ç”¨ï¼‰
        self.current_timeout = min(
            self.current_timeout * self.config.timeout_multiplier,
            self.config.max_timeout
        )
        context['timeout'] = self.current_timeout
        logger.info(f"ğŸ“ˆ å¢åŠ è¶…æ—¶æ—¶é—´åˆ° {self.current_timeout} ç§’")
        
        # ç­–ç•¥2: ç®€åŒ– promptï¼ˆç¬¬3æ¬¡åŠä»¥åï¼‰
        if attempt >= 3 and self.config.enable_prompt_simplification and original_prompt:
            simplified_prompt = self._simplify_prompt(original_prompt)
            context['prompt'] = simplified_prompt
            logger.info(f"âœ‚ï¸ ç®€åŒ– promptï¼ˆä» {len(original_prompt)} å­—ç¬¦åˆ° {len(simplified_prompt)} å­—ç¬¦ï¼‰")
        
        # ç­–ç•¥3: ä½¿ç”¨å¤‡ç”¨æ¨¡å‹ï¼ˆç¬¬4æ¬¡ï¼‰
        if attempt >= 4 and self.config.enable_fallback_model and original_model:
            fallback_model = self._get_fallback_model(original_model)
            if fallback_model:
                context['model'] = fallback_model
                logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹: {fallback_model}")
    
    def _simplify_prompt(self, prompt: str) -> str:
        """
        ç®€åŒ– prompt
        
        ç­–ç•¥ï¼š
        1. ç§»é™¤ç¤ºä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰
        2. ä¿ç•™æ ¸å¿ƒæŒ‡ä»¤
        3. ç§»é™¤è¯¦ç»†è¯´æ˜
        
        Args:
            prompt: åŸå§‹ prompt
        
        Returns:
            ç®€åŒ–åçš„ prompt
        """
        # ç®€å•ç­–ç•¥ï¼šä¿ç•™å‰30%å’Œå30%ï¼Œç§»é™¤ä¸­é—´éƒ¨åˆ†
        lines = prompt.split('\n')
        total_lines = len(lines)
        
        if total_lines <= 50:
            # prompt ä¸é•¿ï¼Œä¸éœ€è¦ç®€åŒ–
            return prompt
        
        keep_lines = int(total_lines * 0.3)
        
        simplified_lines = (
            lines[:keep_lines] +
            ["\n[... ä¸ºäº†åŠ å¿«å“åº”ï¼Œéƒ¨åˆ†è¯¦ç»†è¯´æ˜å·²çœç•¥ ...]\n"] +
            lines[-keep_lines:]
        )
        
        return '\n'.join(simplified_lines)
    
    def _get_fallback_model(self, original_model: str) -> Optional[str]:
        """
        è·å–å¤‡ç”¨æ¨¡å‹
        
        ç­–ç•¥ï¼š
        - qwen-max â†’ qwen-plus
        - qwen-plus â†’ qwen-turbo
        - gpt-4 â†’ gpt-3.5-turbo
        - deepseek-coder â†’ deepseek-chat
        
        Args:
            original_model: åŸå§‹æ¨¡å‹
        
        Returns:
            å¤‡ç”¨æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        fallback_map = {
            'qwen-max': 'qwen-plus',
            'qwen-plus': 'qwen-turbo',
            'gpt-4': 'gpt-3.5-turbo',
            'gpt-4-turbo': 'gpt-3.5-turbo',
            'deepseek-coder': 'deepseek-chat',
            'claude-opus': 'claude-sonnet',
        }
        
        return fallback_map.get(original_model)
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.retry_count = 0
        self.current_timeout = self.config.initial_timeout


def create_timeout_recovery_wrapper(
    config: Optional[TimeoutRecoveryConfig] = None
) -> Callable:
    """
    åˆ›å»ºè¶…æ—¶æ¢å¤è£…é¥°å™¨
    
    Args:
        config: è¶…æ—¶æ¢å¤é…ç½®
    
    Returns:
        è£…é¥°å™¨å‡½æ•°
    
    Example:
        @create_timeout_recovery_wrapper()
        async def call_llm(prompt: str):
            # LLM è°ƒç”¨é€»è¾‘
            pass
    """
    strategy = TimeoutRecoveryStrategy(config)
    
    def decorator(func: Callable) -> Callable:
        async def wrapper(*args, **kwargs):
            return await strategy.execute_with_timeout_recovery(
                func, *args, **kwargs
            )
        return wrapper
    
    return decorator


# ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
def get_user_friendly_timeout_message(retry_count: int) -> str:
    """
    è·å–ç”¨æˆ·å‹å¥½çš„è¶…æ—¶é”™è¯¯æ¶ˆæ¯
    
    Args:
        retry_count: é‡è¯•æ¬¡æ•°
    
    Returns:
        ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
    """
    messages = {
        1: "è¯·æ±‚è¶…æ—¶äº†ï¼Œæ­£åœ¨é‡è¯•...",
        2: "è¯·æ±‚ä»ç„¶è¶…æ—¶ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åé‡è¯•...",
        3: "è¯·æ±‚æŒç»­è¶…æ—¶ï¼Œç®€åŒ–é—®é¢˜åé‡è¯•...",
    }
    
    if retry_count <= 3:
        return messages.get(retry_count, "è¯·æ±‚è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•...")
    
    return (
        "å¾ˆæŠ±æ­‰ï¼Œå¤šæ¬¡é‡è¯•åä»ç„¶è¶…æ—¶ã€‚å¯èƒ½çš„åŸå› ï¼š\n"
        "1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n"
        "2. é—®é¢˜è¿‡äºå¤æ‚\n"
        "3. LLM æœåŠ¡ç¹å¿™\n\n"
        "å»ºè®®ï¼š\n"
        "- æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
        "- ç®€åŒ–é—®é¢˜æˆ–åˆ†æ­¥éª¤æé—®\n"
        "- ç¨åé‡è¯•"
    )
