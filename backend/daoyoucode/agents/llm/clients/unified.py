"""
ç»Ÿä¸€LLMå®¢æˆ·ç«¯ï¼ˆOpenAIå…¼å®¹ï¼‰
"""

import httpx
import json
import time
from typing import AsyncIterator, Optional
import logging

from ..base import BaseLLMClient, LLMRequest, LLMResponse
from ..exceptions import LLMConnectionError, LLMTimeoutError

logger = logging.getLogger(__name__)


class UnifiedLLMClient(BaseLLMClient):
    """ç»Ÿä¸€LLMå®¢æˆ·ç«¯ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰"""
    
    # æ¨¡å‹å®šä»·ï¼ˆæ¯1000 tokensï¼Œå•ä½ï¼šå…ƒï¼‰
    PRICING = {
        # é€šä¹‰åƒé—®
        "qwen-max": {"input": 0.02, "output": 0.06},
        "qwen-plus": {"input": 0.004, "output": 0.012},
        "qwen-turbo": {"input": 0.002, "output": 0.006},
        "qwen-coder-plus": {"input": 0.004, "output": 0.012},
        # DeepSeek
        "deepseek-chat": {"input": 0.001, "output": 0.002},
        "deepseek-coder": {"input": 0.001, "output": 0.002},
        # é»˜è®¤ä»·æ ¼
        "default": {"input": 0.01, "output": 0.03},
    }
    
    def __init__(
        self,
        http_client: httpx.AsyncClient,
        api_key: str,
        base_url: str,
        model: str
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            http_client: å…±äº«çš„HTTPå®¢æˆ·ç«¯ï¼ˆå¸¦è¿æ¥æ± ï¼‰
            api_key: APIå¯†é’¥
            base_url: APIç«¯ç‚¹
            model: æ¨¡å‹åç§°
        """
        self.http_client = http_client
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.model = model
    
    async def chat(self, request: LLMRequest) -> LLMResponse:
        """åŒæ­¥å¯¹è¯"""
        start_time = time.time()
        
        # æ”¯æŒå¤šè½®å¯¹è¯ï¼šå¦‚æœrequestä¸­æœ‰messagesï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™æ„å»ºå•è½®æ¶ˆæ¯
        if hasattr(request, 'messages') and request.messages:
            messages = request.messages
        else:
            messages = [{"role": "user", "content": request.prompt}]
        
        try:
            payload = {
                "model": request.model,
                "messages": messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
            }
            
            # æ·»åŠ Function Callingæ”¯æŒ
            if hasattr(request, 'functions') and request.functions:
                payload["functions"] = request.functions
                if hasattr(request, 'function_call'):
                    payload["function_call"] = request.function_call
            
            # è¯¦ç»†è¯·æ±‚æ—¥å¿—ï¼šé»˜è®¤ logger.debugï¼Œè®¾ç½® DEBUG_LLM=1 æ—¶ç”¨ info é¿å…ç”Ÿäº§åˆ·å±ï¼ˆè§ä¼˜åŒ–å»ºè®® 3.5ï¼‰
            import os
            _log = logger.info if os.getenv('DEBUG_LLM') == '1' else logger.debug
            _log("=" * 60)
            _log("ğŸ” LLMè¯·æ±‚è°ƒè¯•ä¿¡æ¯")
            _log(f"æ¨¡å‹: {request.model}")
            _log(f"API Key: {self.api_key[:15]}...{self.api_key[-4:]}")
            _log(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
            _log(f"Functionsæ•°é‡: {len(payload.get('functions', []))}")
            for i, msg in enumerate(messages[:3]):
                content = str(msg.get('content', ''))[:200]
                _log(f"æ¶ˆæ¯ {i+1} ({msg.get('role')}): {content}...")
            if len(messages) > 3:
                _log(f"... è¿˜æœ‰ {len(messages) - 3} æ¡æ¶ˆæ¯")
            if payload.get('functions'):
                _log("Functions:")
                for i, func in enumerate(payload['functions'][:3]):
                    _log(f"  {i+1}. {func.get('name')}")
                if len(payload['functions']) > 3:
                    _log(f"  ... è¿˜æœ‰ {len(payload['functions']) - 3} ä¸ªå‡½æ•°")
            payload_size = len(json.dumps(payload, ensure_ascii=False))
            _log(f"Payloadå¤§å°: {payload_size} å­—èŠ‚ ({payload_size/1024:.2f} KB)")
            if os.getenv('DEBUG_LLM_REQUEST') == '1':
                debug_file = f"debug_llm_request_{int(time.time())}.json"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    json.dump(payload, f, ensure_ascii=False, indent=2)
                _log(f"ğŸ’¾ å®Œæ•´è¯·æ±‚å·²ä¿å­˜åˆ°: {debug_file}")
            _log("=" * 60)
            
            response = await self.http_client.post(
                f"{self.base_url}/chat/completions",
                headers=self._get_headers(),
                json=payload,
                timeout=1800.0  # ğŸ†• 30 åˆ†é’Ÿï¼ˆæ”¯æŒå¤§è§„æ¨¡æ–‡ä»¶è¯»å†™å’Œå¤æ‚ä»»åŠ¡ï¼‰
            )
            response.raise_for_status()
            data = response.json()
            
            latency = time.time() - start_time
            
            # æ£€æŸ¥æ˜¯å¦æœ‰function_call
            message = data["choices"][0]["message"]
            function_call = message.get("function_call")
            
            return LLMResponse(
                content=message.get("content", ""),
                model=request.model,
                tokens_used=data["usage"]["total_tokens"],
                cost=self._calculate_cost(data["usage"], request.model),
                latency=latency,
                metadata={
                    "prompt_tokens": data["usage"]["prompt_tokens"],
                    "completion_tokens": data["usage"]["completion_tokens"],
                    "function_call": function_call
                }
            )
        
        except httpx.TimeoutException as e:
            raise LLMTimeoutError(f"è¯·æ±‚è¶…æ—¶: {e}")
        except httpx.HTTPStatusError as e:
            # ğŸ” DEBUG: æ‰“å°é”™è¯¯å“åº”
            logger.error(f"=" * 60)
            logger.error(f"âŒ APIé”™è¯¯å“åº”")
            logger.error(f"çŠ¶æ€ç : {e.response.status_code}")
            logger.error(f"URL: {e.request.url}")
            logger.error(f"è¯·æ±‚æ–¹æ³•: {e.request.method}")
            
            # å°è¯•æ‰“å°å“åº”å†…å®¹
            try:
                error_body = e.response.text
                logger.error(f"å“åº”å†…å®¹: {error_body[:500]}")
            except:
                logger.error("æ— æ³•è¯»å–å“åº”å†…å®¹")
            
            logger.error(f"=" * 60)
            
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            status_code = e.response.status_code
            if status_code == 500:
                error_msg = (
                    f"APIæœåŠ¡å™¨é”™è¯¯ (500)ã€‚å¯èƒ½åŸå› ï¼š\n"
                    f"1. APIé…é¢ä¸è¶³ï¼ˆæ£€æŸ¥é˜¿é‡Œäº‘è´¦æˆ·ä½™é¢ï¼‰\n"
                    f"2. è¯·æ±‚æ ¼å¼é”™è¯¯ï¼ˆç‰¹åˆ«æ˜¯Function Callingï¼‰\n"
                    f"3. è¯·æ±‚è¿‡å¤§ï¼ˆmessageså†å²è¿‡é•¿ï¼‰\n"
                    f"4. æœåŠ¡ç«¯ä¸´æ—¶æ•…éšœï¼ˆç¨åé‡è¯•ï¼‰\n"
                    f"è¯¦æƒ…: {e}"
                )
            elif status_code == 429:
                error_msg = f"è¯·æ±‚é¢‘ç‡è¶…é™ (429)ã€‚è¯·ç¨åé‡è¯•ã€‚è¯¦æƒ…: {e}"
            elif status_code == 401:
                error_msg = f"API Keyæ— æ•ˆæˆ–è¿‡æœŸ (401)ã€‚è¯·æ£€æŸ¥DASHSCOPE_API_KEYç¯å¢ƒå˜é‡ã€‚è¯¦æƒ…: {e}"
            else:
                error_msg = f"HTTPé”™è¯¯ ({status_code}): {e}"
            raise LLMConnectionError(error_msg)
        except httpx.HTTPError as e:
            raise LLMConnectionError(f"è¿æ¥é”™è¯¯: {e}")
    
    async def stream_chat(self, request: LLMRequest) -> AsyncIterator[str]:
        """æµå¼å¯¹è¯"""
        messages = [{"role": "user", "content": request.prompt}]
        
        try:
            async with self.http_client.stream(
                "POST",
                f"{self.base_url}/chat/completions",
                headers=self._get_headers(),
                json={
                    "model": request.model,
                    "messages": messages,
                    "temperature": request.temperature,
                    "max_tokens": request.max_tokens,
                    "stream": True,
                },
                timeout=1800.0  # ğŸ†• 30 åˆ†é’Ÿï¼ˆæ”¯æŒå¤§è§„æ¨¡æ–‡ä»¶è¯»å†™å’Œå¤æ‚ä»»åŠ¡ï¼‰
            ) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data:"):
                        data_str = line[5:].strip()
                        if data_str == "[DONE]":
                            break
                        try:
                            data = json.loads(data_str)
                            if "choices" in data and len(data["choices"]) > 0:
                                delta = data["choices"][0].get("delta", {})
                                content = delta.get("content", "")
                                if content:
                                    yield content
                        except json.JSONDecodeError:
                            continue
        
        except httpx.TimeoutException as e:
            raise LLMTimeoutError(f"æµå¼è¯·æ±‚è¶…æ—¶: {e}")
        except httpx.HTTPError as e:
            raise LLMConnectionError(f"æµå¼è¿æ¥é”™è¯¯: {e}")
    
    def _get_headers(self) -> dict:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
    
    def _calculate_cost(self, usage: dict, model: str) -> float:
        """è®¡ç®—æˆæœ¬"""
        input_tokens = usage.get("prompt_tokens", 0)
        output_tokens = usage.get("completion_tokens", 0)
        
        # è·å–å®šä»·
        pricing = self.PRICING.get(model, self.PRICING["default"])
        
        # è®¡ç®—æˆæœ¬ï¼ˆå…ƒï¼‰
        cost = (
            input_tokens * pricing["input"] +
            output_tokens * pricing["output"]
        ) / 1000
        
        return cost
