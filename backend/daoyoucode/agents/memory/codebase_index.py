"""
ä»£ç åº“å‘é‡ç´¢å¼•ï¼ˆCursor åŒçº§æŒ‰é—®æ£€ç´¢ï¼‰

å¯¹ä»“åº“åš chunk â†’ embed â†’ å­˜å‚¨ï¼Œæ”¯æŒæŒ‰ query æ£€ç´¢ top-k ç›¸å…³ä»£ç å—ã€‚
ä¾èµ–ï¼šsentence-transformers å¯é€‰ï¼›æœªå®‰è£…æ—¶é€€åŒ–ä¸ºå…³é”®è¯åŒ¹é…ã€‚
"""

from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging
import json
import hashlib
import re

logger = logging.getLogger(__name__)

# å•ä¾‹ï¼šæŒ‰ repo è·¯å¾„ç¼“å­˜ç´¢å¼•
_index_cache: Dict[str, "CodebaseIndex"] = {}
# é»˜è®¤ chunk æœ€å¤§è¡Œæ•°
DEFAULT_CHUNK_LINES = 55
# ç´¢å¼•ç›®å½•å
INDEX_DIR = ".daoyoucode/codebase_index"


def _repo_key(repo_path: Path) -> str:
    try:
        abs_path = repo_path.resolve()
        return hashlib.sha256(str(abs_path).encode()).hexdigest()[:16]
    except Exception:
        return "default"


def _get_index_dir(repo_path: Path) -> Path:
    key = _repo_key(repo_path)
    base = repo_path if repo_path.is_dir() else repo_path.parent
    return base / INDEX_DIR / key


def _load_ignore_patterns(repo_path: Path) -> set:
    """è¯»å– .cursorignore ä¸ .gitignoreï¼Œè¿”å›ç”¨äºæ’é™¤çš„ pattern é›†åˆï¼ˆCursor åŒçº§ï¼‰"""
    out = set()
    for name in (".cursorignore", ".gitignore"):
        f = repo_path / name
        if not f.is_file():
            continue
        try:
            for line in f.read_text(encoding="utf-8", errors="ignore").splitlines():
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                # å»æ‰å°¾éš /ï¼Œä¿ç•™ä¸ºå­—é¢åŒ¹é…ç”¨
                out.add(line.rstrip("/"))
        except Exception:
            pass
    return out


def _should_ignore(path: Path, repo_path: Path, extra_patterns: Optional[set] = None) -> bool:
    rel = path.relative_to(repo_path) if repo_path in path.parents or path == repo_path else path
    rel_str = str(rel).replace("\\", "/")
    parts = rel_str.split("/")
    ignore = {
        ".git", "node_modules", "__pycache__", ".venv", "venv", "dist", "build",
        ".daoyoucode", ".cursor", ".idea", ".pytest_cache", "vendor"
    }
    for p in parts:
        if p in ignore or (p.startswith(".") and len(p) > 1):
            return True
        if p.endswith(".pyc"):
            return True
    for pat in extra_patterns or set():
        if "/" in pat:
            if pat in rel_str or rel_str.startswith(pat + "/"):
                return True
        elif pat in parts:
            return True
    return False


def _chunk_file(content: str, path: Path, max_lines: int = DEFAULT_CHUNK_LINES) -> List[Dict[str, Any]]:
    """æŒ‰è¡Œæˆ– def/class è¾¹ç•Œåˆ‡åˆ†ä¸ºå—ï¼ˆPython æŒ‰ def/classï¼Œå…¶å®ƒæŒ‰è¡Œï¼‰"""
    lines = content.splitlines()
    if not lines:
        return []
    ext = path.suffix.lower()
    chunks = []
    if ext == ".py":
        # Python: æŒ‰ def/class è¾¹ç•Œ
        current_start = 0
        for i, line in enumerate(lines):
            stripped = line.strip()
            if (stripped.startswith("def ") or stripped.startswith("class ")) and i > current_start:
                block = "\n".join(lines[current_start:i])
                if block.strip():
                    chunks.append({"start": current_start + 1, "end": i, "text": block})
                current_start = i
        if current_start < len(lines):
            block = "\n".join(lines[current_start:])
            if block.strip():
                chunks.append({"start": current_start + 1, "end": len(lines), "text": block})
    else:
        for start in range(0, len(lines), max_lines):
            end = min(start + max_lines, len(lines))
            block = "\n".join(lines[start:end])
            if block.strip():
                chunks.append({"start": start + 1, "end": end, "text": block})
    return chunks


class CodebaseIndex:
    """ä»£ç åº“å‘é‡ç´¢å¼•ï¼šchunk + embed + æ£€ç´¢"""

    def __init__(self, repo_path: Path):
        self.repo_path = Path(repo_path).resolve()
        if not self.repo_path.is_dir():
            self.repo_path = self.repo_path.parent
        self.index_dir = _get_index_dir(self.repo_path)
        self.chunks: List[Dict[str, Any]] = []  # [{path, start, end, text}, ...]
        self.embeddings: Optional[Any] = None   # np.ndarray (n, dim) or None
        self._retriever = None

    def _get_retriever(self):
        if self._retriever is None:
            from .vector_retriever_factory import get_retriever_singleton
            r = get_retriever_singleton()
            if hasattr(r, 'enable'):
                r.enable()
            self._retriever = r
        return self._retriever

    def build_index(
        self,
        max_file_size: int = 200_000,
        extensions: Optional[Tuple[str, ...]] = None,
        force: bool = False
    ) -> int:
        """
        æ‰«æä»“åº“ã€åˆ†å—ã€ç¼–ç å¹¶æŒä¹…åŒ–ã€‚è¿”å› chunk æ•°é‡ã€‚
        
        ğŸ†• ä¼˜åŒ–ï¼šå¤ç”¨RepoMapçš„tree-sitterè§£æç»“æœï¼Œé¿å…é‡å¤è§£æ
        """
        if extensions is None:
            extensions = (".py", ".js", ".ts", ".tsx", ".jsx", ".md", ".yaml", ".yml", ".json")
        self.index_dir.mkdir(parents=True, exist_ok=True)
        meta_file = self.index_dir / "meta.json"
        npy_file = self.index_dir / "embeddings.npy"

        if not force and meta_file.exists():
            try:
                with open(meta_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self.chunks = data.get("chunks", [])
                if npy_file.exists():
                    import numpy as np
                    self.embeddings = np.load(npy_file)
                logger.info(f"å·²åŠ è½½ä»£ç åº“ç´¢å¼•: {len(self.chunks)} å—")
                return len(self.chunks)
            except Exception as e:
                logger.warning(f"åŠ è½½ç´¢å¼•å¤±è´¥ï¼Œé‡å»º: {e}")

        # ğŸ†• ä½¿ç”¨RepoMapçš„tree-sitterè§£æç»“æœ
        try:
            from ..tools.repomap_tools import RepoMapTool
            repomap_tool = RepoMapTool()
            
            # è·å–ä»£ç å®šä¹‰ï¼ˆå·²åŒ…å«end_lineï¼‰
            logger.info("ğŸ” ä½¿ç”¨RepoMapè§£æä»£ç ç»“æ„...")
            definitions = repomap_tool.get_definitions(str(self.repo_path))
            
            # è·å–å¼•ç”¨å›¾ï¼ˆç”¨äºPageRankï¼‰
            reference_graph = repomap_tool.get_reference_graph(str(self.repo_path), definitions)
            
            # è·å–PageRankåˆ†æ•°
            pagerank_scores = repomap_tool.get_pagerank_scores(
                str(self.repo_path),
                reference_graph=reference_graph,
                definitions=definitions
            )
            
            logger.info(f"âœ… RepoMapè§£æå®Œæˆ: {len(definitions)} æ–‡ä»¶, {sum(len(defs) for defs in definitions.values())} å®šä¹‰")
            
            # ğŸ†• é˜¶æ®µ2ï¼šé¢„å…ˆæå–æ‰€æœ‰æ–‡ä»¶çš„å¯¼å…¥å…³ç³»
            logger.info("ğŸ“¦ æå–å¯¼å…¥å…³ç³»...")
            file_imports = {}
            for file_path in definitions.keys():
                full_path = self.repo_path / file_path
                if full_path.exists():
                    file_imports[file_path] = self._extract_imports(full_path)
            
            # åŸºäºdefinitionsæ„å»ºé«˜è´¨é‡çš„chunks
            self.chunks = []
            for file_path, defs in definitions.items():
                # åªå¤„ç†å®šä¹‰ï¼Œä¸å¤„ç†å¼•ç”¨
                def_only = [d for d in defs if d.get("kind") == "def"]
                
                for d in def_only:
                    # æå–ä»£ç æ–‡æœ¬
                    code_text = self._extract_code_chunk(
                        self.repo_path / file_path,
                        d["line"],
                        d.get("end_line", d["line"] + 50)
                    )
                    
                    if not code_text.strip():
                        continue
                    
                    # ğŸ†• é˜¶æ®µ2ï¼šæå–å‡½æ•°è°ƒç”¨
                    calls = self._extract_calls(code_text)
                    
                    # ğŸ†• é˜¶æ®µ2ï¼šæ‰¾åˆ°è°ƒç”¨è€…
                    called_by = self._find_callers(d["name"], file_path, definitions)
                    
                    # ğŸ†• é˜¶æ®µ2ï¼šè·å–ç›¸å…³æ–‡ä»¶
                    related_files = self._get_related_files(file_path, reference_graph)
                    
                    # æ„å»ºå¢å¼ºçš„chunk
                    chunk = {
                        "path": file_path,
                        "start": d["line"],
                        "end": d.get("end_line", d["line"] + len(code_text.splitlines())),
                        "text": code_text[:4000],  # é™åˆ¶é•¿åº¦
                        
                        # åŸºç¡€å…ƒæ•°æ®ï¼ˆé˜¶æ®µ1ï¼‰
                        "type": d.get("type", "unknown"),
                        "name": d.get("name", ""),
                        "pagerank_score": pagerank_scores.get(file_path, 0.0),
                        
                        # ğŸ†• é˜¶æ®µ2æ–°å¢å­—æ®µ
                        "parent_class": d.get("parent"),
                        "scope": d.get("scope", "global"),
                        "calls": calls,
                        "called_by": called_by,
                        "imports": file_imports.get(file_path, []),
                        "related_files": related_files
                    }
                    
                    self.chunks.append(chunk)
            
            logger.info(f"âœ… æ„å»ºäº† {len(self.chunks)} ä¸ªé«˜è´¨é‡ä»£ç å—ï¼ˆåŸºäºAST + å¼•ç”¨å…³ç³»ï¼‰")
            
        except Exception as e:
            logger.warning(f"RepoMapè§£æå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
            # å›é€€åˆ°åŸæœ‰çš„æ‰«æé€»è¾‘
            self.chunks = []
            extra_ignore = _load_ignore_patterns(self.repo_path)
            for path in self.repo_path.rglob("*"):
                if not path.is_file():
                    continue
                if _should_ignore(path, self.repo_path, extra_ignore):
                    continue
                if path.suffix.lower() not in extensions:
                    continue
                try:
                    content = path.read_text(encoding="utf-8", errors="ignore")
                except Exception as e:
                    logger.debug(f"è·³è¿‡ {path}: {e}")
                    continue
                if len(content) > max_file_size:
                    continue
                rel_path = path.relative_to(self.repo_path)
                rel_str = str(rel_path).replace("\\", "/")
                for c in _chunk_file(content, path):
                    self.chunks.append({
                        "path": rel_str,
                        "start": c["start"],
                        "end": c["end"],
                        "text": c["text"][:4000]
                    })

        if not self.chunks:
            logger.warning("ä»£ç åº“ç´¢å¼•æ—  chunk")
            self._save_meta()
            return 0

        retriever = self._get_retriever()
        if not getattr(retriever, "enabled", False) or not retriever.model:
            logger.warning("embedding æœªå¯ç”¨ï¼Œä»…ä¿å­˜ chunk å…ƒæ•°æ®ï¼Œæ£€ç´¢å°†ä½¿ç”¨å…³é”®è¯å›é€€")
            self._save_meta()
            return len(self.chunks)

        import numpy as np
        dim = getattr(retriever.model, "get_sentence_embedding_dimension", lambda: 384)()
        vecs = []
        for c in self.chunks:
            text = c.get("text", "")[:2000]
            emb = retriever.encode(text)
            if emb is not None:
                vecs.append(emb)
            else:
                vecs.append(np.zeros(dim, dtype=np.float32))
        self.embeddings = np.array(vecs, dtype=np.float32)
        self._save_meta()
        np.save(npy_file, self.embeddings)
        logger.info(f"ä»£ç åº“ç´¢å¼•å·²æ„å»º: {len(self.chunks)} å—, å‘é‡ç»´åº¦ {self.embeddings.shape[1]}")
        return len(self.chunks)

    def _save_meta(self):
        self.index_dir.mkdir(parents=True, exist_ok=True)
        meta_file = self.index_dir / "meta.json"
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({"chunks": self.chunks, "repo": str(self.repo_path)}, f, ensure_ascii=False, indent=0)
    
    def _extract_code_chunk(
        self,
        file_path: Path,
        start_line: int,
        end_line: int
    ) -> str:
        """
        æå–ä»£ç å—ï¼ˆç²¾ç¡®è¾¹ç•Œï¼‰
        
        ç­–ç•¥ï¼š
        1. ä½¿ç”¨start_lineå’Œend_lineæå–ä»£ç 
        2. å‘ä¸Šæ‰©å±•ï¼šåŒ…å«è£…é¥°å™¨å’Œæ³¨é‡Š
        3. ä¿æŒç¼©è¿›ä¸€è‡´æ€§
        """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            if not lines:
                return ""
            
            # è½¬ä¸º0-basedç´¢å¼•
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)
            
            # ğŸ”‘ å‘ä¸Šæ‰©å±•ï¼šåŒ…å«è£…é¥°å™¨å’Œæ³¨é‡Š
            while start_idx > 0:
                prev_line = lines[start_idx - 1].strip()
                if prev_line.startswith('@') or prev_line.startswith('#'):
                    start_idx -= 1
                else:
                    break
            
            # æå–ä»£ç 
            code_lines = lines[start_idx:end_idx]
            code_text = ''.join(code_lines)
            
            return code_text
        
        except Exception as e:
            logger.debug(f"æå–ä»£ç å—å¤±è´¥ {file_path}:{start_line}-{end_line}: {e}")
            return ""
    
    def _extract_calls(self, code_text: str, language: str = "python") -> List[str]:
        """
        ä»ä»£ç ä¸­æå–å‡½æ•°è°ƒç”¨ï¼ˆğŸ†• é˜¶æ®µ2ï¼‰
        
        ç­–ç•¥ï¼š
        1. Python: æ­£åˆ™åŒ¹é… identifier(
        2. è¿‡æ»¤å¸¸è§å…³é”®å­—ï¼ˆif, for, whileç­‰ï¼‰
        3. å»é‡
        """
        if language == "python":
            # åŒ¹é…å‡½æ•°è°ƒç”¨ï¼šidentifier(
            pattern = r'(\w+)\s*\('
            calls = re.findall(pattern, code_text)
            
            # è¿‡æ»¤Pythonå…³é”®å­—å’Œå†…ç½®å‡½æ•°
            keywords = {
                'if', 'for', 'while', 'def', 'class', 'return',
                'import', 'from', 'try', 'except', 'with', 'as',
                'print', 'len', 'str', 'int', 'float', 'list', 'dict',
                'set', 'tuple', 'bool', 'range', 'enumerate', 'zip',
                'open', 'super', 'isinstance', 'hasattr', 'getattr'
            }
            calls = [c for c in calls if c not in keywords]
            
            # å»é‡å¹¶æ’åº
            return sorted(set(calls))[:20]  # é™åˆ¶æ•°é‡
        
        return []
    
    def _extract_imports(self, file_path: Path) -> List[str]:
        """
        æå–æ–‡ä»¶çš„å¯¼å…¥è¯­å¥ï¼ˆğŸ†• é˜¶æ®µ2ï¼‰
        
        è¿”å›ï¼š
        [
            "from ..llm import get_client_manager",
            "from ..tools import get_tool_registry",
            "import json"
        ]
        """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            imports = []
            
            # åŒ¹é… import xxx
            pattern1 = r'^import\s+[\w\.]+(?:\s+as\s+\w+)?'
            imports.extend(re.findall(pattern1, content, re.MULTILINE))
            
            # åŒ¹é… from xxx import yyy
            pattern2 = r'^from\s+[\w\.]+\s+import\s+[\w\s,]+(?:\s+as\s+\w+)?'
            imports.extend(re.findall(pattern2, content, re.MULTILINE))
            
            return imports[:20]  # é™åˆ¶æ•°é‡
        
        except Exception as e:
            logger.debug(f"æå–å¯¼å…¥å¤±è´¥ {file_path}: {e}")
            return []
    
    def _get_related_files(
        self,
        file_path: str,
        reference_graph: Dict[str, Dict[str, float]],
        top_k: int = 5
    ) -> List[str]:
        """
        è·å–ç›¸å…³æ–‡ä»¶ï¼ˆåŸºäºå¼•ç”¨å›¾ï¼‰ï¼ˆğŸ†• é˜¶æ®µ2ï¼‰
        
        ç­–ç•¥ï¼š
        1. ä»å¼•ç”¨å›¾ä¸­è·å–ç›´æ¥å¼•ç”¨çš„æ–‡ä»¶
        2. æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº
        3. è¿”å›top_kä¸ª
        """
        if file_path not in reference_graph:
            return []
        
        # è·å–å¼•ç”¨å…³ç³» {file: count}
        refs = reference_graph[file_path]
        
        # æŒ‰å¼•ç”¨æ¬¡æ•°æ’åº
        sorted_refs = sorted(
            refs.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # è¿”å›æ–‡ä»¶è·¯å¾„
        return [file for file, _ in sorted_refs[:top_k]]
    
    def _find_callers(
        self,
        function_name: str,
        file_path: str,
        all_definitions: Dict[str, List[Dict]]
    ) -> List[str]:
        """
        æ‰¾åˆ°è°ƒç”¨è¿™ä¸ªå‡½æ•°çš„æ–‡ä»¶ï¼ˆğŸ†• é˜¶æ®µ2ï¼‰
        
        ç­–ç•¥ï¼š
        1. éå†æ‰€æœ‰definitions
        2. æ‰¾åˆ°kind="ref"ä¸”nameåŒ¹é…çš„å¼•ç”¨
        3. è¿”å›å¼•ç”¨æ‰€åœ¨çš„æ–‡ä»¶
        """
        callers = []
        
        for other_file, defs in all_definitions.items():
            if other_file == file_path:
                continue  # è·³è¿‡è‡ªå·±
            
            for d in defs:
                if d.get("kind") == "ref" and d.get("name") == function_name:
                    if other_file not in callers:
                        callers.append(other_file)
                        break  # æ¯ä¸ªæ–‡ä»¶åªè®°å½•ä¸€æ¬¡
        
        return callers[:10]  # é™åˆ¶æ•°é‡

    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æŒ‰ query æ£€ç´¢æœ€ç›¸å…³çš„ä»£ç å—ã€‚æ— å‘é‡æ—¶é€€åŒ–ä¸ºå…³é”®è¯åŒ¹é…ã€‚"""
        if not self.chunks:
            self.build_index()
        if not self.chunks:
            return []

        retriever = self._get_retriever()
        if retriever.enabled and self.embeddings is not None:
            import numpy as np
            q = retriever.encode(query)
            if q is not None:
                q_norm = q / np.linalg.norm(q)
                emb_norm = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)
                scores = np.dot(emb_norm, q_norm)
                top_idx = np.argsort(scores)[::-1][:top_k]
                return [
                    {**self.chunks[i], "score": float(scores[i])}
                    for i in top_idx if scores[i] > 1e-6
                ]

        # å…³é”®è¯å›é€€
        words = re.findall(r"\w+", query.lower())
        if not words:
            return self.chunks[:top_k]
        scored = []
        for c in self.chunks:
            text = (c.get("text") or "").lower()
            score = sum(1 for w in words if w in text)
            if score > 0:
                scored.append((score, c))
        scored.sort(key=lambda x: -x[0])
        return [{**c, "score": float(s)} for s, c in scored[:top_k]]
    
    # ========== é˜¶æ®µ3ï¼šå¤šå±‚æ¬¡æ£€ç´¢ ==========
    
    def search_multilayer(
        self,
        query: str,
        top_k: int = 10,
        enable_file_expansion: bool = True,
        enable_reference_expansion: bool = True,
        max_expansion: int = 50
    ) -> List[Dict[str, Any]]:
        """
        å¤šå±‚æ¬¡æ£€ç´¢ï¼ˆğŸ†• é˜¶æ®µ3ï¼‰
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›ç»“æœæ•°é‡
            enable_file_expansion: æ˜¯å¦å¯ç”¨æ–‡ä»¶å…³è”æ‰©å±•
            enable_reference_expansion: æ˜¯å¦å¯ç”¨å¼•ç”¨å…³ç³»æ‰©å±•
            max_expansion: æœ€å¤§æ‰©å±•æ•°é‡
        
        Returns:
            å¢å¼ºçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.chunks:
            self.build_index()
        if not self.chunks:
            return []
        
        logger.info(f"ğŸ” å¤šå±‚æ¬¡æ£€ç´¢: {query}")
        
        # ç¬¬1å±‚ï¼šè¯­ä¹‰æ£€ç´¢
        results = self.search(query, top_k * 2)
        logger.info(f"   ç¬¬1å±‚ï¼ˆè¯­ä¹‰ï¼‰: {len(results)} ä¸ªç»“æœ")
        
        if not results:
            return []
        
        # ç¬¬2å±‚ï¼šæ–‡ä»¶å…³è”æ‰©å±•
        if enable_file_expansion:
            results = self._expand_by_files(results)
            results = results[:max_expansion]  # é™åˆ¶è§„æ¨¡
            logger.info(f"   ç¬¬2å±‚ï¼ˆæ–‡ä»¶å…³è”ï¼‰: {len(results)} ä¸ªç»“æœ")
        
        # ç¬¬3å±‚ï¼šå¼•ç”¨å…³ç³»æ‰©å±•
        if enable_reference_expansion:
            results = self._expand_by_references(results)
            results = results[:max_expansion]  # é™åˆ¶è§„æ¨¡
            logger.info(f"   ç¬¬3å±‚ï¼ˆå¼•ç”¨å…³ç³»ï¼‰: {len(results)} ä¸ªç»“æœ")
        
        # ç¬¬4å±‚ï¼šå»é‡å’Œé‡æ’åº
        results = self._deduplicate_and_rerank(results, query)
        logger.info(f"   ç¬¬4å±‚ï¼ˆå»é‡æ’åºï¼‰: {len(results)} ä¸ªç»“æœ")
        
        # è¿”å›top-k
        final_results = results[:top_k]
        logger.info(f"   âœ… æœ€ç»ˆè¿”å›: {len(final_results)} ä¸ªç»“æœ")
        
        return final_results
    
    def _expand_by_files(
        self,
        results: List[Dict],
        max_per_file: int = 2
    ) -> List[Dict]:
        """
        æ–‡ä»¶å…³è”æ‰©å±•ï¼ˆğŸ†• é˜¶æ®µ3ï¼‰
        
        ç­–ç•¥ï¼š
        1. å¯¹æ¯ä¸ªç»“æœï¼Œè·å–å…¶related_files
        2. ä»æ¯ä¸ªç›¸å…³æ–‡ä»¶ä¸­é€‰æ‹©top-2 chunks
        3. æ·»åŠ åˆ°ç»“æœé›†
        """
        expanded = list(results)  # ä¿ç•™åŸå§‹ç»“æœ
        seen_ids = {self._chunk_id(c) for c in results}
        
        for result in results[:5]:  # åªæ‰©å±•å‰5ä¸ª
            related_files = result.get('related_files', [])
            
            for related_file in related_files[:3]:  # æ¯ä¸ªç»“æœæœ€å¤š3ä¸ªç›¸å…³æ–‡ä»¶
                # è·å–è¯¥æ–‡ä»¶çš„chunks
                file_chunks = [
                    c for c in self.chunks 
                    if c['path'] == related_file
                ]
                
                # æŒ‰PageRankæ’åºï¼Œé€‰æ‹©top-2
                file_chunks.sort(
                    key=lambda c: c.get('pagerank_score', 0),
                    reverse=True
                )
                
                for chunk in file_chunks[:max_per_file]:
                    chunk_id = self._chunk_id(chunk)
                    if chunk_id not in seen_ids:
                        expanded.append(chunk)
                        seen_ids.add(chunk_id)
        
        return expanded
    
    def _expand_by_references(
        self,
        results: List[Dict],
        max_callers: int = 1,
        max_callees: int = 1
    ) -> List[Dict]:
        """
        å¼•ç”¨å…³ç³»æ‰©å±•ï¼ˆğŸ†• é˜¶æ®µ3ï¼‰
        
        ç­–ç•¥ï¼š
        1. å¯¹æ¯ä¸ªç»“æœï¼Œè·å–å…¶called_byï¼ˆè°ƒç”¨è€…ï¼‰
        2. å¯¹æ¯ä¸ªç»“æœï¼Œè·å–å…¶callsï¼ˆè¢«è°ƒç”¨è€…ï¼‰
        3. æ·»åŠ åˆ°ç»“æœé›†
        """
        expanded = list(results)
        seen_ids = {self._chunk_id(c) for c in results}
        
        for result in results[:3]:  # åªæ‰©å±•å‰3ä¸ª
            # æ‰©å±•åˆ°è°ƒç”¨è€…
            called_by = result.get('called_by', [])
            for caller_file in called_by[:max_callers]:
                caller_chunks = [
                    c for c in self.chunks
                    if c['path'] == caller_file
                ]
                
                # é€‰æ‹©PageRankæœ€é«˜çš„
                if caller_chunks:
                    caller_chunks.sort(
                        key=lambda c: c.get('pagerank_score', 0),
                        reverse=True
                    )
                    chunk = caller_chunks[0]
                    chunk_id = self._chunk_id(chunk)
                    if chunk_id not in seen_ids:
                        expanded.append(chunk)
                        seen_ids.add(chunk_id)
            
            # æ‰©å±•åˆ°è¢«è°ƒç”¨è€…
            calls = result.get('calls', [])
            for callee_name in calls[:max_callees]:
                # æŸ¥æ‰¾è¯¥å‡½æ•°çš„å®šä¹‰
                callee_chunks = [
                    c for c in self.chunks
                    if c['name'] == callee_name and c['type'] in ('function', 'method')
                ]
                
                if callee_chunks:
                    # é€‰æ‹©PageRankæœ€é«˜çš„
                    callee_chunks.sort(
                        key=lambda c: c.get('pagerank_score', 0),
                        reverse=True
                    )
                    chunk = callee_chunks[0]
                    chunk_id = self._chunk_id(chunk)
                    if chunk_id not in seen_ids:
                        expanded.append(chunk)
                        seen_ids.add(chunk_id)
        
        return expanded
    
    def _deduplicate_and_rerank(
        self,
        results: List[Dict],
        query: str
    ) -> List[Dict]:
        """
        å»é‡å’Œé‡æ’åºï¼ˆğŸ†• é˜¶æ®µ3ï¼‰
        
        ç­–ç•¥ï¼š
        1. å»é‡ï¼ˆåŸºäºpath+startï¼‰
        2. è®¡ç®—ç»¼åˆåˆ†æ•°ï¼š
           - è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
           - PageRankåˆ†æ•°
           - åœ¨ç»“æœä¸­çš„ä½ç½®ï¼ˆè¶Šæ—©è¶Šé‡è¦ï¼‰
        3. æ’åºå¹¶è¿”å›
        """
        # å»é‡
        seen_ids = set()
        unique_results = []
        
        for result in results:
            chunk_id = self._chunk_id(result)
            if chunk_id not in seen_ids:
                unique_results.append(result)
                seen_ids.add(chunk_id)
        
        # é‡æ’åº
        retriever = self._get_retriever()
        if retriever.enabled and self.embeddings is not None:
            # é‡æ–°è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            q = retriever.encode(query)
            if q is not None:
                import numpy as np
                for i, result in enumerate(unique_results):
                    # æ‰¾åˆ°åŸå§‹embedding
                    chunk_idx = self._find_chunk_index(result)
                    if chunk_idx >= 0:
                        emb = self.embeddings[chunk_idx]
                        similarity = np.dot(emb, q) / (np.linalg.norm(emb) * np.linalg.norm(q))
                    else:
                        similarity = 0.0
                    
                    # ç»¼åˆåˆ†æ•°
                    position_score = 1.0 / (i + 1)  # ä½ç½®è¶Šæ—©åˆ†æ•°è¶Šé«˜
                    pagerank_score = result.get('pagerank_score', 0.0)
                    
                    result['final_score'] = (
                        0.5 * similarity +           # è¯­ä¹‰ç›¸ä¼¼åº¦ 50%
                        0.3 * pagerank_score +       # PageRank 30%
                        0.2 * position_score         # ä½ç½® 20%
                    )
        else:
            # æ²¡æœ‰å‘é‡ï¼Œåªç”¨PageRankå’Œä½ç½®
            for i, result in enumerate(unique_results):
                position_score = 1.0 / (i + 1)
                pagerank_score = result.get('pagerank_score', 0.0)
                
                result['final_score'] = (
                    0.7 * pagerank_score +
                    0.3 * position_score
                )
        
        # æ’åº
        unique_results.sort(key=lambda r: r.get('final_score', 0), reverse=True)
        
        return unique_results
    
    def _chunk_id(self, chunk: Dict) -> str:
        """ç”Ÿæˆchunkçš„å”¯ä¸€IDï¼ˆğŸ†• é˜¶æ®µ3ï¼‰"""
        return f"{chunk['path']}:{chunk['start']}"
    
    def _find_chunk_index(self, chunk: Dict) -> int:
        """æ‰¾åˆ°chunkåœ¨self.chunksä¸­çš„ç´¢å¼•ï¼ˆğŸ†• é˜¶æ®µ3ï¼‰"""
        chunk_id = self._chunk_id(chunk)
        for i, c in enumerate(self.chunks):
            if self._chunk_id(c) == chunk_id:
                return i
        return -1

    @classmethod
    def get_index(cls, repo_path: Path) -> "CodebaseIndex":
        key = _repo_key(Path(repo_path).resolve())
        if key not in _index_cache:
            _index_cache[key] = cls(repo_path)
        return _index_cache[key]


def search_codebase(repo_path: Path, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æˆ–æ„å»ºç´¢å¼•å¹¶æ£€ç´¢ã€‚"""
    idx = CodebaseIndex.get_index(repo_path)
    return idx.search(query, top_k=top_k)
