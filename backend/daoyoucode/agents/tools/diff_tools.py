"""
Diffå·¥å…· - æ™ºèƒ½æ›¿æ¢ç­–ç•¥

é‡‡ç”¨å…ˆè¿›çš„ Diff ç³»ç»Ÿå®ç°ï¼Œæ”¯æŒï¼š
- 9ç§æ™ºèƒ½æ›¿æ¢ç­–ç•¥
- Levenshteinè·ç¦»ç®—æ³•
- BlockAnchorReplacerï¼ˆé¦–å°¾è¡Œé”šå®šï¼‰
- æ¨¡ç³ŠåŒ¹é…å’Œå®¹é”™
"""

from typing import Dict, Any, Generator, Optional, List, Tuple, AsyncGenerator
from pathlib import Path
import re
import asyncio
from .base import BaseTool, ToolResult, StreamingEditTool, EditEvent


# ========== Levenshteinè·ç¦»ç®—æ³• ==========

def levenshtein(a: str, b: str) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„Levenshteinç¼–è¾‘è·ç¦»
    
    ç”¨äºè¡¡é‡å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼Œè·ç¦»è¶Šå°è¶Šç›¸ä¼¼
    """
    if a == "" or b == "":
        return max(len(a), len(b))
    
    # åˆ›å»ºè·ç¦»çŸ©é˜µ
    matrix = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]
    
    # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
    for i in range(len(a) + 1):
        matrix[i][0] = i
    for j in range(len(b) + 1):
        matrix[0][j] = j
    
    # åŠ¨æ€è§„åˆ’è®¡ç®—è·ç¦»
    for i in range(1, len(a) + 1):
        for j in range(1, len(b) + 1):
            cost = 0 if a[i - 1] == b[j - 1] else 1
            matrix[i][j] = min(
                matrix[i - 1][j] + 1,      # åˆ é™¤
                matrix[i][j - 1] + 1,      # æ’å…¥
                matrix[i - 1][j - 1] + cost  # æ›¿æ¢
            )
    
    return matrix[len(a)][len(b)]


# ========== 9ç§Replacerç­–ç•¥ ==========

class Replacer:
    """ReplaceråŸºç±»"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        """æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹"""
        raise NotImplementedError()


class SimpleReplacer(Replacer):
    """ç­–ç•¥1: ç²¾ç¡®åŒ¹é…"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        if find in content:
            yield find


class LineTrimmedReplacer(Replacer):
    """ç­–ç•¥2: å¿½ç•¥è¡Œé¦–å°¾ç©ºç™½"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        original_lines = content.split("\n")
        search_lines = find.split("\n")
        
        # ç§»é™¤æœ«å°¾ç©ºè¡Œ
        if search_lines and search_lines[-1] == "":
            search_lines.pop()
        
        # æ»‘åŠ¨çª—å£åŒ¹é…
        for i in range(len(original_lines) - len(search_lines) + 1):
            matches = True
            
            for j in range(len(search_lines)):
                if original_lines[i + j].strip() != search_lines[j].strip():
                    matches = False
                    break
            
            if matches:
                # è®¡ç®—åŒ¹é…çš„èµ·æ­¢ä½ç½®
                match_start = sum(len(original_lines[k]) + 1 for k in range(i))
                match_end = match_start + sum(
                    len(original_lines[i + k]) + (1 if k < len(search_lines) - 1 else 0)
                    for k in range(len(search_lines))
                )
                yield content[match_start:match_end]


class BlockAnchorReplacer(Replacer):
    """
    ç­–ç•¥3: é¦–å°¾è¡Œé”šå®š + Levenshteinç›¸ä¼¼åº¦
    
    è¿™æ˜¯æœ€å¼ºå¤§çš„ç­–ç•¥ï¼š
    - ä½¿ç”¨é¦–å°¾è¡Œä½œä¸ºé”šç‚¹
    - è®¡ç®—ä¸­é—´è¡Œçš„Levenshteinç›¸ä¼¼åº¦
    - å•å€™é€‰é˜ˆå€¼0.0ï¼Œå¤šå€™é€‰é˜ˆå€¼0.3
    """
    
    SINGLE_CANDIDATE_THRESHOLD = 0.0
    MULTIPLE_CANDIDATES_THRESHOLD = 0.3
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        original_lines = content.split("\n")
        search_lines = find.split("\n")
        
        if len(search_lines) < 3:
            return
        
        if search_lines and search_lines[-1] == "":
            search_lines.pop()
        
        first_line = search_lines[0].strip()
        last_line = search_lines[-1].strip()
        search_block_size = len(search_lines)
        
        # æ”¶é›†æ‰€æœ‰å€™é€‰ä½ç½®
        candidates: List[Tuple[int, int]] = []
        for i in range(len(original_lines)):
            if original_lines[i].strip() != first_line:
                continue
            
            # æŸ¥æ‰¾åŒ¹é…çš„æœ«å°¾è¡Œ
            for j in range(i + 2, len(original_lines)):
                if original_lines[j].strip() == last_line:
                    candidates.append((i, j))
                    break
        
        if not candidates:
            return
        
        # å•å€™é€‰æƒ…å†µï¼ˆä½¿ç”¨å®½æ¾é˜ˆå€¼ï¼‰
        if len(candidates) == 1:
            start_line, end_line = candidates[0]
            actual_block_size = end_line - start_line + 1
            
            similarity = 0.0
            lines_to_check = min(search_block_size - 2, actual_block_size - 2)
            
            if lines_to_check > 0:
                for j in range(1, min(search_block_size - 1, actual_block_size - 1)):
                    original_line = original_lines[start_line + j].strip()
                    search_line = search_lines[j].strip()
                    max_len = max(len(original_line), len(search_line))
                    
                    if max_len == 0:
                        continue
                    
                    distance = levenshtein(original_line, search_line)
                    similarity += (1 - distance / max_len) / lines_to_check
                    
                    if similarity >= BlockAnchorReplacer.SINGLE_CANDIDATE_THRESHOLD:
                        break
            else:
                similarity = 1.0
            
            if similarity >= BlockAnchorReplacer.SINGLE_CANDIDATE_THRESHOLD:
                match_start = sum(len(original_lines[k]) + 1 for k in range(start_line))
                match_end = match_start + sum(
                    len(original_lines[k]) + (1 if k < end_line else 0)
                    for k in range(start_line, end_line + 1)
                )
                yield content[match_start:match_end]
            return
        
        # å¤šå€™é€‰æƒ…å†µï¼ˆè®¡ç®—æœ€ä½³åŒ¹é…ï¼‰
        best_match: Optional[Tuple[int, int]] = None
        max_similarity = -1.0
        
        for start_line, end_line in candidates:
            actual_block_size = end_line - start_line + 1
            
            similarity = 0.0
            lines_to_check = min(search_block_size - 2, actual_block_size - 2)
            
            if lines_to_check > 0:
                for j in range(1, min(search_block_size - 1, actual_block_size - 1)):
                    original_line = original_lines[start_line + j].strip()
                    search_line = search_lines[j].strip()
                    max_len = max(len(original_line), len(search_line))
                    
                    if max_len == 0:
                        continue
                    
                    distance = levenshtein(original_line, search_line)
                    similarity += 1 - distance / max_len
                
                similarity /= lines_to_check
            else:
                similarity = 1.0
            
            if similarity > max_similarity:
                max_similarity = similarity
                best_match = (start_line, end_line)
        
        if max_similarity >= BlockAnchorReplacer.MULTIPLE_CANDIDATES_THRESHOLD and best_match:
            start_line, end_line = best_match
            match_start = sum(len(original_lines[k]) + 1 for k in range(start_line))
            match_end = match_start + sum(
                len(original_lines[k]) + (1 if k < end_line else 0)
                for k in range(start_line, end_line + 1)
            )
            yield content[match_start:match_end]


class WhitespaceNormalizedReplacer(Replacer):
    """ç­–ç•¥4: ç©ºç™½å½’ä¸€åŒ–"""
    
    @staticmethod
    def normalize_whitespace(text: str) -> str:
        return re.sub(r'\s+', ' ', text).strip()
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        normalized_find = WhitespaceNormalizedReplacer.normalize_whitespace(find)
        
        # å•è¡ŒåŒ¹é…
        lines = content.split("\n")
        for line in lines:
            if WhitespaceNormalizedReplacer.normalize_whitespace(line) == normalized_find:
                yield line
        
        # å¤šè¡ŒåŒ¹é…
        find_lines = find.split("\n")
        if len(find_lines) > 1:
            for i in range(len(lines) - len(find_lines) + 1):
                block = "\n".join(lines[i:i + len(find_lines)])
                if WhitespaceNormalizedReplacer.normalize_whitespace(block) == normalized_find:
                    yield block


class IndentationFlexibleReplacer(Replacer):
    """ç­–ç•¥5: ç¼©è¿›çµæ´»åŒ¹é…"""
    
    @staticmethod
    def remove_indentation(text: str) -> str:
        lines = text.split("\n")
        non_empty_lines = [line for line in lines if line.strip()]
        
        if not non_empty_lines:
            return text
        
        min_indent = min(
            len(line) - len(line.lstrip())
            for line in non_empty_lines
        )
        
        return "\n".join(
            line[min_indent:] if line.strip() else line
            for line in lines
        )
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        normalized_find = IndentationFlexibleReplacer.remove_indentation(find)
        content_lines = content.split("\n")
        find_lines = find.split("\n")
        
        for i in range(len(content_lines) - len(find_lines) + 1):
            block = "\n".join(content_lines[i:i + len(find_lines)])
            if IndentationFlexibleReplacer.remove_indentation(block) == normalized_find:
                yield block


class EscapeNormalizedReplacer(Replacer):
    """ç­–ç•¥6: è½¬ä¹‰å­—ç¬¦å¤„ç†"""
    
    @staticmethod
    def unescape_string(s: str) -> str:
        replacements = {
            r'\n': '\n',
            r'\t': '\t',
            r'\r': '\r',
            r"\'": "'",
            r'\"': '"',
            r'\`': '`',
            r'\\': '\\',
            r'\$': '$',
        }
        result = s
        for escaped, unescaped in replacements.items():
            result = result.replace(escaped, unescaped)
        return result
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        unescaped_find = EscapeNormalizedReplacer.unescape_string(find)
        
        if unescaped_find in content:
            yield unescaped_find
        
        # å°è¯•æŸ¥æ‰¾è½¬ä¹‰ç‰ˆæœ¬
        lines = content.split("\n")
        find_lines = unescaped_find.split("\n")
        
        for i in range(len(lines) - len(find_lines) + 1):
            block = "\n".join(lines[i:i + len(find_lines)])
            if EscapeNormalizedReplacer.unescape_string(block) == unescaped_find:
                yield block


class TrimmedBoundaryReplacer(Replacer):
    """ç­–ç•¥7: è¾¹ç•Œtrim"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        trimmed_find = find.strip()
        
        if trimmed_find == find:
            return
        
        if trimmed_find in content:
            yield trimmed_find
        
        lines = content.split("\n")
        find_lines = find.split("\n")
        
        for i in range(len(lines) - len(find_lines) + 1):
            block = "\n".join(lines[i:i + len(find_lines)])
            if block.strip() == trimmed_find:
                yield block


class ContextAwareReplacer(Replacer):
    """ç­–ç•¥8: ä¸Šä¸‹æ–‡æ„ŸçŸ¥"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        find_lines = find.split("\n")
        if len(find_lines) < 3:
            return
        
        if find_lines and find_lines[-1] == "":
            find_lines.pop()
        
        content_lines = content.split("\n")
        first_line = find_lines[0].strip()
        last_line = find_lines[-1].strip()
        
        for i in range(len(content_lines)):
            if content_lines[i].strip() != first_line:
                continue
            
            for j in range(i + 2, len(content_lines)):
                if content_lines[j].strip() == last_line:
                    block_lines = content_lines[i:j + 1]
                    
                    if len(block_lines) == len(find_lines):
                        # æ£€æŸ¥ä¸­é—´è¡Œç›¸ä¼¼åº¦ï¼ˆè‡³å°‘50%åŒ¹é…ï¼‰
                        matching_lines = 0
                        total_non_empty = 0
                        
                        for k in range(1, len(block_lines) - 1):
                            block_line = block_lines[k].strip()
                            find_line = find_lines[k].strip()
                            
                            if block_line or find_line:
                                total_non_empty += 1
                                if block_line == find_line:
                                    matching_lines += 1
                        
                        if total_non_empty == 0 or matching_lines / total_non_empty >= 0.5:
                            yield "\n".join(block_lines)
                            break
                    break


class MultiOccurrenceReplacer(Replacer):
    """ç­–ç•¥9: å¤šæ¬¡å‡ºç°å¤„ç†"""
    
    @staticmethod
    def find_matches(content: str, find: str) -> Generator[str, None, None]:
        start_index = 0
        while True:
            index = content.find(find, start_index)
            if index == -1:
                break
            yield find
            start_index = index + len(find)


# ========== æ ¸å¿ƒæ›¿æ¢å‡½æ•° ==========

def replace(content: str, old_string: str, new_string: str, replace_all: bool = False) -> str:
    """
    ä½¿ç”¨9ç§ç­–ç•¥è¿›è¡Œæ™ºèƒ½æ›¿æ¢
    
    Args:
        content: åŸå§‹å†…å®¹
        old_string: è¦æ›¿æ¢çš„å­—ç¬¦ä¸²
        new_string: æ›¿æ¢åçš„å­—ç¬¦ä¸²
        replace_all: æ˜¯å¦æ›¿æ¢æ‰€æœ‰å‡ºç°
    
    Returns:
        æ›¿æ¢åçš„å†…å®¹
    
    Raises:
        ValueError: å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…æˆ–æœ‰å¤šä¸ªåŒ¹é…
    """
    if old_string == new_string:
        raise ValueError("old_string and new_string must be different")
    
    not_found = True
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•9ç§ç­–ç•¥
    replacers = [
        SimpleReplacer,
        LineTrimmedReplacer,
        BlockAnchorReplacer,
        WhitespaceNormalizedReplacer,
        IndentationFlexibleReplacer,
        EscapeNormalizedReplacer,
        TrimmedBoundaryReplacer,
        ContextAwareReplacer,
        MultiOccurrenceReplacer,
    ]
    
    for replacer_class in replacers:
        for search in replacer_class.find_matches(content, old_string):
            index = content.find(search)
            if index == -1:
                continue
            
            not_found = False
            
            if replace_all:
                return content.replace(search, new_string)
            
            # æ£€æŸ¥æ˜¯å¦å”¯ä¸€
            last_index = content.rfind(search)
            if index != last_index:
                continue
            
            # æ‰§è¡Œæ›¿æ¢
            return content[:index] + new_string + content[index + len(search):]
    
    if not_found:
        raise ValueError("old_string not found in content")
    
    raise ValueError(
        "Found multiple matches for old_string. "
        "Provide more surrounding lines to identify the correct match."
    )


# ========== å·¥å…·å°è£… ==========

class SearchReplaceTool(BaseTool):
    """SEARCH/REPLACEç¼–è¾‘å·¥å…·"""
    
    def __init__(self):
        super().__init__(
            name="search_replace",
            description="ä½¿ç”¨SEARCH/REPLACEæ¨¡å¼ç¼–è¾‘æ–‡ä»¶ï¼ˆæ”¯æŒ9ç§æ™ºèƒ½åŒ¹é…ç­–ç•¥ï¼‰"
        )
    
    async def execute(
        self,
        file_path: str,
        search: str,
        replace: str,
        replace_all: bool = False
    ) -> ToolResult:
        """
        æ‰§è¡ŒSEARCH/REPLACE
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            search: è¦æœç´¢çš„å†…å®¹
            replace: æ›¿æ¢åçš„å†…å®¹
            replace_all: æ˜¯å¦æ›¿æ¢æ‰€æœ‰å‡ºç°
        """
        try:
            # ä½¿ç”¨ resolve_path è§£æè·¯å¾„
            path = self.resolve_path(file_path)
            
            if not path.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"File not found: {file_path} (resolved to {path})"
                )
            
            # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
            old_content = path.read_text(encoding='utf-8', errors='ignore')
            
            # æ‰§è¡Œæ›¿æ¢ï¼ˆä½¿ç”¨æ¨¡å—çº§å‡½æ•°ï¼‰
            from . import diff_tools
            new_content = diff_tools.replace(old_content, search, replace, replace_all)
            
            # ç”Ÿæˆ diff
            import difflib
            diff_lines = list(difflib.unified_diff(
                old_content.splitlines(keepends=True),
                new_content.splitlines(keepends=True),
                fromfile=f"a/{file_path}",
                tofile=f"b/{file_path}",
                lineterm=''
            ))
            
            diff_text = ''.join(diff_lines) if diff_lines else "No changes"
            
            # å†™å…¥æ–‡ä»¶
            path.write_text(new_content, encoding='utf-8')
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_message = f"âœ… Successfully modified {file_path}\n\n"
            result_message += "ğŸ“ Changes:\n"
            result_message += "```diff\n"
            result_message += diff_text
            result_message += "\n```"
            
            return ToolResult(
                success=True,
                content=result_message,
                metadata={
                    'file_path': str(path),
                    'old_size': len(old_content),
                    'new_size': len(new_content),
                    'replace_all': replace_all,
                    'diff': diff_text,
                    'changes_count': len([line for line in diff_lines if line.startswith('+') or line.startswith('-')])
                }
            )
        except Exception as e:
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    def get_function_schema(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡ä»¶è·¯å¾„"
                    },
                    "search": {
                        "type": "string",
                        "description": "è¦æœç´¢çš„å†…å®¹"
                    },
                    "replace": {
                        "type": "string",
                        "description": "æ›¿æ¢åçš„å†…å®¹"
                    },
                    "replace_all": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æ›¿æ¢æ‰€æœ‰å‡ºç°",
                        "default": False
                    }
                },
                "required": ["file_path", "search", "replace"]
            }
        }


# ========== Unified Diff (editblock/udiff å¼ç»†ç²’åº¦ç¼–è¾‘) ==========

def _parse_unified_diff(diff_text: str) -> List[Dict[str, Any]]:
    """
    è§£æ unified diffï¼Œè¿”å› [{"path": str, "hunks": [(old_start, old_count, new_start, new_count, lines)]}, ...]
    path ä¸ºç›¸å¯¹è·¯å¾„ï¼ˆå·²å»æ‰ a/ b/ å‰ç¼€ï¼‰
    """
    files = []
    current_file: Optional[Dict[str, Any]] = None
    current_hunk: Optional[Tuple] = None
    for line in diff_text.splitlines(keepends=True):
        if line.startswith("--- "):
            # æ—§æ–‡ä»¶è·¯å¾„ï¼š--- a/foo.py æˆ– --- foo.py
            raw = line[4:].rstrip()
            path = raw.split("\t")[0].strip()
            if path.startswith("a/"):
                path = path[2:]
            if current_file and current_hunk is not None:
                current_file["hunks"].append(current_hunk)
            current_file = {"path": path, "hunks": []}
            current_hunk = None
        elif line.startswith("+++ "):
            # æ–°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ä½¿ç”¨ï¼‰
            raw = line[4:].rstrip()
            path = raw.split("\t")[0].strip()
            if path.startswith("b/"):
                path = path[2:]
            if current_file:
                current_file["path"] = path
            current_hunk = None
        elif line.startswith("@@ "):
            if current_file is None:
                continue
            if current_hunk is not None:
                current_file["hunks"].append(current_hunk)
            # @@ -old_start,old_count +new_start,new_count @@
            m = re.match(r"@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@", line.strip())
            if m:
                old_s, old_c, new_s, new_c = m.groups()
                current_hunk = (
                    int(old_s),
                    int(old_c) if old_c else 1,
                    int(new_s),
                    int(new_c) if new_c else 1,
                    [],
                )
        elif current_hunk is not None:
            # è¡Œå†…å®¹ï¼šç©ºæ ¼=ä¸Šä¸‹æ–‡ï¼Œ-=åˆ é™¤ï¼Œ+=æ·»åŠ 
            current_hunk[4].append(line)
    if current_file and current_hunk is not None:
        current_file["hunks"].append(current_hunk)
    if current_file:
        files.append(current_file)
    return files


def _apply_hunk(old_lines: List[str], old_start: int, old_count: int, new_start: int, new_count: int, hunk_lines: List[str]) -> List[str]:
    """åº”ç”¨å•ä¸ª hunkã€‚old_lines ä¸ºå¸¦æ¢è¡Œç¬¦çš„è¡Œåˆ—è¡¨ï¼›unified diffï¼šç©ºæ ¼=ä¸Šä¸‹æ–‡ï¼Œ-=åˆ é™¤ï¼Œ+=æ·»åŠ ã€‚"""
    if old_start <= 0:
        result = []
        old_pos = 0
    else:
        result = old_lines[: old_start - 1]
        old_pos = old_start - 1
    for hunk_line in hunk_lines:
        if len(hunk_line) < 1:
            continue
        if hunk_line[0] == " ":
            result.append(hunk_line[1:] if hunk_line.endswith("\n") else hunk_line[1:] + "\n")
            old_pos += 1
        elif hunk_line[0] == "-":
            old_pos += 1
        elif hunk_line[0] == "+":
            result.append(hunk_line[1:] if hunk_line.endswith("\n") else hunk_line[1:] + "\n")
    result.extend(old_lines[old_pos:])
    return result


class ApplyPatchTool(BaseTool):
    """
    åº”ç”¨ Unified Diffï¼ˆeditblock/udiff å¼ç»†ç²’åº¦ç¼–è¾‘ï¼‰
    
    æ¥å—æ¨¡å‹è¾“å‡ºçš„æ ‡å‡† unified diff æ–‡æœ¬ï¼ŒæŒ‰ hunk ç²¾ç¡®åº”ç”¨ï¼Œä¾¿äºå®¡è®¡å’Œå›æ»šã€‚
    é‡‡ç”¨æ ‡å‡† unified diff ç¼–è¾‘èŒƒå¼ã€‚
    """

    def __init__(self):
        super().__init__(
            name="apply_patch",
            description="åº”ç”¨ unified diff åˆ°æ–‡ä»¶ã€‚è¾“å…¥ä¸ºæ ‡å‡† diff æ–‡æœ¬ï¼ˆ---/+++ æ–‡ä»¶è·¯å¾„ï¼Œ@@ hunkï¼Œ- åˆ é™¤è¡Œï¼Œ+ æ·»åŠ è¡Œï¼‰ã€‚è·¯å¾„ä¸ºç›¸å¯¹é¡¹ç›®æ ¹ã€‚"
        )

    async def execute(
        self,
        diff: str,
        base_path: Optional[str] = None
    ) -> ToolResult:
        """
        åº”ç”¨ diffã€‚
        
        Args:
            diff: unified diff å­—ç¬¦ä¸²ï¼ˆå¯å«å¤šä¸ªæ–‡ä»¶ï¼‰
            base_path: ç›¸å¯¹è·¯å¾„çš„åŸºå‡†ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ä»“åº“æ ¹
        """
        try:
            base = self.context.repo_path
            if base_path:
                base = self.resolve_path(base_path)
            parsed = _parse_unified_diff(diff)
            applied = []
            errors = []
            for file_info in parsed:
                rel_path = file_info["path"]
                full_path = base / rel_path
                if not full_path.exists() and not any(h[4] for h in file_info["hunks"] if any(l.startswith("+") for l in h[4])):
                    errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨ä¸”æ— æ–°å¢å†…å®¹: {rel_path}")
                    continue
                try:
                    if full_path.exists():
                        content = full_path.read_text(encoding="utf-8", errors="ignore")
                        old_lines = content.splitlines(keepends=True)
                        if not content.endswith("\n") and old_lines:
                            old_lines[-1] = old_lines[-1].rstrip("\n") + "\n"
                    else:
                        full_path.parent.mkdir(parents=True, exist_ok=True)
                        old_lines = []
                    for (old_start, old_count, new_start, new_count, hunk_lines) in file_info["hunks"]:
                        old_lines = _apply_hunk(old_lines, old_start, old_count, new_start, new_count, hunk_lines)
                    full_path.write_text("".join(old_lines), encoding="utf-8")
                    applied.append(rel_path)
                except Exception as e:
                    errors.append(f"{rel_path}: {e}")
            if errors and not applied:
                return ToolResult(success=False, content=None, error="; ".join(errors))
            return ToolResult(
                success=True,
                content=f"å·²åº”ç”¨ diff åˆ°: {', '.join(applied)}" + ("; é”™è¯¯: " + "; ".join(errors) if errors else ""),
                metadata={"applied": applied, "errors": errors if errors else None}
            )
        except Exception as e:
            return ToolResult(success=False, content=None, error=str(e))

    def get_function_schema(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "diff": {
                        "type": "string",
                        "description": "Unified diff å…¨æ–‡ï¼ˆåŒ…å« ---/+++ è·¯å¾„å’Œ @@ hunkï¼‰"
                    },
                    "base_path": {
                        "type": "string",
                        "description": "ç›¸å¯¹è·¯å¾„åŸºå‡†ï¼Œé»˜è®¤å½“å‰ä»“åº“æ ¹ã€‚ä½¿ç”¨ '.' è¡¨ç¤ºä»“åº“æ ¹"
                    }
                },
                "required": ["diff"]
            }
        }



# ========== æ™ºèƒ½ Diff ç¼–è¾‘å·¥å…·ï¼ˆæµå¼ï¼‰ ==========

class IntelligentDiffEditTool(StreamingEditTool):
    """
    æ™ºèƒ½ Diff ç¼–è¾‘å·¥å…·ï¼ˆæ”¯æŒæµå¼æ˜¾ç¤ºï¼‰
    
    åŠŸèƒ½ï¼š
    1. ç²¾ç¡®åŒ¹é… - ç›´æ¥æŸ¥æ‰¾æ›¿æ¢
    2. æ¨¡ç³ŠåŒ¹é… - ä½¿ç”¨ Levenshtein è·ç¦»
    3. æ™ºèƒ½å›é€€ - éªŒè¯å¤±è´¥æ—¶è‡ªåŠ¨å›æ»š
    4. æµå¼æ˜¾ç¤º - å®æ—¶æ˜¾ç¤ºç¼–è¾‘è¿‡ç¨‹
    
    ä¼˜åŠ¿ï¼š
    - ç²¾ç¡®åˆ°è¡Œï¼Œä¸éœ€è¦å®Œæ•´æ–‡ä»¶å†…å®¹
    - è‡ªåŠ¨å¤„ç†ç©ºç™½å·®å¼‚
    - æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
    - LSP éªŒè¯é›†æˆ
    """
    
    def __init__(self):
        super().__init__(
            name="intelligent_diff_edit",
            description="ä½¿ç”¨æ™ºèƒ½ Diff ç¼–è¾‘æ–‡ä»¶ï¼ˆç²¾ç¡®åˆ°è¡Œï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…å’Œè‡ªåŠ¨å›é€€ï¼‰"
        )
    
    async def execute(
        self,
        file_path: str,
        search_block: str,
        replace_block: str,
        fuzzy_match: bool = True,
        similarity_threshold: float = 0.8,
        verify: bool = True
    ) -> ToolResult:
        """
        æ‰§è¡Œæ™ºèƒ½ Diff ç¼–è¾‘
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            search_block: è¦æŸ¥æ‰¾çš„ä»£ç å—
            replace_block: æ›¿æ¢çš„ä»£ç å—
            fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
            verify: æ˜¯å¦ä½¿ç”¨ LSP éªŒè¯
        
        Returns:
            ToolResult
        """
        try:
            # è§£æè·¯å¾„
            path = self.resolve_path(file_path)
            
            if not path.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                )
            
            # è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾æœ€ä½³åŒ¹é…
            match_result = self._find_best_match(
                content,
                search_block,
                fuzzy_match,
                similarity_threshold
            )
            
            if not match_result:
                return ToolResult(
                    success=False,
                    content=None,
                    error="æœªæ‰¾åˆ°åŒ¹é…çš„ä»£ç å—"
                )
            
            match_start, match_end, similarity = match_result
            matched_text = content[match_start:match_end]
            
            # åº”ç”¨æ›¿æ¢
            new_content = (
                content[:match_start] +
                replace_block +
                content[match_end:]
            )
            
            # ç”Ÿæˆ Diff
            import difflib
            diff_lines = list(difflib.unified_diff(
                content.splitlines(keepends=True),
                new_content.splitlines(keepends=True),
                fromfile=f"a/{file_path}",
                tofile=f"b/{file_path}",
                lineterm=''
            ))
            
            diff_text = ''.join(diff_lines) if diff_lines else "No changes"
            
            # å†™å…¥æ–‡ä»¶
            with open(path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            # LSP éªŒè¯
            diagnostics = []
            if verify and self._should_verify(path):
                diagnostics = await self._verify_with_lsp(path)
                
                if diagnostics:
                    error_count = len([d for d in diagnostics if d.get('severity') == 1])
                    
                    if error_count > 0:
                        # æœ‰é”™è¯¯ï¼Œå›é€€
                        with open(path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        error_messages = [
                            f"Line {d.get('range', {}).get('start', {}).get('line', '?') + 1}: {d.get('message', 'Unknown')}"
                            for d in diagnostics if d.get('severity') == 1
                        ]
                        
                        return ToolResult(
                            success=False,
                            content=None,
                            error=f"éªŒè¯å¤±è´¥ï¼Œå·²å›é€€ã€‚{error_count} ä¸ªé”™è¯¯:\n" + "\n".join(error_messages[:5])
                        )
            
            # æ„å»ºç»“æœæ¶ˆæ¯
            result_message = f"âœ… æˆåŠŸç¼–è¾‘ {file_path}\n\n"
            result_message += f"ğŸ“Š åŒ¹é…ä¿¡æ¯:\n"
            result_message += f"  â€¢ ç›¸ä¼¼åº¦: {similarity:.1%}\n"
            result_message += f"  â€¢ åŒ¹é…ä½ç½®: {match_start}-{match_end}\n"
            result_message += f"  â€¢ åŒ¹é…å†…å®¹:\n```\n{matched_text[:200]}{'...' if len(matched_text) > 200 else ''}\n```\n\n"
            result_message += f"ğŸ“ å˜æ›´:\n```diff\n{diff_text}\n```"
            
            if diagnostics:
                warning_count = len([d for d in diagnostics if d.get('severity') == 2])
                if warning_count > 0:
                    result_message += f"\n\nâš ï¸  {warning_count} ä¸ªè­¦å‘Šï¼ˆå·²å¿½ç•¥ï¼‰"
            
            return ToolResult(
                success=True,
                content=result_message,
                metadata={
                    'file_path': str(path),
                    'similarity': similarity,
                    'match_start': match_start,
                    'match_end': match_end,
                    'diff': diff_text,
                    'verified': verify and self._should_verify(path),
                    'diagnostics': diagnostics
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    async def execute_streaming(
        self,
        file_path: str,
        search_block: str,
        replace_block: str,
        fuzzy_match: bool = True,
        similarity_threshold: float = 0.8,
        verify: bool = True
    ) -> AsyncGenerator[EditEvent, None]:
        """
        æµå¼æ‰§è¡Œæ™ºèƒ½ Diff ç¼–è¾‘
        
        å®æ—¶æ˜¾ç¤ºç¼–è¾‘è¿‡ç¨‹ï¼š
        1. åˆ†ææ–‡ä»¶
        2. æŸ¥æ‰¾åŒ¹é…
        3. åº”ç”¨ä¿®æ”¹
        4. éªŒè¯ä»£ç 
        
        Yields:
            EditEvent - ç¼–è¾‘äº‹ä»¶
        """
        try:
            # è§£æè·¯å¾„
            path = self.resolve_path(file_path)
            
            # äº‹ä»¶1: å¼€å§‹ç¼–è¾‘
            yield EditEvent(
                type=EditEvent.EDIT_START,
                data={
                    'file_path': file_path,
                    'action': 'intelligent_diff_edit'
                }
            )
            
            await asyncio.sleep(0.01)
            
            if not path.exists():
                yield EditEvent(
                    type=EditEvent.EDIT_ERROR,
                    data={
                        'error': f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                    }
                )
                return
            
            # äº‹ä»¶2: åˆ†ææ–‡ä»¶
            yield EditEvent(
                type=EditEvent.EDIT_ANALYZING,
                data={
                    'file_path': file_path,
                    'status': 'è¯»å–æ–‡ä»¶å†…å®¹'
                }
            )
            
            await asyncio.sleep(0.01)
            
            # è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            file_size = len(content)
            line_count = content.count('\n') + 1
            
            yield EditEvent(
                type=EditEvent.EDIT_ANALYZING,
                data={
                    'file_path': file_path,
                    'status': 'åˆ†æå®Œæˆ',
                    'size': file_size,
                    'lines': line_count
                }
            )
            
            await asyncio.sleep(0.01)
            
            # äº‹ä»¶3: æŸ¥æ‰¾åŒ¹é…
            yield EditEvent(
                type=EditEvent.EDIT_PLANNING,
                data={
                    'status': 'æŸ¥æ‰¾åŒ¹é…çš„ä»£ç å—',
                    'fuzzy_match': fuzzy_match,
                    'similarity_threshold': similarity_threshold
                }
            )
            
            await asyncio.sleep(0.02)
            
            # æŸ¥æ‰¾æœ€ä½³åŒ¹é…
            match_result = self._find_best_match(
                content,
                search_block,
                fuzzy_match,
                similarity_threshold
            )
            
            if not match_result:
                yield EditEvent(
                    type=EditEvent.EDIT_ERROR,
                    data={
                        'error': "æœªæ‰¾åˆ°åŒ¹é…çš„ä»£ç å—"
                    }
                )
                return
            
            match_start, match_end, similarity = match_result
            matched_text = content[match_start:match_end]
            
            # è®¡ç®—åŒ¹é…çš„è¡Œå·
            match_start_line = content[:match_start].count('\n') + 1
            match_end_line = content[:match_end].count('\n') + 1
            
            yield EditEvent(
                type=EditEvent.EDIT_PLANNING,
                data={
                    'status': 'æ‰¾åˆ°åŒ¹é…',
                    'similarity': similarity,
                    'match_start_line': match_start_line,
                    'match_end_line': match_end_line,
                    'matched_lines': match_end_line - match_start_line + 1
                }
            )
            
            await asyncio.sleep(0.01)
            
            # äº‹ä»¶4: åº”ç”¨ä¿®æ”¹
            yield EditEvent(
                type=EditEvent.EDIT_APPLYING,
                data={
                    'status': 'åº”ç”¨ä¿®æ”¹',
                    'old_size': len(matched_text),
                    'new_size': len(replace_block)
                }
            )
            
            await asyncio.sleep(0.01)
            
            # åº”ç”¨æ›¿æ¢
            new_content = (
                content[:match_start] +
                replace_block +
                content[match_end:]
            )
            
            # ç”Ÿæˆ Diff
            import difflib
            diff_lines = list(difflib.unified_diff(
                content.splitlines(keepends=True),
                new_content.splitlines(keepends=True),
                fromfile=f"a/{file_path}",
                tofile=f"b/{file_path}",
                lineterm=''
            ))
            
            diff_text = ''.join(diff_lines) if diff_lines else "No changes"
            
            # ç»Ÿè®¡å˜æ›´
            added_lines = len([l for l in diff_lines if l.startswith('+')])
            removed_lines = len([l for l in diff_lines if l.startswith('-')])
            
            yield EditEvent(
                type=EditEvent.EDIT_BLOCK,
                data={
                    'status': 'ç”Ÿæˆ Diff',
                    'added_lines': added_lines,
                    'removed_lines': removed_lines,
                    'diff_preview': diff_text[:500]  # åªæ˜¾ç¤ºå‰500å­—ç¬¦
                }
            )
            
            await asyncio.sleep(0.01)
            
            # å†™å…¥æ–‡ä»¶
            with open(path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            yield EditEvent(
                type=EditEvent.EDIT_APPLYING,
                data={
                    'status': 'æ–‡ä»¶å·²å†™å…¥'
                }
            )
            
            await asyncio.sleep(0.01)
            
            # äº‹ä»¶5: LSP éªŒè¯
            if verify and self._should_verify(path):
                yield EditEvent(
                    type=EditEvent.EDIT_VERIFYING,
                    data={
                        'status': 'ä½¿ç”¨ LSP éªŒè¯ä»£ç '
                    }
                )
                
                await asyncio.sleep(0.02)
                
                diagnostics = await self._verify_with_lsp(path)
                
                if diagnostics:
                    error_count = len([d for d in diagnostics if d.get('severity') == 1])
                    warning_count = len([d for d in diagnostics if d.get('severity') == 2])
                    
                    yield EditEvent(
                        type=EditEvent.EDIT_VERIFYING,
                        data={
                            'status': 'éªŒè¯å®Œæˆ',
                            'errors': error_count,
                            'warnings': warning_count
                        }
                    )
                    
                    await asyncio.sleep(0.01)
                    
                    if error_count > 0:
                        # æœ‰é”™è¯¯ï¼Œå›é€€
                        with open(path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        error_messages = [
                            f"Line {d.get('range', {}).get('start', {}).get('line', '?') + 1}: {d.get('message', 'Unknown')}"
                            for d in diagnostics if d.get('severity') == 1
                        ]
                        
                        yield EditEvent(
                            type=EditEvent.EDIT_ERROR,
                            data={
                                'error': f"éªŒè¯å¤±è´¥ï¼Œå·²å›é€€ã€‚{error_count} ä¸ªé”™è¯¯",
                                'error_messages': error_messages[:5]
                            }
                        )
                        return
                else:
                    yield EditEvent(
                        type=EditEvent.EDIT_VERIFYING,
                        data={
                            'status': 'éªŒè¯é€šè¿‡ï¼Œæ— é”™è¯¯'
                        }
                    )
                    
                    await asyncio.sleep(0.01)
            
            # äº‹ä»¶6: ç¼–è¾‘å®Œæˆ
            yield EditEvent(
                type=EditEvent.EDIT_COMPLETE,
                data={
                    'file_path': file_path,
                    'similarity': similarity,
                    'match_start_line': match_start_line,
                    'match_end_line': match_end_line,
                    'added_lines': added_lines,
                    'removed_lines': removed_lines,
                    'verified': verify and self._should_verify(path)
                }
            )
            
        except Exception as e:
            yield EditEvent(
                type=EditEvent.EDIT_ERROR,
                data={
                    'error': str(e)
                }
            )
    
    def _find_best_match(
        self,
        content: str,
        search_block: str,
        fuzzy_match: bool,
        similarity_threshold: float
    ) -> Optional[Tuple[int, int, float]]:
        """
        æŸ¥æ‰¾æœ€ä½³åŒ¹é…
        
        Returns:
            (match_start, match_end, similarity) æˆ– None
        """
        
        # 1. ç²¾ç¡®åŒ¹é…
        if search_block in content:
            start = content.index(search_block)
            end = start + len(search_block)
            return (start, end, 1.0)
        
        if not fuzzy_match:
            return None
        
        # 2. æ¨¡ç³ŠåŒ¹é…
        content_lines = content.split('\n')
        search_lines = search_block.split('\n')
        
        best_match = None
        best_similarity = 0.0
        
        # æ»‘åŠ¨çª—å£
        for i in range(len(content_lines) - len(search_lines) + 1):
            window = content_lines[i:i + len(search_lines)]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self._calculate_similarity(
                window,
                search_lines
            )
            
            if similarity > best_similarity and similarity >= similarity_threshold:
                best_similarity = similarity
                
                # è®¡ç®—å­—ç¬¦ä½ç½®
                start = sum(len(content_lines[j]) + 1 for j in range(i))
                end = start + sum(len(window[j]) + 1 for j in range(len(window))) - 1
                
                best_match = (start, end, similarity)
        
        return best_match
    
    def _calculate_similarity(
        self,
        lines1: List[str],
        lines2: List[str]
    ) -> float:
        """
        è®¡ç®—ä¸¤ç»„è¡Œçš„ç›¸ä¼¼åº¦
        
        ä½¿ç”¨ Levenshtein è·ç¦»
        """
        if len(lines1) != len(lines2):
            return 0.0
        
        total_similarity = 0.0
        
        for line1, line2 in zip(lines1, lines2):
            # å½’ä¸€åŒ–ç©ºç™½
            line1_norm = ' '.join(line1.split())
            line2_norm = ' '.join(line2.split())
            
            if line1_norm == line2_norm:
                total_similarity += 1.0
            else:
                # Levenshtein è·ç¦»
                max_len = max(len(line1_norm), len(line2_norm))
                if max_len == 0:
                    total_similarity += 1.0
                else:
                    distance = levenshtein(line1_norm, line2_norm)
                    total_similarity += 1.0 - (distance / max_len)
        
        return total_similarity / len(lines1)
    
    def _should_verify(self, path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥éªŒè¯æ–‡ä»¶"""
        code_extensions = {'.py', '.js', '.ts', '.jsx', '.tsx', '.go', '.rs'}
        return path.suffix in code_extensions
    
    async def _verify_with_lsp(self, file_path: Path) -> List[Dict]:
        """
        ä½¿ç”¨ LSP éªŒè¯ä»£ç 
        
        Returns:
            è¯Šæ–­ä¿¡æ¯åˆ—è¡¨ï¼ˆé”™è¯¯å’Œè­¦å‘Šï¼‰
        """
        try:
            from .lsp_tools import with_lsp_client
            import logging
            
            logger = logging.getLogger(__name__)
            
            result = await with_lsp_client(
                str(file_path),
                lambda client: client.diagnostics(str(file_path), wait_time=3.0)
            )
            
            diagnostics = result.get('items', [])
            logger.debug(f"LSPè¿”å›{len(diagnostics)}ä¸ªè¯Šæ–­ä¿¡æ¯")
            
            # åªè¿”å›é”™è¯¯å’Œè­¦å‘Š
            filtered = [
                d for d in diagnostics 
                if d.get('severity') in [1, 2]  # 1=Error, 2=Warning
            ]
            
            logger.debug(f"è¿‡æ»¤å{len(filtered)}ä¸ªé”™è¯¯/è­¦å‘Š")
            return filtered
        
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.debug(f"LSPéªŒè¯å¤±è´¥: {e}")
            return []
    
    def get_function_schema(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰"
                    },
                    "search_block": {
                        "type": "string",
                        "description": "è¦æŸ¥æ‰¾çš„ä»£ç å—"
                    },
                    "replace_block": {
                        "type": "string",
                        "description": "æ›¿æ¢çš„ä»£ç å—"
                    },
                    "fuzzy_match": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…ï¼ˆé»˜è®¤Trueï¼‰",
                        "default": True
                    },
                    "similarity_threshold": {
                        "type": "number",
                        "description": "ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼Œé»˜è®¤0.8ï¼‰",
                        "default": 0.8
                    },
                    "verify": {
                        "type": "boolean",
                        "description": "æ˜¯å¦ä½¿ç”¨LSPéªŒè¯ä»£ç ï¼ˆé»˜è®¤Trueï¼‰",
                        "default": True
                    }
                },
                "required": ["file_path", "search_block", "replace_block"]
            }
        }
