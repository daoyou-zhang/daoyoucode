"""
RepoMapå·¥å…· - ä»£ç åœ°å›¾ç”Ÿæˆ

åŸºäºŽdaoyouCodePilotçš„æœ€ä½³å®žçŽ°ï¼š
- Tree-sitterè§£æžä»£ç ç»“æž„
- PageRankç®—æ³•æ™ºèƒ½æŽ’åº
- ä¸ªæ€§åŒ–æƒé‡ï¼ˆå¯¹è¯æ–‡ä»¶Ã—50ï¼Œæåˆ°çš„æ ‡è¯†ç¬¦Ã—10ï¼‰
- ç¼“å­˜æœºåˆ¶ï¼ˆSQLite + mtimeæ£€æµ‹ï¼‰
- Tokené¢„ç®—æŽ§åˆ¶
"""

from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
import logging
import sqlite3
import json
from collections import defaultdict, namedtuple
import warnings

from .base import BaseTool, ToolResult

# å¿½ç•¥ tree_sitter çš„ FutureWarning
warnings.simplefilter("ignore", category=FutureWarning)

# å¯¼å…¥ grep_ast åº“
try:
    from grep_ast import filename_to_lang
    from grep_ast.tsl import USING_TSL_PACK, get_language, get_parser
    from pygments.lexers import guess_lexer_for_filename
    from pygments.token import Token
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False

logger = logging.getLogger(__name__)

# Tag æ•°æ®ç»“æž„
Tag = namedtuple("Tag", "rel_fname fname line name kind".split())


class RepoMapTool(BaseTool):
    """
    ç”Ÿæˆä»£ç ä»“åº“åœ°å›¾
    
    åŠŸèƒ½ï¼š
    - æå–å‡½æ•°ã€ç±»å®šä¹‰å’Œå¼•ç”¨å…³ç³»
    - PageRankæŽ’åºï¼ˆåŸºäºŽå¼•ç”¨å…³ç³»ï¼‰
    - ä¸ªæ€§åŒ–æƒé‡ï¼ˆå¯¹è¯æ–‡ä»¶ã€æåˆ°çš„æ ‡è¯†ç¬¦ï¼‰
    - ç¼“å­˜æœºåˆ¶ï¼ˆé¿å…é‡å¤è§£æžï¼‰
    - Tokené¢„ç®—æŽ§åˆ¶
    """
    
    # RepoMapå¯ä»¥ç¨å¾®é•¿ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒæ˜¯æ™ºèƒ½æŽ’åºçš„
    MAX_OUTPUT_CHARS = 10000
    MAX_OUTPUT_LINES = 1000
    
    def __init__(self):
        super().__init__(
            name="repo_map",
            description="ç”Ÿæˆä»£ç ä»“åº“åœ°å›¾ï¼Œæ™ºèƒ½æŽ’åºæœ€ç›¸å…³çš„ä»£ç å®šä¹‰"
        )
        self.cache_db = None
        self.graph = None
    
    def get_function_schema(self) -> Dict[str, Any]:
        """èŽ·å–Function Calling schema"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "ä»“åº“æ ¹ç›®å½•è·¯å¾„ã€‚å¿…é¡»ä½¿ç”¨ '.' è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦è·¯å¾„ï¼"
                    },
                    "chat_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "å¯¹è¯ä¸­æåˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæƒé‡Ã—50ï¼‰ã€‚å¦‚æžœä¸ºç©ºï¼Œä¼šè‡ªåŠ¨æ‰©å¤§tokené¢„ç®—ä»¥æä¾›æ›´å…¨é¢çš„é¡¹ç›®è§†å›¾"
                    },
                    "mentioned_idents": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "å¯¹è¯ä¸­æåˆ°çš„æ ‡è¯†ç¬¦åˆ—è¡¨ï¼ˆæƒé‡Ã—10ï¼‰"
                    },
                    "max_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§tokenæ•°é‡ï¼ˆé»˜è®¤3000ï¼‰ã€‚å¦‚æžœchat_filesä¸ºç©ºï¼Œä¼šè‡ªåŠ¨æ‰©å¤§åˆ°6000",
                        "default": 3000
                    },
                    "auto_scale": {
                        "type": "boolean",
                        "description": "æ˜¯å¦è‡ªåŠ¨è°ƒæ•´tokené¢„ç®—ï¼ˆé»˜è®¤trueï¼‰ã€‚å½“chat_filesä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨æ‰©å¤§é¢„ç®—ä»¥æä¾›æ›´å…¨é¢çš„è§†å›¾",
                        "default": True
                    }
                },
                "required": ["repo_path"]
            }
        }
        
    async def execute(
        self,
        repo_path: str,
        chat_files: Optional[List[str]] = None,
        mentioned_idents: Optional[List[str]] = None,
        max_tokens: int = 3000,
        auto_scale: bool = True
    ) -> ToolResult:
        """
        ç”ŸæˆRepoMap
        
        Args:
            repo_path: ä»“åº“æ ¹ç›®å½•
            chat_files: å¯¹è¯ä¸­çš„æ–‡ä»¶ï¼ˆæƒé‡Ã—50ï¼‰
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦ï¼ˆæƒé‡Ã—10ï¼‰
            max_tokens: æœ€å¤§tokenæ•°é‡
            auto_scale: æ˜¯å¦è‡ªåŠ¨è°ƒæ•´tokené¢„ç®—
            
        Returns:
            ToolResult
        """
        try:
            # ä½¿ç”¨ resolve_path è§£æžè·¯å¾„ï¼ˆä½¿ç”¨ ToolContextï¼‰
            repo_path_resolved = self.resolve_path(repo_path)
            
            if not repo_path_resolved.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}"
                )
            
            chat_files = chat_files or []
            mentioned_idents = mentioned_idents or []
            
            # æ™ºèƒ½è°ƒæ•´tokené¢„ç®—ï¼ˆå€Ÿé‰´aiderï¼‰
            original_max_tokens = max_tokens
            if auto_scale:
                if not chat_files or len(chat_files) == 0:
                    # æ²¡æœ‰å¯¹è¯æ–‡ä»¶ï¼Œæ‰©å¤§é¢„ç®—ï¼ˆ2å€ï¼Œæœ€å¤š6000ï¼‰
                    max_tokens = min(max_tokens * 2, 6000)
                    logger.info(
                        f"ðŸ” æ™ºèƒ½è°ƒæ•´: æ— å¯¹è¯æ–‡ä»¶ï¼Œæ‰©å¤§tokené¢„ç®— "
                        f"{original_max_tokens} â†’ {max_tokens} "
                        f"(æä¾›æ›´å…¨é¢çš„é¡¹ç›®è§†å›¾)"
                    )
                else:
                    logger.info(
                        f"ðŸ“ æ™ºèƒ½è°ƒæ•´: æœ‰ {len(chat_files)} ä¸ªå¯¹è¯æ–‡ä»¶ï¼Œ"
                        f"ä½¿ç”¨æ ‡å‡†tokené¢„ç®— {max_tokens}"
                    )
            
            # åˆå§‹åŒ–ç¼“å­˜
            self._init_cache(repo_path_resolved)
            
            # æ‰«æä»“åº“
            definitions = self._scan_repository(repo_path_resolved)
            
            # æž„å»ºå¼•ç”¨å›¾
            graph = self._build_reference_graph(definitions, repo_path_resolved)
            
            # PageRankæŽ’åº
            ranked = self._pagerank(
                graph,
                definitions,  # ä¼ é€’ definitions
                chat_files=chat_files,
                mentioned_idents=mentioned_idents
            )
            
            # ç”Ÿæˆåœ°å›¾ï¼ˆæŽ§åˆ¶tokenï¼‰
            repo_map = self._generate_map(
                ranked,
                definitions,
                max_tokens=max_tokens
            )
            
            # å…³é—­æ•°æ®åº“
            if self.cache_db:
                self.cache_db.close()
                self.cache_db = None
            
            return ToolResult(
                success=True,
                content=repo_map,
                metadata={
                    'repo_path': str(repo_path_resolved),
                    'file_count': len(definitions),
                    'definition_count': sum(len(defs) for defs in definitions.values()),
                    'max_tokens': max_tokens,
                    'original_max_tokens': original_max_tokens,
                    'auto_scaled': auto_scale and (max_tokens != original_max_tokens),
                    'chat_files_count': len(chat_files)
                }
            )
            
        except Exception as e:
            logger.error(f"ç”ŸæˆRepoMapå¤±è´¥: {e}", exc_info=True)
            # å…³é—­æ•°æ®åº“
            if self.cache_db:
                self.cache_db.close()
                self.cache_db = None
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    def _init_cache(self, repo_path: Path):
        """åˆå§‹åŒ–SQLiteç¼“å­˜"""
        cache_dir = repo_path / ".daoyoucode" / "cache"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        cache_file = cache_dir / "repomap.db"
        self.cache_db = sqlite3.connect(str(cache_file))
        
        # åˆ›å»ºè¡¨
        self.cache_db.execute("""
            CREATE TABLE IF NOT EXISTS definitions (
                file_path TEXT,
                mtime REAL,
                definitions TEXT,
                PRIMARY KEY (file_path)
            )
        """)
        self.cache_db.commit()
    
    def _scan_repository(self, repo_path: Path) -> Dict[str, List[Dict]]:
        """
        æ‰«æä»“åº“ï¼Œæå–å®šä¹‰
        
        Returns:
            {file_path: [definition, ...]}
        """
        definitions = {}
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        extensions = {".py", ".js", ".ts", ".jsx", ".tsx", ".java", ".go", ".rs"}
        
        for file_path in repo_path.rglob("*"):
            if not file_path.is_file():
                continue
            if file_path.suffix not in extensions:
                continue
            if self._should_ignore(file_path):
                continue
            
            # ðŸ†• subtree_only è¿‡æ»¤
            rel_path_str = str(file_path.relative_to(repo_path))
            if not self.context.should_include_path(rel_path_str):
                logger.debug(f"è·³è¿‡æ–‡ä»¶ï¼ˆsubtree_onlyï¼‰: {rel_path_str}")
                continue
            
            # ðŸ†• subtree_only è¿‡æ»¤
            rel_path_str = str(file_path.relative_to(repo_path))
            if not self.context.should_include_path(rel_path_str):
                logger.debug(f"è·³è¿‡æ–‡ä»¶ï¼ˆsubtree_onlyï¼‰: {rel_path_str}")
                continue
            
            # æ£€æŸ¥ç¼“å­˜
            rel_path = rel_path_str
            mtime = file_path.stat().st_mtime
            
            cached = self._get_cached_definitions(rel_path, mtime)
            if cached is not None:
                definitions[rel_path] = cached
                continue
            
            # è§£æžæ–‡ä»¶
            file_defs = self._parse_file(file_path)
            definitions[rel_path] = file_defs
            
            # ç¼“å­˜ç»“æžœ
            self._cache_definitions(rel_path, mtime, file_defs)
        
        return definitions
    
    def _should_ignore(self, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥æ–‡ä»¶
        
        å¿½ç•¥è§„åˆ™ï¼š
        1. å¸¸è§çš„æž„å»ºå’Œä¾èµ–ç›®å½•
        2. è¯»å– .daoyoucodeignore æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        """
        # å¸¸è§çš„æž„å»ºå’Œä¾èµ–ç›®å½•
        common_ignore = {
            ".git", "node_modules", "__pycache__", ".venv", "venv",
            "dist", "build", ".next", ".nuxt", "target"
        }
        
        for part in file_path.parts:
            if part in common_ignore:
                return True
        
        # TODO: è¯»å– .daoyoucodeignore æ–‡ä»¶
        # è¿™æ ·ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰å¿½ç•¥è§„åˆ™ï¼Œä¸éœ€è¦ç¡¬ç¼–ç 
        
        return False
    
    def _get_cached_definitions(self, file_path: str, mtime: float) -> Optional[List[Dict]]:
        """ä»Žç¼“å­˜èŽ·å–å®šä¹‰"""
        cursor = self.cache_db.execute(
            "SELECT mtime, definitions FROM definitions WHERE file_path = ?",
            (file_path,)
        )
        row = cursor.fetchone()
        
        if row is None:
            return None
        
        cached_mtime, cached_defs = row
        if cached_mtime != mtime:
            return None
        
        return json.loads(cached_defs)
    
    def _cache_definitions(self, file_path: str, mtime: float, definitions: List[Dict]):
        """ç¼“å­˜å®šä¹‰"""
        self.cache_db.execute(
            "INSERT OR REPLACE INTO definitions (file_path, mtime, definitions) VALUES (?, ?, ?)",
            (file_path, mtime, json.dumps(definitions))
        )
        self.cache_db.commit()
    
    def _parse_file(self, file_path: Path) -> List[Dict]:
        """
        è§£æžæ–‡ä»¶ï¼Œæå–å®šä¹‰å’Œå¼•ç”¨
        
        ä½¿ç”¨ Tree-sitter è§£æžï¼ˆå®Œæ•´å®žçŽ°ï¼‰
        """
        if not TREE_SITTER_AVAILABLE:
            logger.warning("Tree-sitter ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡ä»¶è§£æž")
            return []
        
        # èŽ·å–è¯­è¨€
        lang = filename_to_lang(str(file_path))
        if not lang:
            return []
        
        try:
            language = get_language(lang)
            parser = get_parser(lang)
        except Exception as err:
            logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {err}")
            return []
        
        # èŽ·å–æŸ¥è¯¢æ–‡ä»¶
        query_scm = self._get_scm_fname(lang)
        if not query_scm or not query_scm.exists():
            return []
        
        query_scm_content = query_scm.read_text()
        
        # è¯»å–ä»£ç 
        try:
            code = file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
        
        if not code:
            return []
        
        # è§£æžä»£ç 
        tree = parser.parse(bytes(code, "utf-8"))
        
        # è¿è¡Œæ ‡ç­¾æŸ¥è¯¢
        try:
            from tree_sitter import Query, QueryCursor
            query = Query(language, query_scm_content)
            cursor = QueryCursor(query)
            matches = cursor.matches(tree.root_node)
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ {file_path}: {e}")
            return []
        
        definitions = []
        saw = set()
        
        # å¤„ç†åŒ¹é…ç»“æžœ: [(pattern_index, {capture_name: [nodes]})]
        for pattern_index, captures_dict in matches:
            for tag, nodes in captures_dict.items():
                for node in nodes:
                    if tag.startswith("name.definition."):
                        kind = "def"
                    elif tag.startswith("name.reference."):
                        kind = "ref"
                    else:
                        continue
                    
                    saw.add(kind)
                    
                    # æå–ç±»åž‹ï¼ˆclassã€functionã€methodç­‰ï¼‰
                    type_name = tag.split(".")[-1]
                    
                    definitions.append({
                        "type": type_name,
                        "name": node.text.decode("utf-8"),
                        "line": node.start_point[0] + 1,
                        "kind": kind
                    })
        
        # å¦‚æžœåªæœ‰å®šä¹‰æ²¡æœ‰å¼•ç”¨ï¼Œä½¿ç”¨ Pygments è¡¥å……å¼•ç”¨
        if "ref" not in saw and "def" in saw:
            try:
                lexer = guess_lexer_for_filename(str(file_path), code)
                tokens = list(lexer.get_tokens(code))
                tokens = [token[1] for token in tokens if token[0] in Token.Name]
                
                for token in tokens:
                    definitions.append({
                        "type": "reference",
                        "name": token,
                        "line": -1,
                        "kind": "ref"
                    })
            except Exception:
                pass
        
        return definitions
    
    def _get_scm_fname(self, lang: str) -> Optional[Path]:
        """èŽ·å– Tree-sitter æŸ¥è¯¢æ–‡ä»¶è·¯å¾„"""
        # æŸ¥è¯¢æ–‡ä»¶ç›®å½•
        queries_dir = Path(__file__).parent / "queries"
        
        # ä¼˜å…ˆä½¿ç”¨ tree-sitter-language-pack
        if USING_TSL_PACK:
            subdir = "tree-sitter-language-pack"
            path = queries_dir / subdir / f"{lang}-tags.scm"
            if path.exists():
                return path
        
        # å›žé€€åˆ° tree-sitter-languages
        subdir = "tree-sitter-languages"
        path = queries_dir / subdir / f"{lang}-tags.scm"
        if path.exists():
            return path
        
        return None

    
    def _build_reference_graph(self, definitions: Dict[str, List[Dict]], repo_path: Path) -> Dict[str, Dict[str, float]]:
        """
        æž„å»ºå¼•ç”¨å›¾
        
        Returns:
            {node: {referenced_node: weight, ...}}
        """
        graph = defaultdict(lambda: defaultdict(float))
        
        # æž„å»ºæ ‡è¯†ç¬¦åˆ°æ–‡ä»¶çš„æ˜ å°„ï¼ˆåªåŒ…å«å®šä¹‰ï¼‰
        ident_to_files = defaultdict(set)
        for file_path, defs in definitions.items():
            for d in defs:
                # åªæ·»åŠ å®šä¹‰ï¼Œä¸æ·»åŠ å¼•ç”¨
                if d.get("kind") == "def":
                    ident_to_files[d["name"]].add(file_path)
        
        # æ‰«æå¼•ç”¨å…³ç³»
        for file_path, defs in definitions.items():
            # æ”¶é›†æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¼•ç”¨
            references_in_file = set()
            for d in defs:
                if d.get("kind") == "ref":
                    references_in_file.add(d["name"])
            
            # ä¸ºæ¯ä¸ªå¼•ç”¨æ·»åŠ è¾¹
            for ident in references_in_file:
                if ident in ident_to_files:
                    # æ–‡ä»¶å¼•ç”¨äº†è¿™ä¸ªæ ‡è¯†ç¬¦
                    for ref_file in ident_to_files[ident]:
                        if ref_file != file_path:
                            # æ·»åŠ è¾¹ï¼šfile_path -> ref_file
                            graph[file_path][ref_file] += 1.0
        
        return dict(graph)
    
    def _pagerank(
        self,
        graph: Dict[str, Dict[str, float]],
        definitions: Dict[str, List[Dict]],  # æ·»åŠ  definitions å‚æ•°
        chat_files: List[str],
        mentioned_idents: List[str],
        damping: float = 0.85,
        iterations: int = 20
    ) -> List[Tuple[str, float]]:
        """
        PageRankç®—æ³•æŽ’åº
        
        Args:
            graph: å¼•ç”¨å›¾
            chat_files: å¯¹è¯ä¸­çš„æ–‡ä»¶ï¼ˆæƒé‡Ã—50ï¼‰
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦ï¼ˆæƒé‡Ã—10ï¼‰
            damping: é˜»å°¼ç³»æ•°
            iterations: è¿­ä»£æ¬¡æ•°
            
        Returns:
            [(file_path, score), ...] æŒ‰åˆ†æ•°é™åº
        """
        # æ‰€æœ‰èŠ‚ç‚¹
        nodes = set(graph.keys())
        for targets in graph.values():
            nodes.update(targets.keys())
        
        if not nodes:
            return []
        
        # åˆå§‹åŒ–åˆ†æ•°
        scores = {node: 1.0 / len(nodes) for node in nodes}
        
        # ä¸ªæ€§åŒ–æƒé‡
        personalization = {}
        for node in nodes:
            weight = 1.0
            
            # å¯¹è¯æ–‡ä»¶æƒé‡Ã—50
            if node in chat_files:
                weight *= 50
            
            # æåˆ°çš„æ ‡è¯†ç¬¦æƒé‡Ã—10
            # æ£€æŸ¥ï¼š1) è·¯å¾„ç»„ä»¶  2) æ–‡ä»¶ä¸­çš„å®šä¹‰åç§°
            if mentioned_idents:
                # æ£€æŸ¥è·¯å¾„ç»„ä»¶ï¼ˆå¦‚ agents/llm/timeoutï¼‰
                path_components = set(Path(node).parts)
                basename_with_ext = Path(node).name
                basename_without_ext = Path(node).stem
                components_to_check = path_components.union({basename_with_ext, basename_without_ext})
                
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«æåˆ°çš„æ ‡è¯†ç¬¦
                matched_path = components_to_check.intersection(set(ident.lower() for ident in mentioned_idents))
                if matched_path:
                    weight *= 10
                
                # æ£€æŸ¥æ–‡ä»¶ä¸­çš„å®šä¹‰æ˜¯å¦åŒ…å«æåˆ°çš„æ ‡è¯†ç¬¦
                if node in definitions:
                    file_defs = definitions.get(node, [])
                    def_names = {d['name'].lower() for d in file_defs if d.get('kind') == 'def'}
                    mentioned_lower = {ident.lower() for ident in mentioned_idents}
                    
                    # ç²¾ç¡®åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…
                    if def_names.intersection(mentioned_lower):
                        weight *= 10
                    else:
                        # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚ 'timeout' åŒ¹é… 'TimeoutError'ï¼‰
                        for def_name in def_names:
                            for ident in mentioned_lower:
                                if ident in def_name or def_name in ident:
                                    weight *= 5  # éƒ¨åˆ†åŒ¹é…æƒé‡è¾ƒä½Ž
                                    break
            
            personalization[node] = weight
        
        # å½’ä¸€åŒ–
        total = sum(personalization.values())
        personalization = {k: v / total for k, v in personalization.items()}
        
        # PageRankè¿­ä»£
        for _ in range(iterations):
            new_scores = {}
            
            for node in nodes:
                # åŸºç¡€åˆ†æ•°ï¼ˆéšæœºè·³è½¬ï¼‰
                score = (1 - damping) * personalization.get(node, 1.0 / len(nodes))
                
                # æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„åˆ†æ•°
                for source, targets in graph.items():
                    if node in targets:
                        # source -> node
                        weight = targets[node]
                        out_weight = sum(targets.values())
                        score += damping * scores[source] * (weight / out_weight)
                
                new_scores[node] = score
            
            scores = new_scores
        
        # æŽ’åº
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return ranked
    
    def _generate_map(
        self,
        ranked: List[Tuple[str, float]],
        definitions: Dict[str, List[Dict]],
        max_tokens: int
    ) -> str:
        """
        ç”Ÿæˆä»£ç åœ°å›¾ï¼ˆæŽ§åˆ¶tokenæ•°é‡ï¼‰
        
        ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°æœ€ä¼˜tokenæ•°é‡
        """
        lines = []
        current_tokens = 0
        
        for file_path, score in ranked:
            if file_path not in definitions:
                continue
            
            file_defs = definitions[file_path]
            if not file_defs:
                continue
            
            # åªåŒ…å«å®šä¹‰ï¼Œä¸åŒ…å«å¼•ç”¨
            file_defs = [d for d in file_defs if d.get("kind") == "def"]
            if not file_defs:
                continue
            
            # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆç¡®ä¿è¿”å›žç›¸å¯¹äºŽ repo_path çš„è·¯å¾„ï¼‰
            normalized_path = self.normalize_path(file_path)
            
            # æ–‡ä»¶å¤´
            file_line = f"\n{normalized_path}:"
            file_tokens = len(file_line.split())
            
            if current_tokens + file_tokens > max_tokens:
                break
            
            lines.append(file_line)
            current_tokens += file_tokens
            
            # å®šä¹‰åˆ—è¡¨
            for d in file_defs:
                def_line = f"  {d.get('type', 'unknown')} {d['name']} (line {d['line']})"
                def_tokens = len(def_line.split())
                
                if current_tokens + def_tokens > max_tokens:
                    break
                
                lines.append(def_line)
                current_tokens += def_tokens
            
            if current_tokens >= max_tokens:
                break
        
        if not lines:
            return "ä»£ç åœ°å›¾ä¸ºç©º"
        
        # æ·»åŠ å¤´éƒ¨
        file_count = len([l for l in lines if l.startswith('\n')])
        header = f"# ä»£ç åœ°å›¾ (Top {file_count} æ–‡ä»¶)\n"
        return header + "\n".join(lines)


class GetRepoStructureTool(BaseTool):
    """
    èŽ·å–ä»“åº“ç»“æž„ï¼ˆç®€åŒ–ç‰ˆRepoMapï¼‰
    
    åªè¿”å›žæ–‡ä»¶æ ‘ï¼Œä¸åšæ™ºèƒ½æŽ’åº
    æ”¯æŒæ™ºèƒ½æ³¨é‡Šï¼Œå¸®åŠ©ç†è§£ç›®å½•å«ä¹‰
    """
    
    # ç›®å½•ç»“æž„ä¹Ÿéœ€è¦é™åˆ¶
    MAX_OUTPUT_LINES = 500
    MAX_OUTPUT_CHARS = 8000
    
    # æ™ºèƒ½æ³¨é‡Šæ˜ å°„
    DIRECTORY_ANNOTATIONS = {
        'backend': 'åŽç«¯ä»£ç ',
        'frontend': 'å‰ç«¯ä»£ç ',
        'src': 'æºä»£ç ',
        'lib': 'åº“æ–‡ä»¶',
        'tests': 'æµ‹è¯•ä»£ç ',
        'test': 'æµ‹è¯•ä»£ç ',
        'docs': 'æ–‡æ¡£',
        'doc': 'æ–‡æ¡£',
        'scripts': 'è„šæœ¬å·¥å…·',
        'script': 'è„šæœ¬å·¥å…·',
        'config': 'é…ç½®æ–‡ä»¶',
        'conf': 'é…ç½®æ–‡ä»¶',
        'agents': 'Agentç³»ç»Ÿ',
        'agent': 'Agentç³»ç»Ÿ',
        'tools': 'å·¥å…·æ¨¡å—',
        'tool': 'å·¥å…·æ¨¡å—',
        'memory': 'è®°å¿†ç³»ç»Ÿ',
        'orchestrators': 'ç¼–æŽ’å™¨',
        'orchestrator': 'ç¼–æŽ’å™¨',
        'llm': 'LLMå®¢æˆ·ç«¯',
        'cli': 'å‘½ä»¤è¡Œç•Œé¢',
        'api': 'APIæŽ¥å£',
        'models': 'æ•°æ®æ¨¡åž‹',
        'model': 'æ•°æ®æ¨¡åž‹',
        'utils': 'å·¥å…·å‡½æ•°',
        'util': 'å·¥å…·å‡½æ•°',
        'core': 'æ ¸å¿ƒç»„ä»¶',
        'common': 'å…¬å…±æ¨¡å—',
        'shared': 'å…±äº«æ¨¡å—',
        'components': 'ç»„ä»¶',
        'component': 'ç»„ä»¶',
        'services': 'æœåŠ¡',
        'service': 'æœåŠ¡',
        'controllers': 'æŽ§åˆ¶å™¨',
        'controller': 'æŽ§åˆ¶å™¨',
        'views': 'è§†å›¾',
        'view': 'è§†å›¾',
        'templates': 'æ¨¡æ¿',
        'template': 'æ¨¡æ¿',
        'static': 'é™æ€èµ„æº',
        'assets': 'èµ„æºæ–‡ä»¶',
        'public': 'å…¬å¼€èµ„æº',
        'private': 'ç§æœ‰æ¨¡å—',
        'internal': 'å†…éƒ¨æ¨¡å—',
        'external': 'å¤–éƒ¨æ¨¡å—',
        'vendor': 'ç¬¬ä¸‰æ–¹åº“',
        'node_modules': 'ä¾èµ–åŒ…',
        'dist': 'æž„å»ºäº§ç‰©',
        'build': 'æž„å»ºäº§ç‰©',
        'out': 'è¾“å‡ºç›®å½•',
        'bin': 'å¯æ‰§è¡Œæ–‡ä»¶',
        'pkg': 'åŒ…æ–‡ä»¶',
        'examples': 'ç¤ºä¾‹ä»£ç ',
        'example': 'ç¤ºä¾‹ä»£ç ',
        'demo': 'æ¼”ç¤ºä»£ç ',
        'plugins': 'æ’ä»¶',
        'plugin': 'æ’ä»¶',
        'extensions': 'æ‰©å±•',
        'extension': 'æ‰©å±•',
        'middleware': 'ä¸­é—´ä»¶',
        'handlers': 'å¤„ç†å™¨',
        'handler': 'å¤„ç†å™¨',
        'routes': 'è·¯ç”±',
        'route': 'è·¯ç”±',
        'database': 'æ•°æ®åº“',
        'db': 'æ•°æ®åº“',
        'migrations': 'æ•°æ®è¿ç§»',
        'migration': 'æ•°æ®è¿ç§»',
        'seeds': 'æ•°æ®ç§å­',
        'seed': 'æ•°æ®ç§å­',
        'fixtures': 'æµ‹è¯•æ•°æ®',
        'fixture': 'æµ‹è¯•æ•°æ®',
    }
    
    def __init__(self):
        super().__init__(
            name="get_repo_structure",
            description="èŽ·å–ä»“åº“ç›®å½•ç»“æž„ï¼Œæ”¯æŒæ™ºèƒ½æ³¨é‡Š"
        )
    
    def get_function_schema(self) -> Dict[str, Any]:
        """èŽ·å–Function Calling schema"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "ä»“åº“æ ¹ç›®å½•è·¯å¾„ã€‚å¿…é¡»ä½¿ç”¨ '.' è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦è·¯å¾„ï¼"
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "æœ€å¤§æ·±åº¦",
                        "default": 3
                    },
                    "show_files": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æ˜¾ç¤ºæ–‡ä»¶ï¼ˆå¦åˆ™åªæ˜¾ç¤ºç›®å½•ï¼‰",
                        "default": True
                    },
                    "annotate": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æ·»åŠ æ™ºèƒ½æ³¨é‡Šï¼ˆå¸®åŠ©ç†è§£ç›®å½•å«ä¹‰ï¼‰",
                        "default": True
                    }
                },
                "required": ["repo_path"]
            }
        }
    
    async def execute(
        self,
        repo_path: str,
        max_depth: int = 3,
        show_files: bool = True,
        annotate: bool = True
    ) -> ToolResult:
        """
        èŽ·å–ä»“åº“ç»“æž„
        
        Args:
            repo_path: ä»“åº“æ ¹ç›®å½•
            max_depth: æœ€å¤§æ·±åº¦
            show_files: æ˜¯å¦æ˜¾ç¤ºæ–‡ä»¶
            annotate: æ˜¯å¦æ·»åŠ æ™ºèƒ½æ³¨é‡Š
            
        Returns:
            ToolResult
        """
        try:
            repo_path = Path(repo_path).resolve()
            if not repo_path.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}"
                )
            
            lines = [f"{repo_path.name}/"]
            self._build_tree(repo_path, lines, "", max_depth, show_files, annotate)
            
            return ToolResult(
                success=True,
                content="\n".join(lines),
                metadata={
                    'repo_path': str(repo_path),
                    'max_depth': max_depth,
                    'show_files': show_files,
                    'annotate': annotate
                }
            )
            
        except Exception as e:
            logger.error(f"èŽ·å–ä»“åº“ç»“æž„å¤±è´¥: {e}", exc_info=True)
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    def _build_tree(
        self,
        path: Path,
        lines: List[str],
        prefix: str,
        max_depth: int,
        show_files: bool,
        annotate: bool,
        current_depth: int = 0
    ):
        """é€’å½’æž„å»ºæ ‘"""
        if current_depth >= max_depth:
            return
        
        # å¿½ç•¥çš„ç›®å½•
        ignore = {".git", "node_modules", "__pycache__", ".venv", "venv", "dist", "build"}
        
        try:
            items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
        except PermissionError:
            return
        
        for i, item in enumerate(items):
            if item.name in ignore:
                continue
            
            is_last = i == len(items) - 1
            current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            next_prefix = prefix + ("    " if is_last else "â”‚   ")
            
            if item.is_dir():
                # æ·»åŠ æ³¨é‡Š
                dir_display = f"{item.name}/"
                if annotate:
                    annotation = self._get_annotation(item.name)
                    if annotation:
                        dir_display = f"{item.name}/  # {annotation}"
                
                lines.append(f"{prefix}{current_prefix}{dir_display}")
                self._build_tree(item, lines, next_prefix, max_depth, show_files, annotate, current_depth + 1)
            elif show_files:
                lines.append(f"{prefix}{current_prefix}{item.name}")
    
    def _get_annotation(self, dir_name: str) -> Optional[str]:
        """èŽ·å–ç›®å½•æ³¨é‡Š"""
        dir_lower = dir_name.lower()
        
        # ç²¾ç¡®åŒ¹é…
        if dir_lower in self.DIRECTORY_ANNOTATIONS:
            return self.DIRECTORY_ANNOTATIONS[dir_lower]
        
        # éƒ¨åˆ†åŒ¹é…
        for pattern, annotation in self.DIRECTORY_ANNOTATIONS.items():
            if pattern in dir_lower:
                return annotation
        
        return None


class GetFileSymbolsTool(BaseTool):
    """
    èŽ·å–å•æ–‡ä»¶ç¬¦å·è¡¨ï¼ˆç±»/å‡½æ•°/æ–¹æ³•ç­‰ï¼ŒAST æ·±åº¦ï¼‰
    
    ä¸Ž repo_map äº’è¡¥ï¼šå·²çŸ¥æ–‡ä»¶æ—¶å¯ç›´æŽ¥å–è¯¥æ–‡ä»¶çš„å®šä¹‰åˆ—è¡¨ï¼Œä¾¿äºŽç²¾ç¡®ç†è§£ä»£ç ç»“æž„ã€‚
    ä½¿ç”¨ä¸Ž RepoMap ç›¸åŒçš„ Tree-sitter è§£æžã€‚
    """

    def __init__(self):
        super().__init__(
            name="get_file_symbols",
            description="èŽ·å–æŒ‡å®šæ–‡ä»¶ä¸­çš„ç¬¦å·å®šä¹‰ï¼ˆç±»ã€å‡½æ•°ã€æ–¹æ³•ç­‰ï¼‰åŠè¡Œå·ï¼ŒåŸºäºŽ AST è§£æžã€‚"
        )

    async def execute(self, file_path: str) -> ToolResult:
        try:
            path = self.resolve_path(file_path)
            if not path.exists() or not path.is_file():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶: {file_path}"
                )
            # å¤ç”¨ RepoMapTool çš„è§£æžé€»è¾‘
            repomap = RepoMapTool()
            repomap._context = self.context
            defs = repomap._parse_file(path)
            defs = [d for d in defs if d.get("kind") == "def"]
            if not defs:
                return ToolResult(
                    success=True,
                    content="è¯¥æ–‡ä»¶ä¸­æœªè§£æžåˆ°ç¬¦å·å®šä¹‰ï¼ˆæˆ–è¯­è¨€/è§£æžå™¨ä¸æ”¯æŒï¼‰",
                    metadata={"file_path": str(path), "count": 0}
                )
            lines = [f"  {d.get('type', '?')} {d['name']} (line {d['line']})" for d in defs]
            text = f"# {path.name}\n" + "\n".join(lines)
            return ToolResult(
                success=True,
                content=text,
                metadata={"file_path": str(path), "count": len(defs)}
            )
        except Exception as e:
            logger.exception("get_file_symbols å¤±è´¥")
            return ToolResult(success=False, content=None, error=str(e))

    def get_function_schema(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "ç›¸å¯¹é¡¹ç›®æ ¹çš„æ–‡ä»¶è·¯å¾„"
                    }
                },
                "required": ["file_path"]
            }
        }
