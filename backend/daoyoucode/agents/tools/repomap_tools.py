"""
RepoMapå·¥å…· - ä»£ç åœ°å›¾ç”Ÿæˆ

åŸºäºdaoyouCodePilotçš„æœ€ä½³å®ç°ï¼š
- Tree-sitterè§£æä»£ç ç»“æ„
- PageRankç®—æ³•æ™ºèƒ½æ’åº
- ä¸ªæ€§åŒ–æƒé‡ï¼ˆå¯¹è¯æ–‡ä»¶Ã—50ï¼Œæåˆ°çš„æ ‡è¯†ç¬¦Ã—10ï¼‰
- ç¼“å­˜æœºåˆ¶ï¼ˆSQLite + mtimeæ£€æµ‹ï¼‰
- Tokené¢„ç®—æ§åˆ¶
"""

from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
import logging
import json
import time
from collections import defaultdict, namedtuple
import warnings

from diskcache import Cache

from .base import BaseTool, ToolResult

# å¿½ç•¥ tree_sitter çš„ FutureWarning
warnings.simplefilter("ignore", category=FutureWarning)

# å¯¼å…¥ grep_ast åº“
try:
    from grep_ast import filename_to_lang
    from grep_ast.tsl import USING_TSL_PACK, get_language, get_parser
    from pygments.lexers import guess_lexer_for_filename
    from pygments.token import Token
    TREE_SITTER_AVAILABLE = True
except ImportError:
    TREE_SITTER_AVAILABLE = False

logger = logging.getLogger(__name__)

# Tag æ•°æ®ç»“æ„
Tag = namedtuple("Tag", "rel_fname fname line name kind".split())


class RepoMapTool(BaseTool):
    """
    ç”Ÿæˆä»£ç ä»“åº“åœ°å›¾
    
    åŠŸèƒ½ï¼š
    - æå–å‡½æ•°ã€ç±»å®šä¹‰å’Œå¼•ç”¨å…³ç³»
    - PageRankæ’åºï¼ˆåŸºäºå¼•ç”¨å…³ç³»ï¼‰
    - ä¸ªæ€§åŒ–æƒé‡ï¼ˆå¯¹è¯æ–‡ä»¶ã€æåˆ°çš„æ ‡è¯†ç¬¦ï¼‰
    - ç¼“å­˜æœºåˆ¶ï¼ˆé¿å…é‡å¤è§£æï¼‰
    - Tokené¢„ç®—æ§åˆ¶
    
    ğŸ†• å…¬å¼€APIï¼ˆä¾›codebase_indexç­‰å¤–éƒ¨æ¨¡å—ä½¿ç”¨ï¼‰ï¼š
    - get_definitions(): è·å–ä»£ç å®šä¹‰
    - get_reference_graph(): è·å–å¼•ç”¨å›¾
    - get_pagerank_scores(): è·å–PageRankåˆ†æ•°
    """
    
    # RepoMapå¯ä»¥ç¨å¾®é•¿ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒæ˜¯æ™ºèƒ½æ’åºçš„
    MAX_OUTPUT_CHARS = 10000
    MAX_OUTPUT_LINES = 1000
    
    def __init__(self):
        super().__init__(
            name="repo_map",
            description="ç”Ÿæˆä»£ç ä»“åº“åœ°å›¾ï¼Œæ™ºèƒ½æ’åºæœ€ç›¸å…³çš„ä»£ç å®šä¹‰"
        )
        # ğŸ”¥ ç¬¬3å±‚ï¼šæ–‡ä»¶çº§ç¼“å­˜ï¼ˆdiskcacheï¼‰
        self.file_cache = None
        
        # ğŸ”¥ ç¬¬2å±‚ï¼šå†…å­˜çº§ç¼“å­˜ï¼ˆdefinitions + graphï¼‰
        self.definitions_cache = None
        self.graph_cache = None
        self.cache_timestamp = None
        self.cached_repo_path = None
        
        # ğŸ”¥ ç¬¬1å±‚ï¼šç»“æœçº§ç¼“å­˜ï¼ˆmapç»“æœï¼‰
        self.map_cache = {}  # {cache_key: (result, timestamp)}
        self.map_cache_ttl = 300  # 5åˆ†é’Ÿè¿‡æœŸ
        
        self.graph = None
        self._last_definitions = None  # ğŸ†• ä¿å­˜æœ€åä¸€æ¬¡çš„definitions
        
        # ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            'result_hits': 0,
            'result_misses': 0,
            'memory_hits': 0,
            'memory_misses': 0,
            'file_hits': 0,
            'file_misses': 0
        }
    
    def get_function_schema(self) -> Dict[str, Any]:
        """è·å–Function Calling schema"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "ä»“åº“æ ¹ç›®å½•è·¯å¾„ã€‚å¿…é¡»ä½¿ç”¨ '.' è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦è·¯å¾„ï¼"
                    },
                    "chat_files": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "å¯¹è¯ä¸­æåˆ°çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæƒé‡Ã—50ï¼‰ã€‚å¦‚æœä¸ºç©ºï¼Œä¼šè‡ªåŠ¨æ‰©å¤§tokené¢„ç®—ä»¥æä¾›æ›´å…¨é¢çš„é¡¹ç›®è§†å›¾"
                    },
                    "mentioned_idents": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "å¯¹è¯ä¸­æåˆ°çš„æ ‡è¯†ç¬¦åˆ—è¡¨ï¼ˆæƒé‡Ã—10ï¼‰"
                    },
                    "max_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§tokenæ•°é‡ï¼ˆé»˜è®¤3000ï¼‰ã€‚å¦‚æœchat_filesä¸ºç©ºï¼Œä¼šè‡ªåŠ¨æ‰©å¤§åˆ°6000",
                        "default": 3000
                    },
                    "auto_scale": {
                        "type": "boolean",
                        "description": "æ˜¯å¦è‡ªåŠ¨è°ƒæ•´tokené¢„ç®—ï¼ˆé»˜è®¤trueï¼‰ã€‚å½“chat_filesä¸ºç©ºæ—¶ï¼Œè‡ªåŠ¨æ‰©å¤§é¢„ç®—ä»¥æä¾›æ›´å…¨é¢çš„è§†å›¾",
                        "default": True
                    },
                    "enable_lsp": {
                        "type": "boolean",
                        "description": "æ˜¯å¦å¯ç”¨LSPå¢å¼ºï¼ˆé»˜è®¤trueï¼‰ã€‚å¯ç”¨åä¼šæ˜¾ç¤ºç±»å‹ç­¾åå’Œå¼•ç”¨è®¡æ•°",
                        "default": True
                    }
                },
                "required": ["repo_path"]
            }
        }
    
    # ========== ğŸ†• å…¬å¼€APIï¼ˆä¾›å¤–éƒ¨æ¨¡å—ä½¿ç”¨ï¼‰==========
    
    def get_definitions(
        self,
        repo_path: str,
        use_cache: bool = True
    ) -> Dict[str, List[Dict]]:
        """
        è·å–ä»£ç å®šä¹‰ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        Returns:
            {
                "backend/agents/core/agent.py": [
                    {
                        "type": "class",
                        "name": "BaseAgent",
                        "line": 50,
                        "end_line": 150,
                        "kind": "def",
                        "parent": None,
                        "scope": "global"
                    },
                    ...
                ]
            }
        """
        repo_path_resolved = self.resolve_path(repo_path)
        
        if not repo_path_resolved.exists():
            logger.warning(f"ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}")
            return {}
        
        if use_cache:
            self._init_cache(repo_path_resolved)
        
        definitions = self._scan_repository(repo_path_resolved)
        
        # ğŸ†• è®¡ç®—end_lineï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        definitions = self._compute_end_lines(definitions, repo_path_resolved)
        
        # ä¿å­˜ä»¥ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self._last_definitions = definitions
        
        return definitions
    
    def get_reference_graph(
        self,
        repo_path: str,
        definitions: Optional[Dict[str, List[Dict]]] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        è·å–å¼•ç”¨å›¾ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            definitions: ä»£ç å®šä¹‰ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨è·å–ï¼‰
        
        Returns:
            {
                "file_a.py": {
                    "file_b.py": 3.0,  # file_aå¼•ç”¨file_b 3æ¬¡
                    "file_c.py": 1.0
                }
            }
        """
        repo_path_resolved = self.resolve_path(repo_path)
        
        if definitions is None:
            definitions = self.get_definitions(repo_path)
        
        return self._build_reference_graph(definitions, repo_path_resolved)
    
    def get_pagerank_scores(
        self,
        repo_path: str,
        reference_graph: Optional[Dict] = None,
        definitions: Optional[Dict] = None,
        chat_files: Optional[List[str]] = None,
        mentioned_idents: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        è·å–PageRankåˆ†æ•°ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            reference_graph: å¼•ç”¨å›¾ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨è·å–ï¼‰
            definitions: ä»£ç å®šä¹‰ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨è·å–ï¼‰
            chat_files: ç„¦ç‚¹æ–‡ä»¶
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦
        
        Returns:
            {
                "file_a.py": 0.85,
                "file_b.py": 0.65,
                ...
            }
        """
        if definitions is None:
            definitions = self.get_definitions(repo_path)
        
        if reference_graph is None:
            reference_graph = self.get_reference_graph(repo_path, definitions)
        
        ranked = self._pagerank(
            reference_graph,
            definitions,
            chat_files or [],
            mentioned_idents or []
        )
        
        return dict(ranked)
    
    # ========== ç§æœ‰æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰==========
        
    async def execute(
        self,
        repo_path: str,
        chat_files: Optional[List[str]] = None,
        mentioned_idents: Optional[List[str]] = None,
        max_tokens: int = 3000,
        auto_scale: bool = True,
        enable_lsp: bool = True  # ğŸ”¥ æ–°å¢ï¼šé»˜è®¤å¯ç”¨LSP
    ) -> ToolResult:
        """
        ç”ŸæˆRepoMap
        
        Args:
            repo_path: ä»“åº“æ ¹ç›®å½•
            chat_files: å¯¹è¯ä¸­çš„æ–‡ä»¶ï¼ˆæƒé‡Ã—50ï¼‰
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦ï¼ˆæƒé‡Ã—10ï¼‰
            max_tokens: æœ€å¤§tokenæ•°é‡
            auto_scale: æ˜¯å¦è‡ªåŠ¨è°ƒæ•´tokené¢„ç®—
            enable_lsp: æ˜¯å¦å¯ç”¨LSPå¢å¼ºï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            ToolResult
        """
        try:
            # ä½¿ç”¨ resolve_path è§£æè·¯å¾„ï¼ˆä½¿ç”¨ ToolContextï¼‰
            repo_path_resolved = self.resolve_path(repo_path)
            
            if not repo_path_resolved.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}"
                )
            
            chat_files = chat_files or []
            mentioned_idents = mentioned_idents or []
            
            # æ™ºèƒ½è°ƒæ•´tokené¢„ç®—
            original_max_tokens = max_tokens
            if auto_scale:
                if not chat_files or len(chat_files) == 0:
                    # æ²¡æœ‰å¯¹è¯æ–‡ä»¶ï¼Œæ‰©å¤§é¢„ç®—ï¼ˆ2å€ï¼Œæœ€å¤š6000ï¼‰
                    max_tokens = min(max_tokens * 2, 6000)
                    logger.info(
                        f"ğŸ” æ™ºèƒ½è°ƒæ•´: æ— å¯¹è¯æ–‡ä»¶ï¼Œæ‰©å¤§tokené¢„ç®— "
                        f"{original_max_tokens} â†’ {max_tokens} "
                        f"(æä¾›æ›´å…¨é¢çš„é¡¹ç›®è§†å›¾)"
                    )
                else:
                    logger.info(
                        f"ğŸ“ æ™ºèƒ½è°ƒæ•´: æœ‰ {len(chat_files)} ä¸ªå¯¹è¯æ–‡ä»¶ï¼Œ"
                        f"ä½¿ç”¨æ ‡å‡†tokené¢„ç®— {max_tokens}"
                    )
            
            # ğŸ”¥ ç¬¬1å±‚ï¼šæ£€æŸ¥ç»“æœçº§ç¼“å­˜
            cache_key = self._make_cache_key(chat_files, mentioned_idents, max_tokens)
            
            if cache_key in self.map_cache:
                cached_result, timestamp = self.map_cache[cache_key]
                if time.time() - timestamp < self.map_cache_ttl:
                    self.cache_stats['result_hits'] += 1
                    logger.info(f"âœ… å‘½ä¸­ç»“æœçº§ç¼“å­˜ (0.001ç§’) | ç»Ÿè®¡: {self._format_cache_stats()}")
                    return cached_result
            
            self.cache_stats['result_misses'] += 1
            
            # åˆå§‹åŒ–æ–‡ä»¶çº§ç¼“å­˜
            self._init_cache(repo_path_resolved)
            
            # ğŸ”¥ ç¬¬2å±‚ï¼šæ£€æŸ¥å†…å­˜çº§ç¼“å­˜
            files_changed = self._check_files_changed(repo_path_resolved)
            
            if not files_changed and self.definitions_cache and self.cached_repo_path == str(repo_path_resolved):
                self.cache_stats['memory_hits'] += 1
                logger.info(f"âœ… å‘½ä¸­å†…å­˜çº§ç¼“å­˜ï¼Œè·³è¿‡æ‰«æ (0.1ç§’) | ç»Ÿè®¡: {self._format_cache_stats()}")
                definitions = self.definitions_cache
                graph = self.graph_cache
            else:
                self.cache_stats['memory_misses'] += 1
                
                # ğŸ”¥ ç¬¬3å±‚ï¼šæ‰«æä»“åº“ï¼ˆä½¿ç”¨æ–‡ä»¶çº§ç¼“å­˜ + å¢é‡æ›´æ–°ï¼‰
                scan_start = time.time()
                definitions, changed_files = self._scan_repository_incremental(repo_path_resolved)
                scan_time = time.time() - scan_start
                
                # ğŸ”¥ å¢é‡æ›´æ–°å¼•ç”¨å›¾
                graph_start = time.time()
                if changed_files and self.graph_cache:
                    # æœ‰æ”¹åŠ¨ä¸”æœ‰ç¼“å­˜ï¼Œå¢é‡æ›´æ–°
                    graph = self._update_reference_graph_incremental(
                        self.graph_cache,
                        definitions,
                        changed_files,
                        repo_path_resolved
                    )
                    logger.info(f"ğŸ”„ å¢é‡æ›´æ–°å¼•ç”¨å›¾: {len(changed_files)} ä¸ªæ–‡ä»¶")
                    
                    # ğŸ”¥ æ¸…é™¤ç»“æœçº§ç¼“å­˜ï¼ˆå› ä¸º RepoMap å·²æ”¹å˜ï¼‰
                    if self.map_cache:
                        old_cache_size = len(self.map_cache)
                        self.map_cache.clear()
                        logger.info(f"ğŸ—‘ï¸  æ¸…é™¤ç»“æœçº§ç¼“å­˜: {old_cache_size} ä¸ªæ¡ç›®ï¼ˆå› ä¸ºæ–‡ä»¶æ”¹åŠ¨ï¼‰")
                else:
                    # é¦–æ¬¡è¿è¡Œæˆ–å…¨é‡æ›´æ–°
                    graph = self._build_reference_graph(definitions, repo_path_resolved)
                
                graph_time = time.time() - graph_start
                
                logger.info(
                    f"ğŸ” æ‰«æå®Œæˆ: {len(definitions)} ä¸ªæ–‡ä»¶ "
                    f"(æ‰«æ {scan_time:.2f}ç§’, æ„å›¾ {graph_time:.2f}ç§’) | "
                    f"ç»Ÿè®¡: {self._format_cache_stats()}"
                )
                
                # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
                self.definitions_cache = definitions
                self.graph_cache = graph
                self.cache_timestamp = time.time()
                self.cached_repo_path = str(repo_path_resolved)
            
            # PageRankæ’åº
            ranked = self._pagerank(
                graph,
                definitions,
                chat_files=chat_files,
                mentioned_idents=mentioned_idents
            )
            
            # ğŸ”¥ LSPå¢å¼ºï¼šä¸ºtop-kå®šä¹‰æ·»åŠ ç±»å‹ä¿¡æ¯
            if enable_lsp:
                await self._enhance_with_lsp(ranked, definitions, repo_path_resolved)
            
            # ç”Ÿæˆåœ°å›¾ï¼ˆæ§åˆ¶tokenï¼‰
            repo_map = self._generate_map(
                ranked,
                definitions,
                max_tokens=max_tokens,
                enable_lsp=enable_lsp
            )
            
            # æ„å»ºç»“æœ
            result = ToolResult(
                success=True,
                content=repo_map,
                metadata={
                    'repo_path': str(repo_path_resolved),
                    'file_count': len(definitions),
                    'definition_count': sum(len(defs) for defs in definitions.values()),
                    'max_tokens': max_tokens,
                    'original_max_tokens': original_max_tokens,
                    'auto_scaled': auto_scale and (max_tokens != original_max_tokens),
                    'chat_files_count': len(chat_files),
                    'lsp_enabled': enable_lsp,
                    'cache_stats': self.cache_stats.copy()
                }
            )
            
            # ğŸ”¥ ä¿å­˜åˆ°ç»“æœçº§ç¼“å­˜
            self.map_cache[cache_key] = (result, time.time())
            
            return result
            
        except Exception as e:
            logger.error(f"ç”ŸæˆRepoMapå¤±è´¥: {e}", exc_info=True)
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    def _init_cache(self, repo_path: Path):
        """åˆå§‹åŒ– diskcache ç¼“å­˜"""
        cache_dir = repo_path / ".daoyoucode" / "cache" / "repomap"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨ diskcacheï¼ˆè‡ªåŠ¨ç®¡ç† SQLiteï¼‰
        self.file_cache = Cache(str(cache_dir))
    
    def _make_cache_key(
        self,
        chat_files: List[str],
        mentioned_idents: List[str],
        max_tokens: int
    ) -> Tuple:
        """ç”Ÿæˆç»“æœçº§ç¼“å­˜é”®"""
        return (
            tuple(sorted(chat_files or [])),
            tuple(sorted(mentioned_idents or [])),
            max_tokens
        )
    
    def _check_files_changed(self, repo_path: Path) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¹åŠ¨ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
        
        ç­–ç•¥ï¼š
        1. æ£€æŸ¥ .git/index çš„ mtimeï¼ˆæœ€å¿«ï¼‰
        2. é‡‡æ ·æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çš„ mtimeï¼ˆè¾ƒå¿«ï¼‰
        """
        if not self.cache_timestamp:
            return True
        
        # æ–¹æ³•1ï¼šæ£€æŸ¥ .git/index çš„ mtimeï¼ˆæœ€å¿«ï¼‰
        git_index = repo_path / ".git" / "index"
        if git_index.exists():
            index_mtime = git_index.stat().st_mtime
            if index_mtime > self.cache_timestamp:
                logger.info("ğŸ”„ æ£€æµ‹åˆ° Git æ”¹åŠ¨ï¼Œæ¸…é™¤å†…å­˜ç¼“å­˜")
                return True
        
        # æ–¹æ³•2ï¼šé‡‡æ ·æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çš„ mtimeï¼ˆè¾ƒå¿«ï¼‰
        if self.definitions_cache:
            # é‡‡æ ·æ£€æŸ¥å‰10ä¸ªæ–‡ä»¶
            sample_files = list(self.definitions_cache.keys())[:10]
            for file_path in sample_files:
                full_path = repo_path / file_path
                if full_path.exists():
                    file_mtime = full_path.stat().st_mtime
                    if file_mtime > self.cache_timestamp:
                        logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ–‡ä»¶æ”¹åŠ¨: {file_path}")
                        return True
        
        return False
    
    def _format_cache_stats(self) -> str:
        """æ ¼å¼åŒ–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.cache_stats
        
        # è®¡ç®—å‘½ä¸­ç‡
        result_total = stats['result_hits'] + stats['result_misses']
        memory_total = stats['memory_hits'] + stats['memory_misses']
        file_total = stats['file_hits'] + stats['file_misses']
        
        result_rate = stats['result_hits'] / result_total if result_total > 0 else 0
        memory_rate = stats['memory_hits'] / memory_total if memory_total > 0 else 0
        file_rate = stats['file_hits'] / file_total if file_total > 0 else 0
        
        return (
            f"ç»“æœçº§ {result_rate:.0%} ({stats['result_hits']}/{result_total}), "
            f"å†…å­˜çº§ {memory_rate:.0%} ({stats['memory_hits']}/{memory_total}), "
            f"æ–‡ä»¶çº§ {file_rate:.0%} ({stats['file_hits']}/{file_total})"
        )
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆå…¬å¼€APIï¼‰"""
        stats = self.cache_stats.copy()
        
        # è®¡ç®—å‘½ä¸­ç‡
        result_total = stats['result_hits'] + stats['result_misses']
        memory_total = stats['memory_hits'] + stats['memory_misses']
        file_total = stats['file_hits'] + stats['file_misses']
        
        stats['result_hit_rate'] = stats['result_hits'] / result_total if result_total > 0 else 0
        stats['memory_hit_rate'] = stats['memory_hits'] / memory_total if memory_total > 0 else 0
        stats['file_hit_rate'] = stats['file_hits'] / file_total if file_total > 0 else 0
        
        return stats
    
    def _scan_repository(self, repo_path: Path) -> Dict[str, List[Dict]]:
        """
        æ‰«æä»“åº“ï¼Œæå–å®šä¹‰ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
        
        Returns:
            {file_path: [definition, ...]}
        """
        definitions, _ = self._scan_repository_incremental(repo_path)
        return definitions
    
    def _scan_repository_incremental(self, repo_path: Path) -> Tuple[Dict[str, List[Dict]], List[str]]:
        """
        å¢é‡æ‰«æä»“åº“ï¼Œæå–å®šä¹‰
        
        Returns:
            (definitions, changed_files)
            - definitions: {file_path: [definition, ...]}
            - changed_files: [æ”¹åŠ¨çš„æ–‡ä»¶åˆ—è¡¨]
        """
        definitions = {}
        changed_files = []
        unchanged_files = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        extensions = {".py", ".js", ".ts", ".jsx", ".tsx", ".java", ".go", ".rs"}
        
        for file_path in repo_path.rglob("*"):
            if not file_path.is_file():
                continue
            if file_path.suffix not in extensions:
                continue
            if self._should_ignore(file_path):
                continue
            
            # ğŸ†• subtree_only è¿‡æ»¤
            rel_path_str = str(file_path.relative_to(repo_path))
            if not self.context.should_include_path(rel_path_str):
                logger.debug(f"è·³è¿‡æ–‡ä»¶ï¼ˆsubtree_onlyï¼‰: {rel_path_str}")
                continue
            
            # æ£€æŸ¥ç¼“å­˜
            rel_path = rel_path_str
            mtime = file_path.stat().st_mtime
            
            cached = self._get_cached_definitions(rel_path, mtime)
            if cached is not None:
                # ğŸ”¥ å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨
                definitions[rel_path] = cached
                unchanged_files.append(rel_path)
                continue
            
            # ğŸ”¥ æœªå‘½ä¸­ç¼“å­˜ï¼Œéœ€è¦é‡æ–°è§£æ
            changed_files.append(rel_path)
            file_defs = self._parse_file(file_path)
            definitions[rel_path] = file_defs
            
            # ç¼“å­˜ç»“æœ
            self._cache_definitions(rel_path, mtime, file_defs)
        
        # ğŸ”¥ å¢é‡æ›´æ–°æ—¥å¿—
        if changed_files:
            logger.info(
                f"ğŸ”„ å¢é‡æ›´æ–°: {len(changed_files)} ä¸ªæ–‡ä»¶æ”¹åŠ¨, "
                f"{len(unchanged_files)} ä¸ªæ–‡ä»¶å¤ç”¨ç¼“å­˜"
            )
        else:
            logger.info(f"âœ… å…¨éƒ¨å‘½ä¸­ç¼“å­˜: {len(unchanged_files)} ä¸ªæ–‡ä»¶")
        
        return definitions, changed_files
    
    def _should_ignore(self, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥æ–‡ä»¶
        
        å¿½ç•¥è§„åˆ™ï¼š
        1. å¸¸è§çš„æ„å»ºå’Œä¾èµ–ç›®å½•
        2. è¯»å– .daoyoucodeignore æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        # å¸¸è§çš„æ„å»ºå’Œä¾èµ–ç›®å½•
        common_ignore = {
            ".git", "node_modules", "__pycache__", ".venv", "venv",
            "dist", "build", ".next", ".nuxt", "target"
        }
        
        for part in file_path.parts:
            if part in common_ignore:
                return True
        
        # TODO: è¯»å– .daoyoucodeignore æ–‡ä»¶
        # è¿™æ ·ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰å¿½ç•¥è§„åˆ™ï¼Œä¸éœ€è¦ç¡¬ç¼–ç 
        
        return False
    
    def _get_cached_definitions(self, file_path: str, mtime: float) -> Optional[List[Dict]]:
        """ä»ç¼“å­˜è·å–å®šä¹‰ï¼ˆä½¿ç”¨ diskcacheï¼‰"""
        val = self.file_cache.get(file_path)
        
        if val is not None and val.get("mtime") == mtime:
            self.cache_stats['file_hits'] += 1
            return val["data"]
        
        self.cache_stats['file_misses'] += 1
        return None
    
    def _cache_definitions(self, file_path: str, mtime: float, definitions: List[Dict]):
        """ç¼“å­˜å®šä¹‰ï¼ˆä½¿ç”¨ diskcacheï¼‰"""
        self.file_cache[file_path] = {
            "mtime": mtime,
            "data": definitions
        }
    
    def _parse_file(self, file_path: Path) -> List[Dict]:
        """
        è§£ææ–‡ä»¶ï¼Œæå–å®šä¹‰å’Œå¼•ç”¨
        
        ä½¿ç”¨ Tree-sitter è§£æï¼ˆå®Œæ•´å®ç°ï¼‰
        """
        if not TREE_SITTER_AVAILABLE:
            logger.warning("Tree-sitter ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡ä»¶è§£æ")
            return []
        
        # è·å–è¯­è¨€
        lang = filename_to_lang(str(file_path))
        if not lang:
            return []
        
        try:
            language = get_language(lang)
            parser = get_parser(lang)
        except Exception as err:
            logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {err}")
            return []
        
        # è·å–æŸ¥è¯¢æ–‡ä»¶
        query_scm = self._get_scm_fname(lang)
        if not query_scm or not query_scm.exists():
            return []
        
        query_scm_content = query_scm.read_text()
        
        # è¯»å–ä»£ç 
        try:
            code = file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
        
        if not code:
            return []
        
        # è§£æä»£ç 
        tree = parser.parse(bytes(code, "utf-8"))
        
        # è¿è¡Œæ ‡ç­¾æŸ¥è¯¢
        try:
            from tree_sitter import Query, QueryCursor
            query = Query(language, query_scm_content)
            cursor = QueryCursor(query)
            matches = cursor.matches(tree.root_node)
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥ {file_path}: {e}")
            return []
        
        definitions = []
        saw = set()
        parent_stack = []  # ğŸ†• è·Ÿè¸ªçˆ¶çº§ï¼ˆç”¨äºç¡®å®šæ–¹æ³•æ‰€å±çš„ç±»ï¼‰
        
        # å¤„ç†åŒ¹é…ç»“æœ: [(pattern_index, {capture_name: [nodes]})]
        for pattern_index, captures_dict in matches:
            for tag, nodes in captures_dict.items():
                for node in nodes:
                    if tag.startswith("name.definition."):
                        kind = "def"
                    elif tag.startswith("name.reference."):
                        kind = "ref"
                    else:
                        continue
                    
                    saw.add(kind)
                    
                    # æå–ç±»å‹ï¼ˆclassã€functionã€methodç­‰ï¼‰
                    type_name = tag.split(".")[-1]
                    name = node.text.decode("utf-8")
                    
                    # ğŸ†• ç¡®å®šçˆ¶çº§å’Œä½œç”¨åŸŸï¼ˆä»…å¯¹å®šä¹‰ï¼‰
                    parent = None
                    scope = "global"
                    
                    if kind == "def":
                        # ç¡®å®šçˆ¶çº§
                        parent = parent_stack[-1] if parent_stack else None
                        
                        # ç¡®å®šä½œç”¨åŸŸ
                        if type_name == "class":
                            scope = "global"
                            # å°†ç±»åå‹å…¥æ ˆï¼ˆç”¨äºåç»­æ–¹æ³•ï¼‰
                            parent_stack.append(name)
                        elif type_name in ("function", "method"):
                            scope = "class" if parent else "global"
                        else:
                            scope = "global"
                    
                    definitions.append({
                        "type": type_name,
                        "name": name,
                        "line": node.start_point[0] + 1,
                        "kind": kind,
                        # ğŸ†• é˜¶æ®µ2æ–°å¢å­—æ®µ
                        "parent": parent,
                        "scope": scope
                    })
        
        # å¦‚æœåªæœ‰å®šä¹‰æ²¡æœ‰å¼•ç”¨ï¼Œä½¿ç”¨ Pygments è¡¥å……å¼•ç”¨
        if "ref" not in saw and "def" in saw:
            try:
                lexer = guess_lexer_for_filename(str(file_path), code)
                tokens = list(lexer.get_tokens(code))
                tokens = [token[1] for token in tokens if token[0] in Token.Name]
                
                for token in tokens:
                    definitions.append({
                        "type": "reference",
                        "name": token,
                        "line": -1,
                        "kind": "ref"
                    })
            except Exception:
                pass
        
        return definitions
    
    def _get_scm_fname(self, lang: str) -> Optional[Path]:
        """è·å– Tree-sitter æŸ¥è¯¢æ–‡ä»¶è·¯å¾„"""
        # æŸ¥è¯¢æ–‡ä»¶ç›®å½•
        queries_dir = Path(__file__).parent / "queries"
        
        # ä¼˜å…ˆä½¿ç”¨ tree-sitter-language-pack
        if USING_TSL_PACK:
            subdir = "tree-sitter-language-pack"
            path = queries_dir / subdir / f"{lang}-tags.scm"
            if path.exists():
                return path
        
        # å›é€€åˆ° tree-sitter-languages
        subdir = "tree-sitter-languages"
        path = queries_dir / subdir / f"{lang}-tags.scm"
        if path.exists():
            return path
        
        return None
    
    def _compute_end_lines(
        self,
        definitions: Dict[str, List[Dict]],
        repo_path: Path
    ) -> Dict[str, List[Dict]]:
        """
        è®¡ç®—æ¯ä¸ªå®šä¹‰çš„ç»“æŸè¡Œï¼ˆğŸ†• å…¬å¼€APIæ”¯æŒï¼‰
        
        ç­–ç•¥ï¼š
        1. å¦‚æœå·²æœ‰end_lineï¼Œä¿æŒä¸å˜
        2. å¦åˆ™ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªå®šä¹‰çš„èµ·å§‹è¡Œä½œä¸ºç»“æŸè¡Œ
        3. å¦‚æœæ˜¯æœ€åä¸€ä¸ªå®šä¹‰ï¼Œä½¿ç”¨æ–‡ä»¶æœ«å°¾
        """
        for file_path, defs in definitions.items():
            # åªå¤„ç†å®šä¹‰ï¼Œä¸å¤„ç†å¼•ç”¨
            def_only = [d for d in defs if d.get("kind") == "def"]
            
            if not def_only:
                continue
            
            # æŒ‰è¡Œå·æ’åº
            def_only.sort(key=lambda d: d["line"])
            
            for i, d in enumerate(def_only):
                if "end_line" in d and d["end_line"] > 0:
                    continue
                
                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå®šä¹‰
                if i + 1 < len(def_only):
                    d["end_line"] = def_only[i + 1]["line"] - 1
                else:
                    # æœ€åä¸€ä¸ªå®šä¹‰ï¼Œè¯»å–æ–‡ä»¶è·å–æ€»è¡Œæ•°
                    try:
                        full_path = repo_path / file_path
                        with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                            total_lines = len(f.readlines())
                        d["end_line"] = total_lines
                    except Exception as e:
                        logger.debug(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
                        # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¼°è®¡50è¡Œ
                        d["end_line"] = d["line"] + 50
        
        return definitions

    
    def _build_reference_graph(self, definitions: Dict[str, List[Dict]], repo_path: Path) -> Dict[str, Dict[str, float]]:
        """
        æ„å»ºå¼•ç”¨å›¾
        
        Returns:
            {node: {referenced_node: weight, ...}}
        """
        graph = defaultdict(lambda: defaultdict(float))
        
        # æ„å»ºæ ‡è¯†ç¬¦åˆ°æ–‡ä»¶çš„æ˜ å°„ï¼ˆåªåŒ…å«å®šä¹‰ï¼‰
        ident_to_files = defaultdict(set)
        for file_path, defs in definitions.items():
            for d in defs:
                # åªæ·»åŠ å®šä¹‰ï¼Œä¸æ·»åŠ å¼•ç”¨
                if d.get("kind") == "def":
                    ident_to_files[d["name"]].add(file_path)
        
        # æ‰«æå¼•ç”¨å…³ç³»
        for file_path, defs in definitions.items():
            # æ”¶é›†æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¼•ç”¨
            references_in_file = set()
            for d in defs:
                if d.get("kind") == "ref":
                    references_in_file.add(d["name"])
            
            # ä¸ºæ¯ä¸ªå¼•ç”¨æ·»åŠ è¾¹
            for ident in references_in_file:
                if ident in ident_to_files:
                    # æ–‡ä»¶å¼•ç”¨äº†è¿™ä¸ªæ ‡è¯†ç¬¦
                    for ref_file in ident_to_files[ident]:
                        if ref_file != file_path:
                            # æ·»åŠ è¾¹ï¼šfile_path -> ref_file
                            graph[file_path][ref_file] += 1.0
        
        return dict(graph)
    
    def _update_reference_graph_incremental(
        self,
        old_graph: Dict[str, Dict[str, float]],
        definitions: Dict[str, List[Dict]],
        changed_files: List[str],
        repo_path: Path
    ) -> Dict[str, Dict[str, float]]:
        """
        å¢é‡æ›´æ–°å¼•ç”¨å›¾ï¼ˆåªé‡æ–°è®¡ç®—æ”¹åŠ¨æ–‡ä»¶çš„å¼•ç”¨å…³ç³»ï¼‰
        
        Args:
            old_graph: æ—§çš„å¼•ç”¨å›¾
            definitions: æ‰€æœ‰æ–‡ä»¶çš„å®šä¹‰
            changed_files: æ”¹åŠ¨çš„æ–‡ä»¶åˆ—è¡¨
            repo_path: ä»“åº“è·¯å¾„
        
        Returns:
            æ›´æ–°åçš„å¼•ç”¨å›¾
        """
        # å¤åˆ¶æ—§å›¾
        graph = defaultdict(lambda: defaultdict(float))
        for source, targets in old_graph.items():
            graph[source] = defaultdict(float, targets)
        
        # ğŸ”¥ æ­¥éª¤1ï¼šåˆ é™¤æ”¹åŠ¨æ–‡ä»¶çš„æ—§å¼•ç”¨å…³ç³»
        for file in changed_files:
            # åˆ é™¤è¯¥æ–‡ä»¶ä½œä¸ºæºçš„å¼•ç”¨
            if file in graph:
                del graph[file]
            
            # åˆ é™¤æŒ‡å‘è¯¥æ–‡ä»¶çš„å¼•ç”¨
            for source in list(graph.keys()):
                if file in graph[source]:
                    del graph[source][file]
                    # å¦‚æœæºæ–‡ä»¶æ²¡æœ‰å…¶ä»–å¼•ç”¨ï¼Œåˆ é™¤è¯¥æº
                    if not graph[source]:
                        del graph[source]
        
        # ğŸ”¥ æ­¥éª¤2ï¼šé‡æ–°æ„å»ºæ ‡è¯†ç¬¦æ˜ å°„ï¼ˆåªåŒ…å«æ”¹åŠ¨æ–‡ä»¶çš„å®šä¹‰ï¼‰
        ident_to_files = defaultdict(set)
        
        # æ·»åŠ æ‰€æœ‰æ–‡ä»¶çš„å®šä¹‰ï¼ˆç”¨äºæŸ¥æ‰¾å¼•ç”¨ç›®æ ‡ï¼‰
        for file_path, defs in definitions.items():
            for d in defs:
                if d.get("kind") == "def":
                    ident_to_files[d["name"]].add(file_path)
        
        # ğŸ”¥ æ­¥éª¤3ï¼šé‡æ–°è®¡ç®—æ”¹åŠ¨æ–‡ä»¶çš„å¼•ç”¨å…³ç³»
        for file_path in changed_files:
            if file_path not in definitions:
                continue
            
            defs = definitions[file_path]
            
            # æ”¶é›†æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¼•ç”¨
            references_in_file = set()
            for d in defs:
                if d.get("kind") == "ref":
                    references_in_file.add(d["name"])
            
            # ä¸ºæ¯ä¸ªå¼•ç”¨æ·»åŠ è¾¹
            for ident in references_in_file:
                if ident in ident_to_files:
                    for ref_file in ident_to_files[ident]:
                        if ref_file != file_path:
                            graph[file_path][ref_file] += 1.0
        
        # ğŸ”¥ æ­¥éª¤4ï¼šé‡æ–°è®¡ç®—æŒ‡å‘æ”¹åŠ¨æ–‡ä»¶çš„å¼•ç”¨
        # å…¶ä»–æ–‡ä»¶å¯èƒ½å¼•ç”¨äº†æ”¹åŠ¨æ–‡ä»¶ä¸­çš„å®šä¹‰
        changed_idents = set()
        for file_path in changed_files:
            if file_path in definitions:
                for d in definitions[file_path]:
                    if d.get("kind") == "def":
                        changed_idents.add(d["name"])
        
        # æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼Œæ‰¾åˆ°å¼•ç”¨äº†æ”¹åŠ¨æ ‡è¯†ç¬¦çš„æ–‡ä»¶
        for file_path, defs in definitions.items():
            if file_path in changed_files:
                continue  # è·³è¿‡æ”¹åŠ¨æ–‡ä»¶ï¼ˆå·²å¤„ç†ï¼‰
            
            # æ”¶é›†æ–‡ä»¶ä¸­çš„å¼•ç”¨
            references_in_file = set()
            for d in defs:
                if d.get("kind") == "ref":
                    references_in_file.add(d["name"])
            
            # æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†æ”¹åŠ¨çš„æ ‡è¯†ç¬¦
            referenced_changed = references_in_file.intersection(changed_idents)
            if referenced_changed:
                # é‡æ–°è®¡ç®—è¯¥æ–‡ä»¶æŒ‡å‘æ”¹åŠ¨æ–‡ä»¶çš„å¼•ç”¨
                for ident in referenced_changed:
                    if ident in ident_to_files:
                        for ref_file in ident_to_files[ident]:
                            if ref_file != file_path and ref_file in changed_files:
                                graph[file_path][ref_file] += 1.0
        
        return dict(graph)
    
    def _pagerank(
        self,
        graph: Dict[str, Dict[str, float]],
        definitions: Dict[str, List[Dict]],  # æ·»åŠ  definitions å‚æ•°
        chat_files: List[str],
        mentioned_idents: List[str],
        damping: float = 0.85,
        iterations: int = 20
    ) -> List[Tuple[str, float]]:
        """
        PageRankç®—æ³•æ’åº
        
        Args:
            graph: å¼•ç”¨å›¾
            chat_files: å¯¹è¯ä¸­çš„æ–‡ä»¶ï¼ˆæƒé‡Ã—50ï¼‰
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦ï¼ˆæƒé‡Ã—10ï¼‰
            damping: é˜»å°¼ç³»æ•°
            iterations: è¿­ä»£æ¬¡æ•°
            
        Returns:
            [(file_path, score), ...] æŒ‰åˆ†æ•°é™åº
        """
        # æ‰€æœ‰èŠ‚ç‚¹
        nodes = set(graph.keys())
        for targets in graph.values():
            nodes.update(targets.keys())
        
        if not nodes:
            return []
        
        # åˆå§‹åŒ–åˆ†æ•°
        scores = {node: 1.0 / len(nodes) for node in nodes}
        
        # ä¸ªæ€§åŒ–æƒé‡
        personalization = {}
        for node in nodes:
            weight = 1.0
            
            # å¯¹è¯æ–‡ä»¶æƒé‡Ã—50
            if node in chat_files:
                weight *= 50
            
            # æåˆ°çš„æ ‡è¯†ç¬¦æƒé‡Ã—10
            # æ£€æŸ¥ï¼š1) è·¯å¾„ç»„ä»¶  2) æ–‡ä»¶ä¸­çš„å®šä¹‰åç§°
            if mentioned_idents:
                # æ£€æŸ¥è·¯å¾„ç»„ä»¶ï¼ˆå¦‚ agents/llm/timeoutï¼‰
                path_components = set(Path(node).parts)
                basename_with_ext = Path(node).name
                basename_without_ext = Path(node).stem
                components_to_check = path_components.union({basename_with_ext, basename_without_ext})
                
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«æåˆ°çš„æ ‡è¯†ç¬¦
                matched_path = components_to_check.intersection(set(ident.lower() for ident in mentioned_idents))
                if matched_path:
                    weight *= 10
                
                # æ£€æŸ¥æ–‡ä»¶ä¸­çš„å®šä¹‰æ˜¯å¦åŒ…å«æåˆ°çš„æ ‡è¯†ç¬¦
                if node in definitions:
                    file_defs = definitions.get(node, [])
                    def_names = {d['name'].lower() for d in file_defs if d.get('kind') == 'def'}
                    mentioned_lower = {ident.lower() for ident in mentioned_idents}
                    
                    # ç²¾ç¡®åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…
                    if def_names.intersection(mentioned_lower):
                        weight *= 10
                    else:
                        # éƒ¨åˆ†åŒ¹é…ï¼ˆå¦‚ 'timeout' åŒ¹é… 'TimeoutError'ï¼‰
                        for def_name in def_names:
                            for ident in mentioned_lower:
                                if ident in def_name or def_name in ident:
                                    weight *= 5  # éƒ¨åˆ†åŒ¹é…æƒé‡è¾ƒä½
                                    break
            
            personalization[node] = weight
        
        # å½’ä¸€åŒ–
        total = sum(personalization.values())
        personalization = {k: v / total for k, v in personalization.items()}
        
        # PageRankè¿­ä»£
        for _ in range(iterations):
            new_scores = {}
            
            for node in nodes:
                # åŸºç¡€åˆ†æ•°ï¼ˆéšæœºè·³è½¬ï¼‰
                score = (1 - damping) * personalization.get(node, 1.0 / len(nodes))
                
                # æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„åˆ†æ•°
                for source, targets in graph.items():
                    if node in targets:
                        # source -> node
                        weight = targets[node]
                        out_weight = sum(targets.values())
                        score += damping * scores[source] * (weight / out_weight)
                
                new_scores[node] = score
            
            scores = new_scores
        
        # æ’åº
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return ranked
    
    def _generate_map(
        self,
        ranked: List[Tuple[str, float]],
        definitions: Dict[str, List[Dict]],
        max_tokens: int,
        enable_lsp: bool = False  # ğŸ”¥ æ–°å¢å‚æ•°
    ) -> str:
        """
        ç”Ÿæˆä»£ç åœ°å›¾ï¼ˆæ§åˆ¶tokenæ•°é‡ï¼‰
        
        ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°æœ€ä¼˜tokenæ•°é‡
        """
        lines = []
        current_tokens = 0
        
        for file_path, score in ranked:
            if file_path not in definitions:
                continue
            
            file_defs = definitions[file_path]
            if not file_defs:
                continue
            
            # åªåŒ…å«å®šä¹‰ï¼Œä¸åŒ…å«å¼•ç”¨
            file_defs = [d for d in file_defs if d.get("kind") == "def"]
            if not file_defs:
                continue
            
            # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆç¡®ä¿è¿”å›ç›¸å¯¹äº repo_path çš„è·¯å¾„ï¼‰
            normalized_path = self.normalize_path(file_path)
            
            # æ–‡ä»¶å¤´
            file_line = f"\n{normalized_path}:"
            file_tokens = len(file_line.split())
            
            if current_tokens + file_tokens > max_tokens:
                break
            
            lines.append(file_line)
            current_tokens += file_tokens
            
            # å®šä¹‰åˆ—è¡¨
            for d in file_defs:
                # ğŸ”¥ LSPå¢å¼ºè¾“å‡ºï¼šæ˜¾ç¤ºç±»å‹ç­¾åå’Œå¼•ç”¨è®¡æ•°
                has_signature = enable_lsp and d.get('lsp_signature')
                has_ref_count = enable_lsp and d.get('lsp_ref_count', 0) > 0
                
                if has_signature and has_ref_count:
                    # å®Œæ•´LSPä¿¡æ¯ï¼šç±»å‹ç­¾å + å¼•ç”¨è®¡æ•°
                    def_line = f"  {d.get('type', 'unknown')} {d['name']}: {d['lsp_signature']}  # {d['lsp_ref_count']}æ¬¡å¼•ç”¨"
                elif has_signature:
                    # åªæœ‰ç±»å‹ç­¾å
                    def_line = f"  {d.get('type', 'unknown')} {d['name']}: {d['lsp_signature']}"
                elif has_ref_count:
                    # åªæœ‰å¼•ç”¨è®¡æ•°
                    def_line = f"  {d.get('type', 'unknown')} {d['name']} (line {d['line']})  # {d['lsp_ref_count']}æ¬¡å¼•ç”¨"
                elif enable_lsp and d.get('lsp_verified'):
                    # LSPéªŒè¯é€šè¿‡ä½†æ— é¢å¤–ä¿¡æ¯
                    def_line = f"  {d.get('type', 'unknown')} {d['name']} (line {d['line']}) âœ“"
                else:
                    # æ ‡å‡†æ ¼å¼
                    def_line = f"  {d.get('type', 'unknown')} {d['name']} (line {d['line']})"
                
                def_tokens = len(def_line.split())
                
                if current_tokens + def_tokens > max_tokens:
                    break
                
                lines.append(def_line)
                current_tokens += def_tokens
            
            if current_tokens >= max_tokens:
                break
        
        if not lines:
            return "ä»£ç åœ°å›¾ä¸ºç©º"
        
        # æ·»åŠ å¤´éƒ¨
        file_count = len([l for l in lines if l.startswith('\n')])
        header = f"# ä»£ç åœ°å›¾ (Top {file_count} æ–‡ä»¶)\n"
        if enable_lsp:
            header += "# (LSPå¢å¼º: åŒ…å«ç±»å‹ç­¾åå’Œå¼•ç”¨è®¡æ•°)\n"
        return header + "\n".join(lines)
    
    async def _enhance_with_lsp(
        self,
        ranked: List[Tuple[str, float]],
        definitions: Dict[str, List[Dict]],
        repo_path: Path,
        top_k: int = 50
    ) -> None:
        """
        ä½¿ç”¨LSPå¢å¼ºå®šä¹‰ä¿¡æ¯
        
        çœŸæ­£çš„å¢å¼ºï¼š
        1. ä½¿ç”¨hoverè·å–ç±»å‹ç­¾å
        2. ä½¿ç”¨referencesè·å–å¼•ç”¨è®¡æ•°ï¼ˆå¦‚æœLSPæ”¯æŒï¼‰
        3. ä¸ºç¬¦å·æ·»åŠ å®Œæ•´çš„LSPä¿¡æ¯
        """
        from .lsp_tools import with_lsp_client, get_lsp_manager
        
        try:
            manager = get_lsp_manager()
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„
            files_to_enhance: Dict[str, List[Dict]] = {}
            count = 0
            
            for file_path, score in ranked:
                if count >= top_k:
                    break
                
                if file_path not in definitions:
                    continue
                
                file_defs = [d for d in definitions[file_path] if d.get("kind") == "def"]
                
                if file_defs:
                    files_to_enhance[file_path] = file_defs
                    count += len(file_defs)
                    if count >= top_k:
                        excess = count - top_k
                        files_to_enhance[file_path] = file_defs[:-excess] if excess > 0 else file_defs
                        break
            
            logger.info(f"ğŸ”¥ LSPå¢å¼º: å¤„ç†{len(files_to_enhance)}ä¸ªæ–‡ä»¶ï¼Œè·å–ç±»å‹ä¿¡æ¯å’Œå¼•ç”¨è®¡æ•°...")
            
            enhanced_count = 0
            skipped_count = 0
            
            for file_path, file_defs in files_to_enhance.items():
                try:
                    abs_file_path = repo_path / file_path
                    
                    if not abs_file_path.exists():
                        skipped_count += len(file_defs)
                        continue
                    
                    # æ£€æŸ¥LSPæ”¯æŒ
                    ext = abs_file_path.suffix
                    server_config = manager.find_server_for_extension(ext)
                    if not server_config or not manager.is_server_installed(server_config):
                        skipped_count += len(file_defs)
                        continue
                    
                    # è·å–LSPç¬¦å·
                    symbols = await with_lsp_client(
                        str(abs_file_path),
                        lambda client: client.document_symbols(str(abs_file_path))
                    )
                    
                    if not symbols:
                        logger.debug(f"  {file_path}: æœªè·å–åˆ°ç¬¦å·")
                        skipped_count += len(file_defs)
                        continue
                    
                    logger.debug(f"  {file_path}: è·å–åˆ°{len(symbols)}ä¸ªç¬¦å·ï¼Œå¤„ç†{len(file_defs)}ä¸ªå®šä¹‰")
                    
                    # ä¸ºæ¯ä¸ªå®šä¹‰è·å–LSPä¿¡æ¯
                    for defn in file_defs:
                        target_line = defn['line'] - 1
                        target_name = defn['name']
                        
                        # åŒ¹é…ç¬¦å·
                        matching_symbol = None
                        for sym in symbols:
                            if 'range' in sym:
                                sym_line = sym['range']['start']['line']
                                sym_name = sym.get('name', '')
                                if abs(sym_line - target_line) <= 2 and sym_name == target_name:
                                    matching_symbol = sym
                                    break
                        
                        if not matching_symbol:
                            for sym in symbols:
                                if 'range' in sym:
                                    sym_line = sym['range']['start']['line']
                                    sym_name = sym.get('name', '')
                                    if abs(sym_line - target_line) <= 10 and sym_name == target_name:
                                        matching_symbol = sym
                                        break
                        
                        if matching_symbol:
                            line = matching_symbol['range']['start']['line']
                            char = matching_symbol['range']['start']['character']
                            
                            # è½¬æ¢ä¸º1-basedè¡Œå·
                            line_1based = line + 1
                            
                            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨selectionRangeï¼ˆç¬¦å·åç§°çš„ä½ç½®ï¼‰è€Œä¸æ˜¯rangeï¼ˆæ•´ä¸ªå®šä¹‰çš„ä½ç½®ï¼‰
                            if 'selectionRange' in matching_symbol:
                                sel_line = matching_symbol['selectionRange']['start']['line']
                                sel_char = matching_symbol['selectionRange']['start']['character']
                                line_1based = sel_line + 1
                                char = sel_char
                            
                            has_info = False
                            
                            # 1. è·å–hoverä¿¡æ¯ï¼ˆç±»å‹ç­¾åï¼‰
                            try:
                                hover_info = await with_lsp_client(
                                    str(abs_file_path),
                                    lambda client: client.hover(str(abs_file_path), line_1based, char)
                                )
                                
                                if hover_info and 'contents' in hover_info:
                                    signature = self._extract_signature(hover_info['contents'])
                                    if signature:
                                        defn['lsp_signature'] = signature
                                        has_info = True
                                        logger.debug(f"    âœ“ {target_name}: {signature}")
                            except Exception as e:
                                logger.debug(f"    hoverå¤±è´¥ {target_name}: {e}")
                            
                            # 2. è·å–å¼•ç”¨è®¡æ•°
                            try:
                                references = await with_lsp_client(
                                    str(abs_file_path),
                                    lambda client: client.references(
                                        str(abs_file_path), line_1based, char,
                                        include_declaration=False
                                    )
                                )
                                
                                if references and len(references) > 0:
                                    defn['lsp_ref_count'] = len(references)
                                    has_info = True
                                    logger.debug(f"    âœ“ {target_name}: {len(references)}æ¬¡å¼•ç”¨")
                            except Exception as e:
                                logger.debug(f"    referenceså¤±è´¥ {target_name}: {e}")
                            
                            # æ ‡è®°ä¸ºå·²éªŒè¯
                            defn['lsp_verified'] = True
                            if has_info:
                                enhanced_count += 1
                            else:
                                skipped_count += 1
                        else:
                            skipped_count += 1
                
                except Exception as e:
                    logger.debug(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    skipped_count += len(file_defs)
                    continue
            
            logger.info(f"âœ… LSPå¢å¼ºå®Œæˆ: {enhanced_count}ä¸ªç¬¦å·å¢å¼º, {skipped_count}ä¸ªè·³è¿‡")
        
        except Exception as e:
            logger.warning(f"LSPå¢å¼ºå¤±è´¥: {e}")
    
    def _extract_signature(self, contents) -> Optional[str]:
        """ä»hover contentsä¸­æå–ç­¾å"""
        try:
            # contentså¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€MarkupContentæˆ–åˆ—è¡¨
            if isinstance(contents, dict) and 'value' in contents:
                text = contents['value']
            elif isinstance(contents, str):
                text = contents
            elif isinstance(contents, list) and len(contents) > 0:
                first = contents[0]
                if isinstance(first, dict) and 'value' in first:
                    text = first['value']
                elif isinstance(first, str):
                    text = first
                else:
                    return None
            else:
                return None
            
            # æ¸…ç†markdown
            text = text.strip()
            if text.startswith('```'):
                lines = text.split('\n')
                if len(lines) > 2:
                    text = '\n'.join(lines[1:-1]).strip()
            
            # åªä¿ç•™ç¬¬ä¸€è¡Œï¼ˆå‡½æ•°ç­¾åï¼‰
            signature = text.split('\n')[0].strip()
            
            # é™åˆ¶é•¿åº¦
            if len(signature) > 100:
                signature = signature[:97] + '...'
            
            return signature if signature else None
        
        except Exception:
            return None


class GetRepoStructureTool(BaseTool):
    """
    è·å–ä»“åº“ç»“æ„ï¼ˆç®€åŒ–ç‰ˆRepoMapï¼‰
    
    åªè¿”å›æ–‡ä»¶æ ‘ï¼Œä¸åšæ™ºèƒ½æ’åº
    æ”¯æŒæ™ºèƒ½æ³¨é‡Šï¼Œå¸®åŠ©ç†è§£ç›®å½•å«ä¹‰
    """
    
    # ç›®å½•ç»“æ„ä¹Ÿéœ€è¦é™åˆ¶
    MAX_OUTPUT_LINES = 500
    MAX_OUTPUT_CHARS = 8000
    
    # æ™ºèƒ½æ³¨é‡Šæ˜ å°„
    DIRECTORY_ANNOTATIONS = {
        'backend': 'åç«¯ä»£ç ',
        'frontend': 'å‰ç«¯ä»£ç ',
        'src': 'æºä»£ç ',
        'lib': 'åº“æ–‡ä»¶',
        'tests': 'æµ‹è¯•ä»£ç ',
        'test': 'æµ‹è¯•ä»£ç ',
        'docs': 'æ–‡æ¡£',
        'doc': 'æ–‡æ¡£',
        'scripts': 'è„šæœ¬å·¥å…·',
        'script': 'è„šæœ¬å·¥å…·',
        'config': 'é…ç½®æ–‡ä»¶',
        'conf': 'é…ç½®æ–‡ä»¶',
        'agents': 'Agentç³»ç»Ÿ',
        'agent': 'Agentç³»ç»Ÿ',
        'tools': 'å·¥å…·æ¨¡å—',
        'tool': 'å·¥å…·æ¨¡å—',
        'memory': 'è®°å¿†ç³»ç»Ÿ',
        'orchestrators': 'ç¼–æ’å™¨',
        'orchestrator': 'ç¼–æ’å™¨',
        'llm': 'LLMå®¢æˆ·ç«¯',
        'cli': 'å‘½ä»¤è¡Œç•Œé¢',
        'api': 'APIæ¥å£',
        'models': 'æ•°æ®æ¨¡å‹',
        'model': 'æ•°æ®æ¨¡å‹',
        'utils': 'å·¥å…·å‡½æ•°',
        'util': 'å·¥å…·å‡½æ•°',
        'core': 'æ ¸å¿ƒç»„ä»¶',
        'common': 'å…¬å…±æ¨¡å—',
        'shared': 'å…±äº«æ¨¡å—',
        'components': 'ç»„ä»¶',
        'component': 'ç»„ä»¶',
        'services': 'æœåŠ¡',
        'service': 'æœåŠ¡',
        'controllers': 'æ§åˆ¶å™¨',
        'controller': 'æ§åˆ¶å™¨',
        'views': 'è§†å›¾',
        'view': 'è§†å›¾',
        'templates': 'æ¨¡æ¿',
        'template': 'æ¨¡æ¿',
        'static': 'é™æ€èµ„æº',
        'assets': 'èµ„æºæ–‡ä»¶',
        'public': 'å…¬å¼€èµ„æº',
        'private': 'ç§æœ‰æ¨¡å—',
        'internal': 'å†…éƒ¨æ¨¡å—',
        'external': 'å¤–éƒ¨æ¨¡å—',
        'vendor': 'ç¬¬ä¸‰æ–¹åº“',
        'node_modules': 'ä¾èµ–åŒ…',
        'dist': 'æ„å»ºäº§ç‰©',
        'build': 'æ„å»ºäº§ç‰©',
        'out': 'è¾“å‡ºç›®å½•',
        'bin': 'å¯æ‰§è¡Œæ–‡ä»¶',
        'pkg': 'åŒ…æ–‡ä»¶',
        'examples': 'ç¤ºä¾‹ä»£ç ',
        'example': 'ç¤ºä¾‹ä»£ç ',
        'demo': 'æ¼”ç¤ºä»£ç ',
        'plugins': 'æ’ä»¶',
        'plugin': 'æ’ä»¶',
        'extensions': 'æ‰©å±•',
        'extension': 'æ‰©å±•',
        'middleware': 'ä¸­é—´ä»¶',
        'handlers': 'å¤„ç†å™¨',
        'handler': 'å¤„ç†å™¨',
        'routes': 'è·¯ç”±',
        'route': 'è·¯ç”±',
        'database': 'æ•°æ®åº“',
        'db': 'æ•°æ®åº“',
        'migrations': 'æ•°æ®è¿ç§»',
        'migration': 'æ•°æ®è¿ç§»',
        'seeds': 'æ•°æ®ç§å­',
        'seed': 'æ•°æ®ç§å­',
        'fixtures': 'æµ‹è¯•æ•°æ®',
        'fixture': 'æµ‹è¯•æ•°æ®',
    }
    
    def __init__(self):
        super().__init__(
            name="get_repo_structure",
            description="è·å–ä»“åº“ç›®å½•ç»“æ„ï¼Œæ”¯æŒæ™ºèƒ½æ³¨é‡Š"
        )
    
    def get_function_schema(self) -> Dict[str, Any]:
        """è·å–Function Calling schema"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "ä»“åº“æ ¹ç›®å½•è·¯å¾„ã€‚å¿…é¡»ä½¿ç”¨ '.' è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•ï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦è·¯å¾„ï¼"
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "æœ€å¤§æ·±åº¦",
                        "default": 3
                    },
                    "show_files": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æ˜¾ç¤ºæ–‡ä»¶ï¼ˆå¦åˆ™åªæ˜¾ç¤ºç›®å½•ï¼‰",
                        "default": True
                    },
                    "annotate": {
                        "type": "boolean",
                        "description": "æ˜¯å¦æ·»åŠ æ™ºèƒ½æ³¨é‡Šï¼ˆå¸®åŠ©ç†è§£ç›®å½•å«ä¹‰ï¼‰",
                        "default": True
                    }
                },
                "required": ["repo_path"]
            }
        }
    
    async def execute(
        self,
        repo_path: str,
        max_depth: int = 3,
        show_files: bool = True,
        annotate: bool = True
    ) -> ToolResult:
        """
        è·å–ä»“åº“ç»“æ„
        
        Args:
            repo_path: ä»“åº“æ ¹ç›®å½•
            max_depth: æœ€å¤§æ·±åº¦
            show_files: æ˜¯å¦æ˜¾ç¤ºæ–‡ä»¶
            annotate: æ˜¯å¦æ·»åŠ æ™ºèƒ½æ³¨é‡Š
            
        Returns:
            ToolResult
        """
        try:
            repo_path = Path(repo_path).resolve()
            if not repo_path.exists():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_path}"
                )
            
            lines = [f"{repo_path.name}/"]
            self._build_tree(repo_path, lines, "", max_depth, show_files, annotate)
            
            return ToolResult(
                success=True,
                content="\n".join(lines),
                metadata={
                    'repo_path': str(repo_path),
                    'max_depth': max_depth,
                    'show_files': show_files,
                    'annotate': annotate
                }
            )
            
        except Exception as e:
            logger.error(f"è·å–ä»“åº“ç»“æ„å¤±è´¥: {e}", exc_info=True)
            return ToolResult(
                success=False,
                content=None,
                error=str(e)
            )
    
    def _build_tree(
        self,
        path: Path,
        lines: List[str],
        prefix: str,
        max_depth: int,
        show_files: bool,
        annotate: bool,
        current_depth: int = 0
    ):
        """é€’å½’æ„å»ºæ ‘"""
        if current_depth >= max_depth:
            return
        
        # å¿½ç•¥çš„ç›®å½•
        ignore = {".git", "node_modules", "__pycache__", ".venv", "venv", "dist", "build"}
        
        try:
            items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
        except PermissionError:
            return
        
        for i, item in enumerate(items):
            if item.name in ignore:
                continue
            
            is_last = i == len(items) - 1
            current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            next_prefix = prefix + ("    " if is_last else "â”‚   ")
            
            if item.is_dir():
                # æ·»åŠ æ³¨é‡Š
                dir_display = f"{item.name}/"
                if annotate:
                    annotation = self._get_annotation(item.name)
                    if annotation:
                        dir_display = f"{item.name}/  # {annotation}"
                
                lines.append(f"{prefix}{current_prefix}{dir_display}")
                self._build_tree(item, lines, next_prefix, max_depth, show_files, annotate, current_depth + 1)
            elif show_files:
                lines.append(f"{prefix}{current_prefix}{item.name}")
    
    def _get_annotation(self, dir_name: str) -> Optional[str]:
        """è·å–ç›®å½•æ³¨é‡Š"""
        dir_lower = dir_name.lower()
        
        # ç²¾ç¡®åŒ¹é…
        if dir_lower in self.DIRECTORY_ANNOTATIONS:
            return self.DIRECTORY_ANNOTATIONS[dir_lower]
        
        # éƒ¨åˆ†åŒ¹é…
        for pattern, annotation in self.DIRECTORY_ANNOTATIONS.items():
            if pattern in dir_lower:
                return annotation
        
        return None


class GetFileSymbolsTool(BaseTool):
    """
    è·å–å•æ–‡ä»¶ç¬¦å·è¡¨ï¼ˆç±»/å‡½æ•°/æ–¹æ³•ç­‰ï¼ŒAST æ·±åº¦ï¼‰
    
    ä¸ repo_map äº’è¡¥ï¼šå·²çŸ¥æ–‡ä»¶æ—¶å¯ç›´æ¥å–è¯¥æ–‡ä»¶çš„å®šä¹‰åˆ—è¡¨ï¼Œä¾¿äºç²¾ç¡®ç†è§£ä»£ç ç»“æ„ã€‚
    ä½¿ç”¨ä¸ RepoMap ç›¸åŒçš„ Tree-sitter è§£æã€‚
    """

    def __init__(self):
        super().__init__(
            name="get_file_symbols",
            description="è·å–æŒ‡å®šæ–‡ä»¶ä¸­çš„ç¬¦å·å®šä¹‰ï¼ˆç±»ã€å‡½æ•°ã€æ–¹æ³•ç­‰ï¼‰åŠè¡Œå·ï¼ŒåŸºäº AST è§£æã€‚"
        )

    async def execute(self, file_path: str) -> ToolResult:
        try:
            path = self.resolve_path(file_path)
            if not path.exists() or not path.is_file():
                return ToolResult(
                    success=False,
                    content=None,
                    error=f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶: {file_path}"
                )
            # å¤ç”¨ RepoMapTool çš„è§£æé€»è¾‘
            repomap = RepoMapTool()
            repomap._context = self.context
            defs = repomap._parse_file(path)
            defs = [d for d in defs if d.get("kind") == "def"]
            if not defs:
                return ToolResult(
                    success=True,
                    content="è¯¥æ–‡ä»¶ä¸­æœªè§£æåˆ°ç¬¦å·å®šä¹‰ï¼ˆæˆ–è¯­è¨€/è§£æå™¨ä¸æ”¯æŒï¼‰",
                    metadata={"file_path": str(path), "count": 0}
                )
            lines = [f"  {d.get('type', '?')} {d['name']} (line {d['line']})" for d in defs]
            text = f"# {path.name}\n" + "\n".join(lines)
            return ToolResult(
                success=True,
                content=text,
                metadata={"file_path": str(path), "count": len(defs)}
            )
        except Exception as e:
            logger.exception("get_file_symbols å¤±è´¥")
            return ToolResult(success=False, content=None, error=str(e))

    def get_function_schema(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "ç›¸å¯¹é¡¹ç›®æ ¹çš„æ–‡ä»¶è·¯å¾„"
                    }
                },
                "required": ["file_path"]
            }
        }
