"""
æµå¼èŠå¤©ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Agentçš„æµå¼è¾“å‡ºåŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from daoyoucode.agents.core.agent import BaseAgent, AgentConfig
from daoyoucode.agents.llm import get_client_manager


async def stream_chat_example():
    """æµå¼èŠå¤©ç¤ºä¾‹"""
    print("="*60)
    print("æµå¼èŠå¤©ç¤ºä¾‹")
    print("="*60)
    
    # 1. é…ç½®LLMå®¢æˆ·ç«¯
    print("\n1. é…ç½®LLMå®¢æˆ·ç«¯...")
    client_manager = get_client_manager()
    
    # é…ç½®é€šä¹‰åƒé—®ï¼ˆéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYï¼‰
    import os
    api_key = os.getenv('DASHSCOPE_API_KEY', 'your-api-key-here')
    
    if api_key == 'your-api-key-here':
        print("âš ï¸ è­¦å‘Š: æœªè®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–ä¿®æ”¹ä»£ç ä¸­çš„ api_key")
        print("\nä½¿ç”¨Mockæ¨¡å¼æ¼”ç¤º...")
        use_mock = True
    else:
        client_manager.configure_provider(
            provider='qwen',
            api_key=api_key,
            base_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
        )
        use_mock = False
        print("âœ“ LLMå®¢æˆ·ç«¯é…ç½®å®Œæˆ")
    
    # 2. åˆ›å»ºAgent
    print("\n2. åˆ›å»ºAgent...")
    config = AgentConfig(
        name="chat_agent",
        description="èŠå¤©åŠ©æ‰‹",
        model="qwen-turbo",
        temperature=0.7,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
    )
    agent = BaseAgent(config)
    print("âœ“ Agentåˆ›å»ºå®Œæˆ")
    
    # 3. æµå¼å¯¹è¯
    print("\n3. å¼€å§‹æµå¼å¯¹è¯...")
    print("="*60)
    
    questions = [
        "ä»‹ç»ä¸€ä¸‹Pythonçš„ä¸»è¦ç‰¹ç‚¹",
        "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹è£…é¥°å™¨çš„ä½œç”¨"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\né—®é¢˜ {i}: {question}")
        print("-"*60)
        print("AI: ", end='', flush=True)
        
        if use_mock:
            # Mockæ¨¡å¼
            from unittest.mock import patch, AsyncMock
            
            async def mock_stream(*args, **kwargs):
                response = f"è¿™æ˜¯å¯¹'{question}'çš„æ¨¡æ‹Ÿå›ç­”ã€‚æµå¼è¾“å‡ºå¯ä»¥è®©ç”¨æˆ·å®æ—¶çœ‹åˆ°å“åº”å†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚"
                for char in response:
                    yield char
                    await asyncio.sleep(0.02)
            
            with patch.object(agent.memory, 'load_context_smart', new_callable=AsyncMock) as mock_load:
                mock_load.return_value = {
                    'history': [],
                    'strategy': 'new_conversation',
                    'cost': 0,
                    'filtered': False
                }
                
                with patch.object(agent.memory, 'is_followup', new_callable=AsyncMock) as mock_followup:
                    mock_followup.return_value = (False, 0.0, "æ–°å¯¹è¯")
                    
                    with patch.object(agent, '_stream_llm', side_effect=mock_stream):
                        async for event in agent.execute_stream(
                            prompt_source={'use_agent_default': True},
                            user_input=question,
                            context={'session_id': f'demo_{i}', 'user_id': 'demo_user'}
                        ):
                            if event['type'] == 'token':
                                print(event['content'], end='', flush=True)
        else:
            # çœŸå®æ¨¡å¼
            async for event in agent.execute_stream(
                prompt_source={'use_agent_default': True},
                user_input=question,
                context={'session_id': f'demo_{i}', 'user_id': 'demo_user'}
            ):
                if event['type'] == 'token':
                    print(event['content'], end='', flush=True)
                elif event['type'] == 'error':
                    print(f"\n[é”™è¯¯] {event['error']}")
        
        print("\n")
    
    print("="*60)
    print("âœ… æµå¼å¯¹è¯æ¼”ç¤ºå®Œæˆ")


async def compare_stream_vs_normal():
    """å¯¹æ¯”æµå¼è¾“å‡ºå’Œæ™®é€šè¾“å‡º"""
    print("\n" + "="*60)
    print("å¯¹æ¯”ï¼šæµå¼è¾“å‡º vs æ™®é€šè¾“å‡º")
    print("="*60)
    
    config = AgentConfig(
        name="test_agent",
        description="æµ‹è¯•Agent",
        model="qwen-turbo",
        temperature=0.7,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"
    )
    agent = BaseAgent(config)
    
    question = "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    
    from unittest.mock import patch, AsyncMock
    import time
    
    # Mockå“åº”
    mock_response = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚" * 3
    
    # 1. æ™®é€šæ¨¡å¼
    print("\n1. æ™®é€šæ¨¡å¼ï¼ˆç­‰å¾…å®Œæ•´å“åº”ï¼‰:")
    print("-"*60)
    
    with patch.object(agent.memory, 'load_context_smart', new_callable=AsyncMock) as mock_load:
        mock_load.return_value = {
            'history': [],
            'strategy': 'new_conversation',
            'cost': 0,
            'filtered': False
        }
        
        with patch.object(agent, '_call_llm', new_callable=AsyncMock) as mock_llm:
            mock_llm.return_value = mock_response
            
            start = time.time()
            await asyncio.sleep(2)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            result = await agent.execute(
                prompt_source={'use_agent_default': True},
                user_input=question,
                context={'session_id': 'compare_1', 'user_id': 'demo_user'}
            )
            end = time.time()
            
            print(f"[ç­‰å¾… {end-start:.1f}ç§’...]")
            print(f"AI: {result.content}")
            print(f"\nç”¨æˆ·ä½“éªŒ: éœ€è¦ç­‰å¾… {end-start:.1f}ç§’ æ‰èƒ½çœ‹åˆ°å“åº”")
    
    # 2. æµå¼æ¨¡å¼
    print("\n2. æµå¼æ¨¡å¼ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰:")
    print("-"*60)
    
    async def mock_stream(*args, **kwargs):
        for char in mock_response:
            yield char
            await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿé€å­—è¾“å‡º
    
    with patch.object(agent.memory, 'load_context_smart', new_callable=AsyncMock) as mock_load:
        mock_load.return_value = {
            'history': [],
            'strategy': 'new_conversation',
            'cost': 0,
            'filtered': False
        }
        
        with patch.object(agent.memory, 'is_followup', new_callable=AsyncMock) as mock_followup:
            mock_followup.return_value = (False, 0.0, "æ–°å¯¹è¯")
            
            with patch.object(agent, '_stream_llm', side_effect=mock_stream):
                start = time.time()
                first_token_time = None
                
                print("AI: ", end='', flush=True)
                async for event in agent.execute_stream(
                    prompt_source={'use_agent_default': True},
                    user_input=question,
                    context={'session_id': 'compare_2', 'user_id': 'demo_user'}
                ):
                    if event['type'] == 'token':
                        if first_token_time is None:
                            first_token_time = time.time()
                        print(event['content'], end='', flush=True)
                
                end = time.time()
                ttft = first_token_time - start if first_token_time else 0
                
                print(f"\n\nç”¨æˆ·ä½“éªŒ: é¦–å­—å»¶è¿Ÿ {ttft*1000:.0f}msï¼Œå®æ—¶çœ‹åˆ°è¾“å‡º")
    
    print("\n" + "="*60)
    print("å¯¹æ¯”æ€»ç»“ï¼š")
    print("  æ™®é€šæ¨¡å¼: ç­‰å¾…æ—¶é—´é•¿ï¼Œç”¨æˆ·ä½“éªŒå·®")
    print("  æµå¼æ¨¡å¼: å®æ—¶åé¦ˆï¼Œç”¨æˆ·ä½“éªŒå¥½")
    print("="*60)


if __name__ == "__main__":
    print("\nğŸš€ æµå¼è¾“å‡ºåŠŸèƒ½æ¼”ç¤º\n")
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(stream_chat_example())
    asyncio.run(compare_stream_vs_normal())
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆ")
