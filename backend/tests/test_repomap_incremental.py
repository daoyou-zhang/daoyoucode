#!/usr/bin/env python3
"""
æµ‹è¯• RepoMap å¢é‡æ›´æ–°åŠŸèƒ½
"""

import asyncio
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from daoyoucode.agents.tools.repomap_tools import RepoMapTool
from daoyoucode.agents.tools.base import ToolContext


async def test_incremental_update():
    """æµ‹è¯•å¢é‡æ›´æ–°"""
    
    print("=" * 80)
    print("RepoMap å¢é‡æ›´æ–°æµ‹è¯•")
    print("=" * 80)
    print()
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool = RepoMapTool()
    
    # è®¾ç½®ä¸Šä¸‹æ–‡
    context = ToolContext(
        repo_path=Path(".").resolve(),
        session_id="test",
        subtree_only=None
    )
    tool.set_context(context)
    
    # æµ‹è¯•å‚æ•°
    repo_path = "."
    max_tokens = 3000
    
    print("æ­¥éª¤1ï¼šé¦–æ¬¡è°ƒç”¨ï¼ˆå†·å¯åŠ¨ï¼‰")
    print("-" * 80)
    start = time.time()
    result1 = await tool.execute(repo_path, [], [], max_tokens, enable_lsp=False)
    time1 = time.time() - start
    print(f"âœ… é¦–æ¬¡è°ƒç”¨å®Œæˆ: {time1:.2f}ç§’")
    print(f"   æ–‡ä»¶æ•°: {result1.metadata.get('file_count', 0)}")
    print(f"   å®šä¹‰æ•°: {result1.metadata.get('definition_count', 0)}")
    if 'cache_stats' in result1.metadata:
        stats = result1.metadata['cache_stats']
        print(f"   æ–‡ä»¶çº§ç¼“å­˜: {stats['file_hits']}/{stats['file_hits'] + stats['file_misses']} "
              f"({stats['file_hits'] / (stats['file_hits'] + stats['file_misses']):.0%})")
    print()
    
    print("æ­¥éª¤2ï¼šæ¨¡æ‹Ÿæ–‡ä»¶ä¿®æ”¹ï¼ˆä¿®æ”¹æµ‹è¯•æ–‡ä»¶ï¼‰")
    print("-" * 80)
    test_file = Path("backend/test_repomap_cache.py")
    if test_file.exists():
        # è¯»å–æ–‡ä»¶
        content = test_file.read_text(encoding='utf-8')
        # æ·»åŠ ä¸€ä¸ªæ³¨é‡Šï¼ˆè§¦å‘ mtime æ”¹å˜ï¼‰
        test_file.write_text(content + "\n# Test modification\n", encoding='utf-8')
        print(f"âœ… ä¿®æ”¹æ–‡ä»¶: {test_file}")
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿ mtime æ”¹å˜
        time.sleep(0.1)
    else:
        print("âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¿®æ”¹")
    print()
    
    print("æ­¥éª¤3ï¼šå†æ¬¡è°ƒç”¨ï¼ˆåº”è¯¥è§¦å‘å¢é‡æ›´æ–°ï¼‰")
    print("-" * 80)
    start = time.time()
    result2 = await tool.execute(repo_path, [], [], max_tokens, enable_lsp=False)
    time2 = time.time() - start
    print(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: {time2:.2f}ç§’")
    print(f"   æ–‡ä»¶æ•°: {result2.metadata.get('file_count', 0)}")
    if 'cache_stats' in result2.metadata:
        stats = result2.metadata['cache_stats']
        print(f"   æ–‡ä»¶çº§ç¼“å­˜: {stats['file_hits']}/{stats['file_hits'] + stats['file_misses']} "
              f"({stats['file_hits'] / (stats['file_hits'] + stats['file_misses']):.0%})")
    print()
    
    print("æ­¥éª¤4ï¼šæ¢å¤æ–‡ä»¶ï¼ˆæ’¤é”€ä¿®æ”¹ï¼‰")
    print("-" * 80)
    if test_file.exists():
        # æ¢å¤æ–‡ä»¶
        test_file.write_text(content, encoding='utf-8')
        print(f"âœ… æ¢å¤æ–‡ä»¶: {test_file}")
    print()
    
    print("æ­¥éª¤5ï¼šç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼ˆéªŒè¯ç¼“å­˜ï¼‰")
    print("-" * 80)
    start = time.time()
    result3 = await tool.execute(repo_path, [], [], max_tokens, enable_lsp=False)
    time3 = time.time() - start
    print(f"âœ… ç¬¬ä¸‰æ¬¡è°ƒç”¨å®Œæˆ: {time3:.4f}ç§’")
    if 'cache_stats' in result3.metadata:
        stats = result3.metadata['cache_stats']
        print(f"   æ–‡ä»¶çº§ç¼“å­˜: {stats['file_hits']}/{stats['file_hits'] + stats['file_misses']} "
              f"({stats['file_hits'] / (stats['file_hits'] + stats['file_misses']):.0%})")
    print()
    
    # æ€»ç»“
    print("=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"é¦–æ¬¡è°ƒç”¨: {time1:.2f}ç§’ (å…¨é‡æ‰«æ)")
    print(f"å¢é‡æ›´æ–°: {time2:.2f}ç§’ (åªé‡æ–°è§£æ1ä¸ªæ–‡ä»¶)")
    print(f"ç¼“å­˜å‘½ä¸­: {time3:.4f}ç§’ (å…¨éƒ¨å‘½ä¸­ç¼“å­˜)")
    print()
    
    # ç¼“å­˜ç»Ÿè®¡
    cache_stats = tool.get_cache_stats()
    print("æœ€ç»ˆç¼“å­˜ç»Ÿè®¡:")
    print(f"  ç»“æœçº§ç¼“å­˜: {cache_stats['result_hits']}/{cache_stats['result_hits'] + cache_stats['result_misses']} "
          f"({cache_stats['result_hit_rate']:.0%})")
    print(f"  å†…å­˜çº§ç¼“å­˜: {cache_stats['memory_hits']}/{cache_stats['memory_hits'] + cache_stats['memory_misses']} "
          f"({cache_stats['memory_hit_rate']:.0%})")
    print(f"  æ–‡ä»¶çº§ç¼“å­˜: {cache_stats['file_hits']}/{cache_stats['file_hits'] + cache_stats['file_misses']} "
          f"({cache_stats['file_hit_rate']:.0%})")
    print()
    
    # éªŒè¯
    print("éªŒè¯å¢é‡æ›´æ–°:")
    if time2 < time1 * 0.5:
        print("  ğŸ‰ å¢é‡æ›´æ–°å·¥ä½œæ­£å¸¸ï¼ï¼ˆæ¯”å…¨é‡æ‰«æå¿«50%ä»¥ä¸Šï¼‰")
    else:
        print("  âš ï¸  å¢é‡æ›´æ–°å¯èƒ½æœªç”Ÿæ•ˆ")
    
    if time3 < 0.01:
        print("  ğŸ‰ ç¼“å­˜å·¥ä½œæ­£å¸¸ï¼")
    else:
        print("  âš ï¸  ç¼“å­˜å¯èƒ½æœªç”Ÿæ•ˆ")


if __name__ == "__main__":
    asyncio.run(test_incremental_update())
