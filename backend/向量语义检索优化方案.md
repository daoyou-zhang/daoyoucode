# å‘é‡è¯­ä¹‰æ£€ç´¢ä¼˜åŒ–æ–¹æ¡ˆ

## é—®é¢˜åˆ†æ

ä½ æå‡ºäº†ä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼š

> **ç°åœ¨daoyoucodeï¼Œrepomapç”Ÿæˆ+tree-sitterï¼Œæœ‰æ²¡æœ‰ä¸€å±‚ç»“æœï¼Œä¾›åé¢å‘é‡åšæ£€ç´¢å‘¢ï¼Ÿ**

ç­”æ¡ˆæ˜¯ï¼š**ç›®å‰æ²¡æœ‰å……åˆ†åˆ©ç”¨ï¼è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„ä¼˜åŒ–æœºä¼šã€‚**

---

## å½“å‰æ¶æ„çš„é—®é¢˜

### é—®é¢˜1ï¼šé‡å¤è§£æ

```
codebase_index.pyï¼ˆå‘é‡æ£€ç´¢ï¼‰:
  â”œâ”€ æ‰«ææ–‡ä»¶ â†’ æŒ‰è¡Œ/def/classåˆ†å—
  â”œâ”€ ç®€å•çš„æ­£åˆ™åŒ¹é…ï¼šif line.startswith("def ") or line.startswith("class ")
  â””â”€ æ²¡æœ‰åˆ©ç”¨tree-sitterçš„ç²¾ç¡®è§£æ

repomap_tools.pyï¼ˆä»£ç åœ°å›¾ï¼‰:
  â”œâ”€ æ‰«ææ–‡ä»¶ â†’ tree-sitterç²¾ç¡®è§£æ
  â”œâ”€ æå–å‡½æ•°ã€ç±»ã€æ–¹æ³•çš„å®šä¹‰å’Œå¼•ç”¨
  â”œâ”€ æ„å»ºå¼•ç”¨å›¾
  â””â”€ PageRankæ’åº

âŒ ä¸¤è€…ç‹¬ç«‹å·¥ä½œï¼Œé‡å¤è§£æï¼
```

### é—®é¢˜2ï¼šåˆ†å—è´¨é‡å·®

```python
# codebase_index.py çš„åˆ†å—é€»è¾‘
def _chunk_file(content: str, path: Path, max_lines: int = 55):
    if ext == ".py":
        # âŒ ç®€å•çš„æ­£åˆ™åŒ¹é…
        for i, line in enumerate(lines):
            if line.strip().startswith("def ") or line.strip().startswith("class "):
                # åˆ†å—
```

**é—®é¢˜**ï¼š
- æ— æ³•è¯†åˆ«åµŒå¥—çš„ç±»å’Œæ–¹æ³•
- æ— æ³•è¯†åˆ«è£…é¥°å™¨
- æ— æ³•è¯†åˆ«ç±»å‹æ³¨è§£
- æ— æ³•è¯†åˆ«importè¯­å¥
- åˆ†å—è¾¹ç•Œä¸å‡†ç¡®

### é—®é¢˜3ï¼šç¼ºå°‘æ–‡ä»¶å…³è”

```python
# å½“å‰çš„chunkç»“æ„
{
    "path": "backend/agents/core/agent.py",
    "start": 100,
    "end": 150,
    "text": "def execute(self, ...):\n    ..."
}

# âŒ ç¼ºå°‘ï¼š
# - è¿™ä¸ªå‡½æ•°å±äºå“ªä¸ªç±»ï¼Ÿ
# - è¿™ä¸ªå‡½æ•°è°ƒç”¨äº†å“ªäº›å…¶ä»–å‡½æ•°ï¼Ÿ
# - è¿™ä¸ªå‡½æ•°è¢«å“ªäº›æ–‡ä»¶å¼•ç”¨ï¼Ÿ
# - è¿™ä¸ªå‡½æ•°çš„ä¾èµ–å…³ç³»ï¼Ÿ
```

---

## Cursorçš„åšæ³•ï¼ˆæ¨æµ‹ï¼‰

æ ¹æ®ä½ çš„æè¿°å’ŒCursorçš„è¡Œä¸ºï¼Œå®ƒå¯èƒ½æ˜¯è¿™æ ·åšçš„ï¼š

```
1. Tree-sitterè§£æ
   â”œâ”€ æå–æ‰€æœ‰å®šä¹‰ï¼ˆå‡½æ•°ã€ç±»ã€æ–¹æ³•ï¼‰
   â”œâ”€ æå–æ‰€æœ‰å¼•ç”¨ï¼ˆå‡½æ•°è°ƒç”¨ã€ç±»å®ä¾‹åŒ–ï¼‰
   â””â”€ æ„å»ºå¼•ç”¨å›¾

2. ä»£ç åˆ†å—ï¼ˆåŸºäºASTï¼‰
   â”œâ”€ æ¯ä¸ªå‡½æ•°/ç±»ä½œä¸ºä¸€ä¸ªchunk
   â”œâ”€ åŒ…å«ä¸Šä¸‹æ–‡ï¼ˆæ‰€å±ç±»ã€è£…é¥°å™¨ã€ç±»å‹æ³¨è§£ï¼‰
   â””â”€ åŒ…å«å¼•ç”¨ä¿¡æ¯ï¼ˆè°ƒç”¨äº†è°ã€è¢«è°è°ƒç”¨ï¼‰

3. å‘é‡åŒ–
   â”œâ”€ chunkæ–‡æœ¬ â†’ embedding
   â”œâ”€ æ–‡ä»¶è·¯å¾„ â†’ embedding
   â””â”€ å¼•ç”¨å…³ç³» â†’ å›¾ç»“æ„

4. æ£€ç´¢
   â”œâ”€ è¯­ä¹‰æ£€ç´¢ï¼šquery â†’ embedding â†’ ç›¸ä¼¼åº¦åŒ¹é…
   â”œâ”€ æ–‡ä»¶å…³è”ï¼šæ‰¾åˆ°ç›¸å…³chunkåï¼Œæ‰©å±•åˆ°ç›¸å…³æ–‡ä»¶
   â””â”€ å¼•ç”¨å…³è”ï¼šæ‰¾åˆ°å‡½æ•°Aåï¼Œè‡ªåŠ¨åŒ…å«è°ƒç”¨Açš„å‡½æ•°B
```

---

## å½“å‰å®ç°çŠ¶æ€ï¼ˆä¸ä»£ç å¯¹é½ï¼‰

| é˜¶æ®µ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| **é˜¶æ®µ1** å¤ç”¨ repomap è§£æ | âœ… å·²å®ç° | ä½¿ç”¨ **å…¬å¼€ API**ï¼š`RepoMapTool().get_definitions(repo_path)`ã€`get_reference_graph()`ã€`get_pagerank_scores()`ï¼›definitions å·²å« `end_line`ï¼ˆrepomap å†…éƒ¨ `_compute_end_lines`ï¼‰ã€‚ |
| **é˜¶æ®µ2** å¢å¼º chunk ç»“æ„ | âœ… å·²å®ç° | chunk å«ï¼špath, start, end, text, type, name, parent_class, scope, calls, called_by, imports, related_files, pagerank_scoreã€‚æœªå®ç°ï¼šdecoratorsã€imported_byï¼ˆå¯é€‰ï¼‰ã€‚ |
| **é˜¶æ®µ3** å¤šå±‚æ¬¡æ£€ç´¢ | âœ… å·²å®ç° | `CodebaseIndex.search_multilayer()`ï¼šè¯­ä¹‰ â†’ æ–‡ä»¶æ‰©å±• â†’ å¼•ç”¨æ‰©å±• â†’ å»é‡é‡æ’ï¼Œå« max_expansion ç­‰ä¸Šé™ã€‚ |
| **é˜¶æ®µ4** æ··åˆæ£€ç´¢ | âœ… å·²å®ç° | `CodebaseIndex.search_hybrid()`ï¼šè¯­ä¹‰ + BM25 + PageRank + contextï¼Œæ”¯æŒè‡ªé€‚åº”æƒé‡ï¼ˆ`_detect_query_type`ï¼‰ã€‚é»˜è®¤æƒé‡ä¸æ–‡æ¡£ç•¥ä¸åŒï¼ˆå¦‚ 0.4/0.3/0.2/0.1ï¼‰ã€‚ |
| **å·¥å…·å±‚å¯¹æ¥** | âš ï¸ å·²ä¼˜åŒ– | `semantic_code_search` å·¥å…·é€šè¿‡ `search_codebase(..., strategy="hybrid")` é»˜è®¤ä½¿ç”¨æ··åˆæ£€ç´¢ï¼Œå¯é…ç½®ä¸º `basic` / `multilayer` / `hybrid`ã€‚ |

---

## ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå¤ç”¨repomapçš„tree-sitterè§£æç»“æœ â­â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**ï¼šrepomapå·²ç»ç”¨tree-sitterç²¾ç¡®è§£æäº†ä»£ç ï¼Œä¸ºä»€ä¹ˆä¸å¤ç”¨è¿™ä¸ªç»“æœï¼Ÿ

**å®ç°è¦ç‚¹**ï¼šä½¿ç”¨ repomap çš„**å…¬å¼€ API**ï¼Œä¸è¦ç›´æ¥è°ƒç”¨ `_scan_repository` ç­‰ç§æœ‰æ–¹æ³•ï¼Œä¾¿äº repomap é‡æ„æ—¶å…¼å®¹ã€‚

```python
# å½“å‰å®ç°ï¼ˆcodebase_index.build_indexï¼‰ä½¿ç”¨å…¬å¼€ API
from ..tools.repomap_tools import RepoMapTool
repomap_tool = RepoMapTool()

# ğŸ”‘ å…³é”®ï¼šå…¬å¼€ APIï¼Œdefinitions å·²å« end_line
definitions = repomap_tool.get_definitions(str(self.repo_path))
reference_graph = repomap_tool.get_reference_graph(str(self.repo_path), definitions)
pagerank_scores = repomap_tool.get_pagerank_scores(str(self.repo_path), reference_graph=reference_graph, definitions=definitions)

# åŸºäº definitions æ„å»º chunksï¼ˆä»… kind=="def"ï¼‰
for file_path, defs in definitions.items():
    for d in defs:
        if d.get("kind") != "def":
            continue
        # ä½¿ç”¨ d["line"], d.get("end_line", ...) æå–ä»£ç å—...
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç²¾ç¡®çš„åˆ†å—è¾¹ç•Œï¼ˆåŸºäºASTï¼‰
- âœ… åŒ…å«å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- âœ… é¿å…é‡å¤è§£æ
- âœ… åˆ©ç”¨å·²æœ‰çš„ç¼“å­˜æœºåˆ¶



### æ–¹æ¡ˆ2ï¼šå¢å¼ºchunkç»“æ„ï¼Œæ·»åŠ æ–‡ä»¶å…³è” â­â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¯ä¸ªchunkä¸ä»…åŒ…å«ä»£ç æ–‡æœ¬ï¼Œè¿˜åŒ…å«ä¸°å¯Œçš„å…ƒæ•°æ®ã€‚

```python
# å¢å¼ºçš„chunkç»“æ„
{
    # åŸºç¡€ä¿¡æ¯
    "path": "backend/agents/core/agent.py",
    "start": 100,
    "end": 150,
    "text": "def execute(self, ...):\n    ...",
    
    # ğŸ†• ASTä¿¡æ¯
    "type": "method",  # function, class, method
    "name": "execute",
    "parent_class": "BaseAgent",  # æ‰€å±ç±»
    "decorators": ["@async"],  # è£…é¥°å™¨
    
    # ğŸ†• å¼•ç”¨å…³ç³»
    "calls": [  # è¿™ä¸ªå‡½æ•°è°ƒç”¨äº†è°
        "self._load_prompt",
        "self._call_llm_with_tools"
    ],
    "called_by": [  # è¿™ä¸ªå‡½æ•°è¢«è°è°ƒç”¨
        "orchestrator.execute",
        "skill_executor.run"
    ],
    
    # ğŸ†• ä¾èµ–å…³ç³»
    "imports": [  # è¿™ä¸ªæ–‡ä»¶å¯¼å…¥äº†ä»€ä¹ˆ
        "from ..llm import get_client_manager",
        "from ..tools import get_tool_registry"
    ],
    "imported_by": [  # è¿™ä¸ªæ–‡ä»¶è¢«è°å¯¼å…¥
        "backend/orchestrators/multi_agent.py",
        "backend/executor.py"
    ],
    
    # ğŸ†• æ–‡ä»¶å…³è”
    "related_files": [  # ç›¸å…³æ–‡ä»¶ï¼ˆåŸºäºå¼•ç”¨å…³ç³»ï¼‰
        "backend/agents/llm/client_manager.py",
        "backend/agents/tools/registry.py"
    ],
    
    # ğŸ†• PageRankåˆ†æ•°
    "pagerank_score": 0.85  # åœ¨å¼•ç”¨å›¾ä¸­çš„é‡è¦æ€§
}
```

**å¦‚ä½•æ„å»ºè¿™äº›ä¿¡æ¯**ï¼š

```python
def _build_chunk_from_definition(
    self,
    file_path: str,
    definition: Dict,
    all_definitions: Dict[str, List[Dict]],
    reference_graph: Dict[str, Dict[str, float]]
) -> Dict:
    """åŸºäºrepomapçš„definitionæ„å»ºå¢å¼ºçš„chunk"""
    
    # 1. è¯»å–ä»£ç æ–‡æœ¬
    code_text = self._extract_code_text(file_path, definition)
    
    # 2. æå–è°ƒç”¨å…³ç³»ï¼ˆä»ä»£ç æ–‡æœ¬ä¸­ï¼‰
    calls = self._extract_function_calls(code_text)
    
    # 3. ä»å¼•ç”¨å›¾ä¸­æ‰¾åˆ°è¢«è°è°ƒç”¨
    called_by = self._find_callers(file_path, definition["name"], reference_graph)
    
    # 4. æå–å¯¼å…¥å…³ç³»
    imports = self._extract_imports(file_path)
    
    # 5. æ‰¾åˆ°ç›¸å…³æ–‡ä»¶ï¼ˆåŸºäºå¼•ç”¨å›¾ï¼‰
    related_files = self._find_related_files(file_path, reference_graph)
    
    # 6. è·å–PageRankåˆ†æ•°
    pagerank_score = self._get_pagerank_score(file_path, reference_graph)
    
    return {
        "path": file_path,
        "start": definition["line"],
        "end": definition["line"] + len(code_text.splitlines()),
        "text": code_text,
        "type": definition["type"],
        "name": definition["name"],
        "calls": calls,
        "called_by": called_by,
        "imports": imports,
        "related_files": related_files,
        "pagerank_score": pagerank_score
    }
```

---

### æ–¹æ¡ˆ3ï¼šå¤šå±‚æ¬¡æ£€ç´¢ç­–ç•¥ â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸ä»…ä»…æ˜¯è¯­ä¹‰åŒ¹é…ï¼Œè¿˜è¦è€ƒè™‘æ–‡ä»¶å…³è”å’Œå¼•ç”¨å…³ç³»ã€‚

```python
def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
    """å¤šå±‚æ¬¡æ£€ç´¢"""
    
    # ç¬¬1å±‚ï¼šè¯­ä¹‰æ£€ç´¢ï¼ˆåŸºç¡€ï¼‰
    semantic_results = self._semantic_search(query, top_k * 2)
    
    # ç¬¬2å±‚ï¼šæ–‡ä»¶å…³è”æ‰©å±•
    expanded_results = []
    for chunk in semantic_results:
        expanded_results.append(chunk)
        
        # ğŸ”‘ å…³é”®ï¼šæ‰©å±•åˆ°ç›¸å…³æ–‡ä»¶
        for related_file in chunk.get("related_files", []):
            related_chunks = self._get_chunks_by_file(related_file)
            expanded_results.extend(related_chunks[:2])  # æ¯ä¸ªç›¸å…³æ–‡ä»¶å–å‰2ä¸ªchunk
    
    # ç¬¬3å±‚ï¼šå¼•ç”¨å…³ç³»æ‰©å±•
    final_results = []
    for chunk in expanded_results:
        final_results.append(chunk)
        
        # ğŸ”‘ å…³é”®ï¼šæ‰©å±•åˆ°è°ƒç”¨è€…å’Œè¢«è°ƒç”¨è€…
        for caller in chunk.get("called_by", []):
            caller_chunk = self._get_chunk_by_function(caller)
            if caller_chunk:
                final_results.append(caller_chunk)
        
        for callee in chunk.get("calls", []):
            callee_chunk = self._get_chunk_by_function(callee)
            if callee_chunk:
                final_results.append(callee_chunk)
    
    # ç¬¬4å±‚ï¼šå»é‡å’Œæ’åº
    unique_results = self._deduplicate(final_results)
    sorted_results = self._rerank(unique_results, query)
    
    return sorted_results[:top_k]
```

**æ£€ç´¢ç¤ºä¾‹**ï¼š

```
ç”¨æˆ·é—®ï¼š"å¦‚ä½•ä¿®å¤Agentæ‰§è¡Œæ—¶çš„è¶…æ—¶é”™è¯¯ï¼Ÿ"

ç¬¬1å±‚ï¼šè¯­ä¹‰æ£€ç´¢
  â†’ æ‰¾åˆ°ï¼šbackend/agents/core/agent.py::execute()
  â†’ ç›¸ä¼¼åº¦ï¼š0.85

ç¬¬2å±‚ï¼šæ–‡ä»¶å…³è”æ‰©å±•
  â†’ execute() å¯¼å…¥äº† timeout_handler
  â†’ æ‰©å±•åˆ°ï¼šbackend/agents/core/timeout_handler.py
  â†’ æ‰©å±•åˆ°ï¼šbackend/agents/core/recovery.py

ç¬¬3å±‚ï¼šå¼•ç”¨å…³ç³»æ‰©å±•
  â†’ execute() è°ƒç”¨äº† _call_llm_with_tools()
  â†’ æ‰©å±•åˆ°ï¼šbackend/agents/core/agent.py::_call_llm_with_tools()
  â†’ execute() è¢« orchestrator.execute() è°ƒç”¨
  â†’ æ‰©å±•åˆ°ï¼šbackend/orchestrators/multi_agent.py::execute()

ç¬¬4å±‚ï¼šå»é‡å’Œæ’åº
  â†’ æŒ‰ç›¸å…³æ€§é‡æ–°æ’åº
  â†’ è¿”å›top-10æœ€ç›¸å…³çš„ä»£ç å—
```

---

### æ–¹æ¡ˆ4ï¼šæ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯+PageRankï¼‰ â­â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**ï¼šç»“åˆå¤šç§ä¿¡å·ï¼Œæé«˜æ£€ç´¢å‡†ç¡®æ€§ã€‚

```python
def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
    """æ··åˆæ£€ç´¢"""
    
    # 1. è¯­ä¹‰æ£€ç´¢åˆ†æ•°
    semantic_scores = self._semantic_search_scores(query)
    
    # 2. å…³é”®è¯åŒ¹é…åˆ†æ•°
    keyword_scores = self._keyword_search_scores(query)
    
    # 3. PageRankåˆ†æ•°ï¼ˆä»£ç é‡è¦æ€§ï¼‰
    pagerank_scores = {
        chunk["id"]: chunk.get("pagerank_score", 0.0)
        for chunk in self.chunks
    }
    
    # 4. æ–‡ä»¶å…³è”åˆ†æ•°ï¼ˆå¦‚æœç”¨æˆ·æ‰“å¼€äº†æŸä¸ªæ–‡ä»¶ï¼‰
    file_affinity_scores = self._file_affinity_scores(
        query, 
        opened_files=self.context.get("initial_files", [])
    )
    
    # 5. æ··åˆæ‰“åˆ†
    final_scores = {}
    for chunk_id in semantic_scores:
        score = (
            0.5 * semantic_scores.get(chunk_id, 0.0) +      # è¯­ä¹‰æƒé‡50%
            0.2 * keyword_scores.get(chunk_id, 0.0) +       # å…³é”®è¯æƒé‡20%
            0.2 * pagerank_scores.get(chunk_id, 0.0) +      # PageRankæƒé‡20%
            0.1 * file_affinity_scores.get(chunk_id, 0.0)   # æ–‡ä»¶å…³è”æƒé‡10%
        )
        final_scores[chunk_id] = score
    
    # 6. æ’åºå¹¶è¿”å›
    sorted_chunks = sorted(
        self.chunks,
        key=lambda c: final_scores.get(c["id"], 0.0),
        reverse=True
    )
    
    return sorted_chunks[:top_k]
```

---

## å®æ–½è®¡åˆ’

### é˜¶æ®µ1ï¼šå¤ç”¨repomapè§£æç»“æœï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰â­â­â­â­â­ âœ… å·²å®ç°

**ç›®æ ‡**ï¼šé¿å…é‡å¤è§£æï¼Œæé«˜åˆ†å—è´¨é‡

**æ­¥éª¤**ï¼š
1. ä½¿ç”¨ `RepoMapTool` **å…¬å¼€ API**ï¼š`get_definitions()`ã€`get_reference_graph()`ã€`get_pagerank_scores()`ï¼ˆå‹¿ç›´æ¥è°ƒç”¨ `_scan_repository`ï¼‰
2. åŸºäº `definitions` æ„å»ºé«˜è´¨é‡çš„ chunksï¼ˆå« end_lineï¼‰
3. ä¿ç•™åŸæœ‰çš„å‘é‡åŒ–å’Œæ£€ç´¢é€»è¾‘ï¼›RepoMap å¤±è´¥æ—¶å›é€€åˆ°ä¼ ç»Ÿ `_chunk_file` æ‰«æ

**é¢„æœŸæ•ˆæœ**ï¼š
- åˆ†å—è´¨é‡æå‡ 50%
- è§£æé€Ÿåº¦æå‡ 2å€ï¼ˆå¤ç”¨ç¼“å­˜ï¼‰
- ä»£ç ç»´æŠ¤æ€§æå‡ï¼ˆç»Ÿä¸€è§£æé€»è¾‘ï¼‰

---

### é˜¶æ®µ2ï¼šå¢å¼ºchunkç»“æ„ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰â­â­â­â­ âœ… å·²å®ç°

**ç›®æ ‡**ï¼šæ·»åŠ æ–‡ä»¶å…³è”å’Œå¼•ç”¨å…³ç³»

**æ­¥éª¤**ï¼š
1. æ‰©å±• chunk ç»“æ„ï¼Œæ·»åŠ å…ƒæ•°æ®å­—æ®µï¼ˆtype, name, parent_class, scope, calls, called_by, imports, related_files, pagerank_scoreï¼‰
2. ä» `get_reference_graph()` è·å–å¼•ç”¨å…³ç³»
3. æå–å¯¼å…¥å…³ç³»ï¼ˆ`_extract_imports`ï¼‰ã€è°ƒç”¨å…³ç³»ï¼ˆ`_extract_calls`ã€`_find_callers`ï¼‰
4. PageRank ç”± `get_pagerank_scores()` æä¾›

**é¢„æœŸæ•ˆæœ**ï¼š
- æ£€ç´¢å‡†ç¡®æ€§æå‡ 30%
- æ”¯æŒ"æ‰¾åˆ°ç›¸å…³æ–‡ä»¶"åŠŸèƒ½
- æ”¯æŒ"æ‰¾åˆ°è°ƒç”¨é“¾"åŠŸèƒ½

---

### é˜¶æ®µ3ï¼šå¤šå±‚æ¬¡æ£€ç´¢ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰â­â­â­â­ âœ… å·²å®ç°

**ç›®æ ‡**ï¼šä¸ä»…è¿”å›åŒ¹é…çš„ä»£ç ï¼Œè¿˜è¿”å›ç›¸å…³çš„ä»£ç 

**æ­¥éª¤**ï¼š
1. å®ç°æ–‡ä»¶å…³è”æ‰©å±•ï¼ˆ`_expand_by_files`ï¼ŒåŸºäº related_filesï¼Œæ¯ç»“æœæœ€å¤šæ‰©å±•å‰ 3 ä¸ªæ–‡ä»¶ã€æ¯æ–‡ä»¶ top-2 chunksï¼‰
2. å®ç°å¼•ç”¨å…³ç³»æ‰©å±•ï¼ˆ`_expand_by_references`ï¼ŒåŸºäº calls/called_byï¼‰
3. å»é‡å’Œé‡æ’åºï¼ˆ`_deduplicate_and_rerank`ï¼‰ï¼›æ€»æ‰©å±•æ•°å— `max_expansion` é™åˆ¶ï¼ˆé»˜è®¤ 50ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- æ£€ç´¢å¬å›ç‡æå‡ 40%
- ç”¨æˆ·ä½“éªŒæå‡ï¼ˆä¸€æ¬¡æ£€ç´¢è·å¾—å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰

---

### é˜¶æ®µ4ï¼šæ··åˆæ£€ç´¢ï¼ˆä½ä¼˜å…ˆçº§ï¼‰â­â­â­ âœ… å·²å®ç°

**ç›®æ ‡**ï¼šç»“åˆå¤šç§ä¿¡å·ï¼Œæé«˜å‡†ç¡®æ€§

**æ­¥éª¤**ï¼š
1. è¯­ä¹‰åˆ†æ•°ï¼ˆå·²æœ‰ï¼‰+ BM25 å…³é”®è¯æ‰“åˆ†ï¼ˆ`_bm25_score`ï¼‰+ PageRank + ä¸Šä¸‹æ–‡åˆ†æ•°ï¼ˆ`_context_score`ï¼‰
2. æ”¯æŒè‡ªé€‚åº”æƒé‡ï¼ˆ`_detect_query_type`ã€`_get_adaptive_weights`ï¼‰ï¼›é»˜è®¤ 0.4/0.3/0.2/0.1
3. å€™é€‰å¯æ¥è‡ª `search_multilayer` æˆ–å•å±‚ `search`ï¼Œå†æŒ‰æ··åˆåˆ†æ•°æ’åº

**é¢„æœŸæ•ˆæœ**ï¼š
- æ£€ç´¢å‡†ç¡®æ€§æå‡ 20%
- æ›´å¥½åœ°å¤„ç†è¾¹ç¼˜æƒ…å†µ

### å·¥å…·å±‚å¯¹æ¥ âœ… å·²ä¼˜åŒ–

- **å…¥å£**ï¼š`semantic_code_search` å·¥å…· â†’ `search_codebase(repo_path, query, top_k, strategy="hybrid")`ã€‚
- **ç­–ç•¥**ï¼š`basic`ï¼ˆä»…è¯­ä¹‰/å…³é”®è¯ï¼‰ã€`multilayer`ï¼ˆå¤šå±‚æ‰©å±•ï¼‰ã€`hybrid`ï¼ˆé»˜è®¤ï¼Œæ··åˆæ£€ç´¢ï¼‰ã€‚æ¨èé»˜è®¤ `hybrid` ä»¥å……åˆ†åˆ©ç”¨ chunk å…ƒæ•°æ®ä¸ repomap èƒ½åŠ›ã€‚

---

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå¤ç”¨repomapè§£æç»“æœï¼ˆä¸å½“å‰å®ç°å¯¹é½ï¼Œä½¿ç”¨å…¬å¼€ APIï¼‰

```python
# backend/daoyoucode/agents/memory/codebase_index.py

# ä½¿ç”¨ RepoMapTool å…¬å¼€ APIï¼ˆå‹¿ç”¨ _scan_repository / _build_reference_graphï¼‰
from ..tools.repomap_tools import RepoMapTool
repomap_tool = RepoMapTool()

definitions = repomap_tool.get_definitions(str(self.repo_path))  # å·²å« end_line
reference_graph = repomap_tool.get_reference_graph(str(self.repo_path), definitions)
pagerank_scores = repomap_tool.get_pagerank_scores(
    str(self.repo_path), reference_graph=reference_graph, definitions=definitions
)

# åŸºäº definitions æ„å»º chunksï¼ˆä»… kind=="def"ï¼‰ï¼Œå¹¶å¡«å…… calls/called_by/related_files ç­‰
# è¯¦è§ CodebaseIndex.build_index å½“å‰å®ç°
```

---

### ç¤ºä¾‹2ï¼šå¢å¼ºchunkç»“æ„

```python
# backend/daoyoucode/agents/memory/codebase_index.py

def _build_enhanced_chunk(
    self,
    file_path: str,
    definition: Dict,
    all_definitions: Dict[str, List[Dict]],
    reference_graph: Dict[str, Dict[str, float]],
    pagerank_scores: Dict[str, float]
) -> Dict:
    """æ„å»ºå¢å¼ºçš„chunkï¼ˆåŒ…å«æ–‡ä»¶å…³è”ï¼‰"""
    
    # åŸºç¡€ä¿¡æ¯
    chunk = {
        "path": file_path,
        "start": definition["line"],
        "text": self._extract_code_text(...),
        "type": definition["type"],
        "name": definition["name"],
    }
    
    # ğŸ†• PageRankåˆ†æ•°
    chunk["pagerank_score"] = pagerank_scores.get(file_path, 0.0)
    
    # ğŸ†• å¼•ç”¨å…³ç³»
    chunk["calls"] = self._extract_calls(chunk["text"])
    chunk["called_by"] = self._find_callers(
        file_path, definition["name"], all_definitions
    )
    
    # ğŸ†• æ–‡ä»¶å…³è”
    chunk["related_files"] = list(reference_graph.get(file_path, {}).keys())
    
    return chunk

def _extract_calls(self, code_text: str) -> List[str]:
    """ä»ä»£ç ä¸­æå–å‡½æ•°è°ƒç”¨"""
    # ç®€å•çš„æ­£åˆ™åŒ¹é…ï¼ˆå¯ä»¥ç”¨tree-sitteræ›´ç²¾ç¡®ï¼‰
    import re
    pattern = r'(\w+)\s*\('
    calls = re.findall(pattern, code_text)
    return list(set(calls))

def _find_callers(
    self,
    file_path: str,
    function_name: str,
    all_definitions: Dict[str, List[Dict]]
) -> List[str]:
    """æ‰¾åˆ°è°ƒç”¨è¿™ä¸ªå‡½æ•°çš„å…¶ä»–å‡½æ•°"""
    callers = []
    for other_file, defs in all_definitions.items():
        for d in defs:
            if d.get("kind") == "ref" and d["name"] == function_name:
                callers.append(f"{other_file}::{d.get('parent', 'unknown')}")
    return callers
```

---

## æ€»ç»“

### å½“å‰é—®é¢˜ï¼ˆå·²é€šè¿‡å®ç°ç¼“è§£ï¼‰
1. ~~repomapå’Œcodebase_indexé‡å¤è§£æ~~ â†’ å·²å¤ç”¨ repomap å…¬å¼€ API
2. ~~åˆ†å—è´¨é‡å·®ï¼ˆç®€å•æ­£åˆ™ï¼‰~~ â†’ å·²åŸºäº definitions + end_line åˆ†å—
3. ~~ç¼ºå°‘æ–‡ä»¶å…³è”å’Œå¼•ç”¨å…³ç³»~~ â†’ chunk å·²å« calls/called_by/related_files
4. ~~åªæœ‰è¯­ä¹‰æ£€ç´¢~~ â†’ å·²æä¾› search_multilayerã€search_hybridï¼Œå·¥å…·å±‚é»˜è®¤èµ° hybrid

### ä¼˜åŒ–æ–¹æ¡ˆï¼ˆå‡å·²å®ç°ï¼‰
1. âœ… å¤ç”¨ repomap çš„ tree-sitter è§£æç»“æœï¼ˆå…¬å¼€ APIï¼šget_definitions / get_reference_graph / get_pagerank_scoresï¼‰
2. âœ… å¢å¼º chunk ç»“æ„ï¼Œæ·»åŠ å…ƒæ•°æ®
3. âœ… å¤šå±‚æ¬¡æ£€ç´¢ï¼ˆè¯­ä¹‰+æ–‡ä»¶æ‰©å±•+å¼•ç”¨æ‰©å±•+å»é‡é‡æ’ï¼‰
4. âœ… æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰+BM25+PageRank+contextï¼Œå¯è‡ªé€‚åº”æƒé‡ï¼‰

### å·¥å…·å±‚å¯¹æ¥
- `semantic_code_search` è°ƒç”¨ `search_codebase(..., strategy="hybrid")`ï¼Œé»˜è®¤ä½¿ç”¨æ··åˆæ£€ç´¢ï¼›å¯é…ç½® `basic` / `multilayer` / `hybrid`ã€‚

### å®æ–½ä¼˜å…ˆçº§ï¼ˆå½“å‰çŠ¶æ€ï¼‰
1. **é«˜ä¼˜å…ˆçº§**ï¼šå¤ç”¨ repomap è§£æç»“æœï¼ˆé˜¶æ®µ1ï¼‰âœ…
2. **ä¸­ä¼˜å…ˆçº§**ï¼šå¢å¼º chunk ç»“æ„ï¼ˆé˜¶æ®µ2ï¼‰âœ…
3. **ä¸­ä¼˜å…ˆçº§**ï¼šå¤šå±‚æ¬¡æ£€ç´¢ï¼ˆé˜¶æ®µ3ï¼‰âœ…
4. **ä½ä¼˜å…ˆçº§**ï¼šæ··åˆæ£€ç´¢ï¼ˆé˜¶æ®µ4ï¼‰âœ…

æ ¸å¿ƒæ€æƒ³ï¼š**å……åˆ†åˆ©ç”¨ repomap+tree-sitter çš„è§£æç»“æœï¼Œæ„å»ºé«˜è´¨é‡ä»£ç ç´¢å¼•ï¼Œæ”¯æŒå¤šå±‚æ¬¡ä¸æ··åˆæ£€ç´¢ï¼›å…¥å£ä½¿ç”¨å…¬å¼€ APIï¼Œå·¥å…·å±‚é»˜è®¤ hybridã€‚**


---

## æŠ€æœ¯ç»†èŠ‚è¡¥å……

### 1. RepoMapå¯¹å¤–APIè®¾è®¡ âœ… å·²å®ç°

**é—®é¢˜**ï¼šæ­¤å‰ repomap æ–¹æ³•å¤šä¸ºç§æœ‰ï¼ˆ`_scan_repository`, `_build_reference_graph`ï¼‰ï¼Œéœ€æ¸…æ™°å…¬å¼€ APIã€‚

**ç°çŠ¶**ï¼š`RepoMapTool` å·²æä¾› `get_definitions()`ã€`get_reference_graph()`ã€`get_pagerank_scores()`ï¼Œcodebase_index ä»…ä¾èµ–ä¸Šè¿° APIã€‚

**æ–¹æ¡ˆ**ï¼š

```python
# backend/daoyoucode/agents/tools/repomap_tools.py

class RepoMapTool(BaseTool):
    """RepoMapå·¥å…·ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå¤–éƒ¨è°ƒç”¨ï¼‰"""
    
    # ========== å…¬å¼€API ==========
    
    def get_definitions(
        self,
        repo_path: str,
        use_cache: bool = True
    ) -> Dict[str, List[Dict]]:
        """
        è·å–ä»£ç å®šä¹‰ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        Returns:
            {
                "backend/agents/core/agent.py": [
                    {
                        "type": "class",
                        "name": "BaseAgent",
                        "line": 50,
                        "end_line": 150,  # ğŸ†• æ·»åŠ ç»“æŸè¡Œ
                        "kind": "def"
                    },
                    ...
                ]
            }
        """
        repo_path_resolved = self.resolve_path(repo_path)
        
        if use_cache:
            self._init_cache(repo_path_resolved)
        
        definitions = self._scan_repository(repo_path_resolved)
        
        # ğŸ†• è®¡ç®—end_lineï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        definitions = self._compute_end_lines(definitions, repo_path_resolved)
        
        return definitions
    
    def get_reference_graph(
        self,
        repo_path: str,
        definitions: Optional[Dict[str, List[Dict]]] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        è·å–å¼•ç”¨å›¾ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            definitions: ä»£ç å®šä¹‰ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨è·å–ï¼‰
        
        Returns:
            {
                "file_a.py": {
                    "file_b.py": 3.0,  # file_aå¼•ç”¨file_b 3æ¬¡
                    "file_c.py": 1.0
                }
            }
        """
        repo_path_resolved = self.resolve_path(repo_path)
        
        if definitions is None:
            definitions = self.get_definitions(repo_path)
        
        return self._build_reference_graph(definitions, repo_path_resolved)
    
    def get_pagerank_scores(
        self,
        repo_path: str,
        reference_graph: Optional[Dict] = None,
        chat_files: Optional[List[str]] = None,
        mentioned_idents: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        è·å–PageRankåˆ†æ•°ï¼ˆå…¬å¼€APIï¼‰
        
        Args:
            repo_path: ä»“åº“è·¯å¾„
            reference_graph: å¼•ç”¨å›¾ï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šè‡ªåŠ¨è·å–ï¼‰
            chat_files: ç„¦ç‚¹æ–‡ä»¶
            mentioned_idents: æåˆ°çš„æ ‡è¯†ç¬¦
        
        Returns:
            {
                "file_a.py": 0.85,
                "file_b.py": 0.65,
                ...
            }
        """
        if reference_graph is None:
            definitions = self.get_definitions(repo_path)
            reference_graph = self.get_reference_graph(repo_path, definitions)
        
        ranked = self._pagerank(
            reference_graph,
            self._last_definitions,  # ä¿å­˜æœ€åä¸€æ¬¡çš„definitions
            chat_files or [],
            mentioned_idents or []
        )
        
        return dict(ranked)
    
    # ========== ç§æœ‰æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼‰==========
    
    def _compute_end_lines(
        self,
        definitions: Dict[str, List[Dict]],
        repo_path: Path
    ) -> Dict[str, List[Dict]]:
        """
        è®¡ç®—æ¯ä¸ªå®šä¹‰çš„ç»“æŸè¡Œ
        
        ç­–ç•¥ï¼š
        1. å¦‚æœå·²æœ‰end_lineï¼Œä¿æŒä¸å˜
        2. å¦åˆ™ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªå®šä¹‰çš„èµ·å§‹è¡Œä½œä¸ºç»“æŸè¡Œ
        3. å¦‚æœæ˜¯æœ€åä¸€ä¸ªå®šä¹‰ï¼Œä½¿ç”¨æ–‡ä»¶æœ«å°¾
        """
        for file_path, defs in definitions.items():
            # æŒ‰è¡Œå·æ’åº
            defs.sort(key=lambda d: d["line"])
            
            for i, d in enumerate(defs):
                if "end_line" in d:
                    continue
                
                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå®šä¹‰
                if i + 1 < len(defs):
                    d["end_line"] = defs[i + 1]["line"] - 1
                else:
                    # æœ€åä¸€ä¸ªå®šä¹‰ï¼Œè¯»å–æ–‡ä»¶è·å–æ€»è¡Œæ•°
                    try:
                        full_path = repo_path / file_path
                        with open(full_path, 'r', encoding='utf-8') as f:
                            total_lines = len(f.readlines())
                        d["end_line"] = total_lines
                    except Exception:
                        # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¼°è®¡50è¡Œ
                        d["end_line"] = d["line"] + 50
        
        return definitions
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¸…æ™°çš„å…¬å¼€API
- âœ… å‘åå…¼å®¹ï¼ˆç§æœ‰æ–¹æ³•ä¿æŒä¸å˜ï¼‰
- âœ… æ”¯æŒç¼“å­˜æ§åˆ¶
- âœ… è‡ªåŠ¨è®¡ç®—end_line

---

### 2. Definitionç»“æ„æ ‡å‡†åŒ–

**é—®é¢˜**ï¼šå½“å‰definitionç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘end_lineç­‰å…³é”®ä¿¡æ¯ã€‚

**æ ‡å‡†åŒ–ç»“æ„**ï¼š

```python
# Definitionæ ‡å‡†ç»“æ„
{
    # åŸºç¡€ä¿¡æ¯
    "type": str,        # "class" | "function" | "method" | "variable"
    "name": str,        # å®šä¹‰åç§°
    "line": int,        # èµ·å§‹è¡Œå·ï¼ˆ1-basedï¼‰
    "end_line": int,    # ğŸ†• ç»“æŸè¡Œå·ï¼ˆ1-basedï¼‰
    "kind": str,        # "def" | "ref"
    
    # ğŸ†• ä¸Šä¸‹æ–‡ä¿¡æ¯
    "parent": str,      # çˆ¶çº§ï¼ˆå¦‚ç±»åï¼‰ï¼Œå¯é€‰
    "scope": str,       # "global" | "class" | "function"
    
    # ğŸ†• ä»£ç æ–‡æœ¬ï¼ˆå¯é€‰ï¼ŒæŒ‰éœ€åŠ è½½ï¼‰
    "text": str,        # ä»£ç æ–‡æœ¬ï¼ˆæ‡’åŠ è½½ï¼‰
    
    # ğŸ†• å…ƒæ•°æ®
    "file_path": str,   # æ‰€å±æ–‡ä»¶ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
    "language": str,    # ç¼–ç¨‹è¯­è¨€
}
```

**å®ç°**ï¼š

```python
def _parse_file(self, file_path: Path) -> List[Dict]:
    """è§£ææ–‡ä»¶ï¼Œæå–å®šä¹‰å’Œå¼•ç”¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    # ... åŸæœ‰çš„tree-sitterè§£æé€»è¾‘ ...
    
    definitions = []
    parent_stack = []  # ğŸ†• è·Ÿè¸ªçˆ¶çº§
    
    for pattern_index, captures_dict in matches:
        for tag, nodes in captures_dict.items():
            for node in nodes:
                # åˆ¤æ–­ç±»å‹
                if tag.startswith("name.definition."):
                    kind = "def"
                    type_name = tag.split(".")[-1]
                    
                    # ğŸ†• è®¡ç®—end_line
                    end_line = node.end_point[0] + 1
                    
                    # ğŸ†• ç¡®å®šçˆ¶çº§
                    parent = parent_stack[-1] if parent_stack else None
                    
                    # ğŸ†• ç¡®å®šä½œç”¨åŸŸ
                    if type_name == "class":
                        scope = "global"
                        parent_stack.append(node.text.decode("utf-8"))
                    elif type_name in ("function", "method"):
                        scope = "class" if parent else "global"
                    else:
                        scope = "global"
                    
                    definitions.append({
                        "type": type_name,
                        "name": node.text.decode("utf-8"),
                        "line": node.start_point[0] + 1,
                        "end_line": end_line,  # ğŸ†•
                        "kind": kind,
                        "parent": parent,      # ğŸ†•
                        "scope": scope,        # ğŸ†•
                        "file_path": str(file_path.relative_to(repo_path)),  # ğŸ†•
                        "language": lang       # ğŸ†•
                    })
                
                elif tag.startswith("name.reference."):
                    # ... å¼•ç”¨å¤„ç† ...
    
    return definitions
```

---

### 3. ç¼“å­˜ä¸€è‡´æ€§ä¿è¯

**é—®é¢˜**ï¼šrepomapå’Œcodebase_indexå„è‡ªç»´æŠ¤ç¼“å­˜ï¼Œå¯èƒ½ä¸ä¸€è‡´ã€‚

**æ–¹æ¡ˆï¼šç»Ÿä¸€ç¼“å­˜ç®¡ç†**

```python
# backend/daoyoucode/agents/tools/cache_manager.py

class CodebaseCacheManager:
    """ä»£ç åº“ç¼“å­˜ç®¡ç†å™¨ï¼ˆç»Ÿä¸€ç®¡ç†repomapå’Œå‘é‡ç´¢å¼•çš„ç¼“å­˜ï¼‰"""
    
    def __init__(self, repo_path: Path):
        self.repo_path = repo_path
        self.cache_dir = repo_path / ".daoyoucode" / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿä¸€çš„ç¼“å­˜æ•°æ®åº“
        self.db = sqlite3.connect(str(self.cache_dir / "codebase.db"))
        self._init_db()
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS file_cache (
                file_path TEXT PRIMARY KEY,
                mtime REAL,
                definitions TEXT,
                updated_at REAL
            )
        """)
        
        self.db.execute("""
            CREATE TABLE IF NOT EXISTS index_meta (
                key TEXT PRIMARY KEY,
                value TEXT,
                updated_at REAL
            )
        """)
        
        self.db.commit()
    
    def get_definitions(
        self,
        file_path: str,
        mtime: float
    ) -> Optional[List[Dict]]:
        """è·å–æ–‡ä»¶çš„definitionsï¼ˆæ£€æŸ¥mtimeï¼‰"""
        cursor = self.db.execute(
            "SELECT mtime, definitions FROM file_cache WHERE file_path = ?",
            (file_path,)
        )
        row = cursor.fetchone()
        
        if row is None:
            return None
        
        cached_mtime, cached_defs = row
        
        # ğŸ”‘ å…³é”®ï¼šæ£€æŸ¥mtimeä¸€è‡´æ€§
        if abs(cached_mtime - mtime) > 0.001:  # å…è®¸1msè¯¯å·®
            return None
        
        return json.loads(cached_defs)
    
    def set_definitions(
        self,
        file_path: str,
        mtime: float,
        definitions: List[Dict]
    ):
        """ä¿å­˜æ–‡ä»¶çš„definitions"""
        import time
        self.db.execute(
            """
            INSERT OR REPLACE INTO file_cache 
            (file_path, mtime, definitions, updated_at) 
            VALUES (?, ?, ?, ?)
            """,
            (file_path, mtime, json.dumps(definitions), time.time())
        )
        self.db.commit()
    
    def invalidate_file(self, file_path: str):
        """ä½¿æ–‡ä»¶ç¼“å­˜å¤±æ•ˆ"""
        self.db.execute(
            "DELETE FROM file_cache WHERE file_path = ?",
            (file_path,)
        )
        self.db.commit()
    
    def get_index_version(self) -> str:
        """è·å–ç´¢å¼•ç‰ˆæœ¬ï¼ˆç”¨äºæ£€æµ‹æ˜¯å¦éœ€è¦é‡å»ºï¼‰"""
        cursor = self.db.execute(
            "SELECT value FROM index_meta WHERE key = 'version'"
        )
        row = cursor.fetchone()
        return row[0] if row else "0"
    
    def set_index_version(self, version: str):
        """è®¾ç½®ç´¢å¼•ç‰ˆæœ¬"""
        import time
        self.db.execute(
            """
            INSERT OR REPLACE INTO index_meta 
            (key, value, updated_at) 
            VALUES ('version', ?, ?)
            """,
            (version, time.time())
        )
        self.db.commit()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        cursor = self.db.execute(
            "SELECT COUNT(*) FROM file_cache"
        )
        file_count = cursor.fetchone()[0]
        
        cursor = self.db.execute(
            "SELECT SUM(LENGTH(definitions)) FROM file_cache"
        )
        total_size = cursor.fetchone()[0] or 0
        
        return {
            "file_count": file_count,
            "total_size": total_size,
            "cache_dir": str(self.cache_dir)
        }


# å…¨å±€å•ä¾‹
_cache_managers: Dict[str, CodebaseCacheManager] = {}

def get_cache_manager(repo_path: Path) -> CodebaseCacheManager:
    """è·å–ç¼“å­˜ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰"""
    key = str(repo_path.resolve())
    if key not in _cache_managers:
        _cache_managers[key] = CodebaseCacheManager(repo_path)
    return _cache_managers[key]
```

**ä½¿ç”¨æ–¹å¼**ï¼š

```python
# repomap_tools.py
def _get_cached_definitions(self, file_path: str, mtime: float):
    cache_manager = get_cache_manager(self.repo_path)
    return cache_manager.get_definitions(file_path, mtime)

def _cache_definitions(self, file_path: str, mtime: float, definitions: List[Dict]):
    cache_manager = get_cache_manager(self.repo_path)
    cache_manager.set_definitions(file_path, mtime, definitions)

# codebase_index.py
def build_index(self):
    cache_manager = get_cache_manager(self.repo_path)
    
    # æ£€æŸ¥ç´¢å¼•ç‰ˆæœ¬
    current_version = cache_manager.get_index_version()
    if current_version == self.INDEX_VERSION and not force:
        # åŠ è½½å·²æœ‰ç´¢å¼•
        return self._load_index()
    
    # é‡å»ºç´¢å¼•
    # ...
    
    # æ›´æ–°ç‰ˆæœ¬
    cache_manager.set_index_version(self.INDEX_VERSION)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†
- âœ… mtimeä¸€è‡´æ€§æ£€æŸ¥
- âœ… ç‰ˆæœ¬æ§åˆ¶
- âœ… é¿å…ç¼“å­˜ä¸ä¸€è‡´

---

### 4. Chunkè¾¹ç•Œç²¾ç¡®æ§åˆ¶

**é—®é¢˜**ï¼šå¦‚ä½•ç¡®ä¿chunkè¾¹ç•Œå‡†ç¡®ï¼Œä¸ä¼šæˆªæ–­ä»£ç ï¼Ÿ

**æ–¹æ¡ˆï¼šåŸºäºASTçš„ç²¾ç¡®è¾¹ç•Œ**

```python
def _extract_code_chunk(
    self,
    file_path: Path,
    definition: Dict
) -> str:
    """
    æå–ä»£ç å—ï¼ˆç²¾ç¡®è¾¹ç•Œï¼‰
    
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨definitionçš„lineå’Œend_line
    2. åŒ…å«è£…é¥°å™¨ï¼ˆå‘ä¸Šæ‰©å±•ï¼‰
    3. åŒ…å«æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆå‘ä¸‹æ‰©å±•ï¼‰
    4. ä¿æŒç¼©è¿›ä¸€è‡´æ€§
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        start_line = definition["line"] - 1  # è½¬ä¸º0-based
        end_line = definition.get("end_line", start_line + 50) - 1
        
        # ğŸ”‘ å‘ä¸Šæ‰©å±•ï¼šåŒ…å«è£…é¥°å™¨å’Œæ³¨é‡Š
        while start_line > 0:
            prev_line = lines[start_line - 1].strip()
            if prev_line.startswith('@') or prev_line.startswith('#'):
                start_line -= 1
            else:
                break
        
        # ğŸ”‘ å‘ä¸‹æ‰©å±•ï¼šç¡®ä¿å®Œæ•´æ€§
        # æ£€æŸ¥ç¼©è¿›ï¼Œç¡®ä¿ä¸ä¼šæˆªæ–­åµŒå¥—ç»“æ„
        base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())
        
        while end_line < len(lines) - 1:
            next_line = lines[end_line + 1]
            if not next_line.strip():
                # ç©ºè¡Œï¼Œç»§ç»­
                end_line += 1
                continue
            
            next_indent = len(next_line) - len(next_line.lstrip())
            if next_indent > base_indent:
                # ä»åœ¨å½“å‰å—å†…ï¼Œç»§ç»­
                end_line += 1
            else:
                # å·²ç»åˆ°ä¸‹ä¸€ä¸ªå—ï¼Œåœæ­¢
                break
        
        # æå–ä»£ç 
        code_lines = lines[start_line:end_line + 1]
        code_text = ''.join(code_lines)
        
        return code_text
    
    except Exception as e:
        logger.warning(f"æå–ä»£ç å—å¤±è´¥: {e}")
        return ""
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç²¾ç¡®çš„è¾¹ç•Œ
- âœ… åŒ…å«è£…é¥°å™¨å’Œæ–‡æ¡£
- âœ… ä¿æŒä»£ç å®Œæ•´æ€§
- âœ… å¤„ç†åµŒå¥—ç»“æ„

---

### 5. å¤šå±‚æ£€ç´¢çš„è§„æ¨¡æ§åˆ¶

**é—®é¢˜**ï¼šå¤šå±‚æ£€ç´¢å¯èƒ½å¯¼è‡´ç»“æœçˆ†ç‚¸ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰ã€‚

**æ–¹æ¡ˆï¼šåˆ†å±‚é™åˆ¶ + å»é‡**

```python
def search_multilayer(
    self,
    query: str,
    top_k: int = 10,
    max_expansion: int = 50  # ğŸ”‘ æœ€å¤§æ‰©å±•æ•°é‡
) -> List[Dict[str, Any]]:
    """
    å¤šå±‚æ£€ç´¢ï¼ˆè§„æ¨¡æ§åˆ¶ï¼‰
    
    ç­–ç•¥ï¼š
    1. ç¬¬1å±‚ï¼šè¯­ä¹‰æ£€ç´¢ top_k*2
    2. ç¬¬2å±‚ï¼šæ–‡ä»¶å…³è”æ‰©å±•ï¼Œæ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªchunk
    3. ç¬¬3å±‚ï¼šå¼•ç”¨å…³ç³»æ‰©å±•ï¼Œæ¯ä¸ªå‡½æ•°æœ€å¤š1ä¸ªè°ƒç”¨è€…/è¢«è°ƒç”¨è€…
    4. å»é‡ + é‡æ’åº â†’ top_k
    """
    results = []
    seen_ids = set()
    
    # ç¬¬1å±‚ï¼šè¯­ä¹‰æ£€ç´¢
    semantic_results = self._semantic_search(query, top_k * 2)
    for chunk in semantic_results:
        chunk_id = f"{chunk['path']}:{chunk['start']}"
        if chunk_id not in seen_ids:
            results.append(chunk)
            seen_ids.add(chunk_id)
    
    # ğŸ”‘ è§„æ¨¡æ§åˆ¶ï¼šé™åˆ¶æ‰©å±•æ•°é‡
    if len(results) >= max_expansion:
        return self._rerank(results, query)[:top_k]
    
    # ç¬¬2å±‚ï¼šæ–‡ä»¶å…³è”æ‰©å±•
    for chunk in semantic_results[:5]:  # åªæ‰©å±•å‰5ä¸ª
        related_files = chunk.get("related_files", [])
        for related_file in related_files[:3]:  # æ¯ä¸ªchunkæœ€å¤š3ä¸ªç›¸å…³æ–‡ä»¶
            related_chunks = self._get_chunks_by_file(related_file, limit=2)
            for rc in related_chunks:
                chunk_id = f"{rc['path']}:{rc['start']}"
                if chunk_id not in seen_ids and len(results) < max_expansion:
                    results.append(rc)
                    seen_ids.add(chunk_id)
    
    # ğŸ”‘ è§„æ¨¡æ§åˆ¶ï¼šå†æ¬¡æ£€æŸ¥
    if len(results) >= max_expansion:
        return self._rerank(results, query)[:top_k]
    
    # ç¬¬3å±‚ï¼šå¼•ç”¨å…³ç³»æ‰©å±•
    for chunk in semantic_results[:3]:  # åªæ‰©å±•å‰3ä¸ª
        # è°ƒç”¨è€…ï¼ˆæœ€å¤š1ä¸ªï¼‰
        called_by = chunk.get("called_by", [])
        if called_by and len(results) < max_expansion:
            caller_chunk = self._get_chunk_by_function(called_by[0])
            if caller_chunk:
                chunk_id = f"{caller_chunk['path']}:{caller_chunk['start']}"
                if chunk_id not in seen_ids:
                    results.append(caller_chunk)
                    seen_ids.add(chunk_id)
        
        # è¢«è°ƒç”¨è€…ï¼ˆæœ€å¤š1ä¸ªï¼‰
        calls = chunk.get("calls", [])
        if calls and len(results) < max_expansion:
            callee_chunk = self._get_chunk_by_function(calls[0])
            if callee_chunk:
                chunk_id = f"{callee_chunk['path']}:{callee_chunk['start']}"
                if chunk_id not in seen_ids:
                    results.append(callee_chunk)
                    seen_ids.add(chunk_id)
    
    # ç¬¬4å±‚ï¼šå»é‡ + é‡æ’åº
    final_results = self._rerank(results, query)
    
    logger.info(
        f"å¤šå±‚æ£€ç´¢: è¯­ä¹‰={len(semantic_results)}, "
        f"æ‰©å±•å={len(results)}, æœ€ç»ˆ={len(final_results[:top_k])}"
    )
    
    return final_results[:top_k]
```

**è§„æ¨¡æ§åˆ¶ç­–ç•¥**ï¼š

| å±‚çº§ | è¾“å…¥ | æ‰©å±•ç­–ç•¥ | è¾“å‡ºä¸Šé™ |
|------|------|---------|---------|
| ç¬¬1å±‚ | query | è¯­ä¹‰æ£€ç´¢ | top_k*2 (20) |
| ç¬¬2å±‚ | å‰5ä¸ª | æ¯ä¸ªâ†’3ä¸ªç›¸å…³æ–‡ä»¶â†’2ä¸ªchunk | +30 |
| ç¬¬3å±‚ | å‰3ä¸ª | æ¯ä¸ªâ†’1ä¸ªè°ƒç”¨è€…+1ä¸ªè¢«è°ƒç”¨è€… | +6 |
| æ€»è®¡ | - | - | 56 â†’ å»é‡ â†’ top_k (10) |

**ä¼˜ç‚¹**ï¼š
- âœ… å¯æ§çš„è§„æ¨¡å¢é•¿
- âœ… é¿å…ç»“æœçˆ†ç‚¸
- âœ… ä¿è¯æ€§èƒ½
- âœ… å¯è°ƒèŠ‚çš„å‚æ•°

---

## é˜¶æ®µ1å®æ–½è®¡åˆ’ï¼ˆè¯¦ç»†ï¼‰

### æ­¥éª¤1ï¼šæŠ½è±¡RepoMapå…¬å¼€APIï¼ˆ1-2å°æ—¶ï¼‰

**æ–‡ä»¶**ï¼š`backend/daoyoucode/agents/tools/repomap_tools.py`

**ä»»åŠ¡**ï¼š
1. æ·»åŠ  `get_definitions()` å…¬å¼€æ–¹æ³•
2. æ·»åŠ  `get_reference_graph()` å…¬å¼€æ–¹æ³•
3. æ·»åŠ  `get_pagerank_scores()` å…¬å¼€æ–¹æ³•
4. å®ç° `_compute_end_lines()` æ–¹æ³•
5. å¢å¼º `_parse_file()` æ·»åŠ parentã€scopeç­‰å­—æ®µ

**æµ‹è¯•**ï¼š
```python
# test_repomap_api.py
repomap = RepoMapTool()
definitions = repomap.get_definitions(".")
assert "backend/agents/core/agent.py" in definitions
assert definitions["backend/agents/core/agent.py"][0]["end_line"] > 0
```

---

### æ­¥éª¤2ï¼šä¿®æ”¹CodebaseIndexå¤ç”¨RepoMapï¼ˆ2-3å°æ—¶ï¼‰

**æ–‡ä»¶**ï¼š`backend/daoyoucode/agents/memory/codebase_index.py`

**ä»»åŠ¡**ï¼š
1. ä¿®æ”¹ `build_index()` è°ƒç”¨ `repomap.get_definitions()`
2. ä¿®æ”¹ `_chunk_file()` æ”¹ä¸º `_build_chunk_from_definition()`
3. æ·»åŠ  `_extract_code_chunk()` æ–¹æ³•ï¼ˆç²¾ç¡®è¾¹ç•Œï¼‰
4. å¢å¼ºchunkç»“æ„ï¼Œæ·»åŠ typeã€nameã€pagerank_scoreç­‰å­—æ®µ

**æµ‹è¯•**ï¼š
```python
# test_codebase_index.py
index = CodebaseIndex.get_index(Path("."))
index.build_index(force=True)
assert len(index.chunks) > 0
assert index.chunks[0]["type"] in ["class", "function", "method"]
assert "end_line" in index.chunks[0]
```

---

### æ­¥éª¤3ï¼šç»Ÿä¸€ç¼“å­˜ç®¡ç†ï¼ˆ1-2å°æ—¶ï¼‰

**æ–‡ä»¶**ï¼š`backend/daoyoucode/agents/tools/cache_manager.py`ï¼ˆæ–°å»ºï¼‰

**ä»»åŠ¡**ï¼š
1. åˆ›å»º `CodebaseCacheManager` ç±»
2. å®ç°ç»Ÿä¸€çš„ç¼“å­˜æ•°æ®åº“
3. ä¿®æ”¹repomapä½¿ç”¨ç»Ÿä¸€ç¼“å­˜
4. ä¿®æ”¹codebase_indexä½¿ç”¨ç»Ÿä¸€ç¼“å­˜

**æµ‹è¯•**ï¼š
```python
# test_cache_manager.py
cache = get_cache_manager(Path("."))
cache.set_definitions("test.py", 123.456, [{"name": "test"}])
defs = cache.get_definitions("test.py", 123.456)
assert defs == [{"name": "test"}]
```

---

### æ­¥éª¤4ï¼šé›†æˆæµ‹è¯•ï¼ˆ1å°æ—¶ï¼‰

**ä»»åŠ¡**ï¼š
1. æµ‹è¯•repomap â†’ codebase_indexçš„å®Œæ•´æµç¨‹
2. æµ‹è¯•ç¼“å­˜ä¸€è‡´æ€§
3. æµ‹è¯•æ€§èƒ½æå‡
4. æµ‹è¯•chunkè´¨é‡

**æµ‹è¯•è„šæœ¬**ï¼š
```python
# test_integration.py
import time

# æµ‹è¯•1ï¼šæ€§èƒ½å¯¹æ¯”
start = time.time()
index_old = build_index_old()  # æ—§æ–¹æ³•
time_old = time.time() - start

start = time.time()
index_new = build_index_new()  # æ–°æ–¹æ³•
time_new = time.time() - start

print(f"æ€§èƒ½æå‡: {(time_old - time_new) / time_old * 100:.1f}%")

# æµ‹è¯•2ï¼šchunkè´¨é‡å¯¹æ¯”
print(f"æ—§æ–¹æ³•chunkæ•°: {len(index_old.chunks)}")
print(f"æ–°æ–¹æ³•chunkæ•°: {len(index_new.chunks)}")
print(f"æ–°æ–¹æ³•åŒ…å«å…ƒæ•°æ®: {index_new.chunks[0].keys()}")
```

---

## é£é™©æ§åˆ¶

### é£é™©1ï¼šAPIå˜æ›´å½±å“ç°æœ‰åŠŸèƒ½

**ç¼“è§£æªæ–½**ï¼š
- ä¿æŒç§æœ‰æ–¹æ³•ä¸å˜
- æ–°å¢å…¬å¼€æ–¹æ³•ï¼Œä¸ä¿®æ”¹ç°æœ‰æ–¹æ³•
- å……åˆ†çš„å•å…ƒæµ‹è¯•

### é£é™©2ï¼šç¼“å­˜ä¸ä¸€è‡´

**ç¼“è§£æªæ–½**ï¼š
- ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
- mtimeä¸¥æ ¼æ£€æŸ¥
- ç‰ˆæœ¬æ§åˆ¶æœºåˆ¶

### é£é™©3ï¼šæ€§èƒ½ä¸‹é™

**ç¼“è§£æªæ–½**ï¼š
- å¤ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è§£æ
- æ‡’åŠ è½½ï¼ˆæŒ‰éœ€åŠ è½½ä»£ç æ–‡æœ¬ï¼‰
- è§„æ¨¡æ§åˆ¶ï¼ˆé™åˆ¶æ‰©å±•æ•°é‡ï¼‰

### é£é™©4ï¼šchunkè¾¹ç•Œä¸å‡†ç¡®

**ç¼“è§£æªæ–½**ï¼š
- åŸºäºASTçš„ç²¾ç¡®è¾¹ç•Œ
- åŒ…å«è£…é¥°å™¨å’Œæ–‡æ¡£
- å……åˆ†çš„æµ‹è¯•ç”¨ä¾‹

---

## æ€»ç»“

è¡¥å……äº†5ä¸ªå…³é”®æŠ€æœ¯ç»†èŠ‚ï¼š
1. âœ… RepoMapå¯¹å¤–APIè®¾è®¡
2. âœ… Definitionç»“æ„æ ‡å‡†åŒ–
3. âœ… ç¼“å­˜ä¸€è‡´æ€§ä¿è¯
4. âœ… Chunkè¾¹ç•Œç²¾ç¡®æ§åˆ¶
5. âœ… å¤šå±‚æ£€ç´¢çš„è§„æ¨¡æ§åˆ¶

é˜¶æ®µ1å®æ–½è®¡åˆ’ï¼š
1. æŠ½è±¡RepoMapå…¬å¼€APIï¼ˆ1-2å°æ—¶ï¼‰
2. ä¿®æ”¹CodebaseIndexå¤ç”¨RepoMapï¼ˆ2-3å°æ—¶ï¼‰
3. ç»Ÿä¸€ç¼“å­˜ç®¡ç†ï¼ˆ1-2å°æ—¶ï¼‰
4. é›†æˆæµ‹è¯•ï¼ˆ1å°æ—¶ï¼‰

æ€»è®¡ï¼š5-8å°æ—¶ï¼Œé£é™©å¯æ§ï¼Œæ”¶ç›Šæ˜æ˜¾ã€‚

**å‡†å¤‡å¥½å¼€å§‹å®æ–½äº†å—ï¼Ÿ**
