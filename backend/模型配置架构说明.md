# 模型配置架构说明

## 核心原则

**模型配置应该在Skill配置文件中，而非Agent代码中硬编码。**

这样设计的优势：
- 更灵活：同一个Agent可以在不同Skill中使用不同模型
- 更易配置：修改模型只需改配置文件，无需改代码
- 更符合设计：Skill是配置层，Agent是执行层

## 架构流程

```
Skill配置文件 (skill.yaml)
    ↓
    llm.model: qwen-coder-plus
    ↓
Orchestrator (simple.py)
    ↓
    llm_config=skill.llm
    ↓
Agent.execute()
    ↓
    model = llm_config.get('model', self.config.model)
    ↓
LLM Client
```

## 配置优先级

1. **Skill配置** (`skills/*/skill.yaml` 中的 `llm.model`) - 最高优先级
2. **Agent默认** (`Agent.__init__()` 中的 `model`) - 作为fallback

## 示例

### Skill配置文件 (skills/testing/skill.yaml)

```yaml
name: testing
version: 1.0.0
description: 测试编写和修复专家

orchestrator: simple
agent: test_expert

llm:
  model: qwen-coder-plus  # ✅ 在这里配置模型
  temperature: 0.3
  max_tokens: 4000
```

### Agent代码 (agents/builtin/test_expert.py)

```python
class TestExpertAgent(BaseAgent):
    def __init__(self):
        config = AgentConfig(
            name="test_expert",
            description="测试编写和修复专家",
            model="qwen-coder-plus",  # ✅ 作为默认值（fallback）
            temperature=0.3,
            system_prompt=""
        )
        super().__init__(config)
```

## 代码实现

### 1. Orchestrator传递配置

`backend/daoyoucode/agents/orchestrators/simple.py` (line 129):

```python
result = await agent.execute(
    prompt_source=prompt_source,
    user_input=user_input,
    context=context,
    llm_config=skill.llm,  # ✅ 传递Skill的LLM配置
    tools=skill.tools if skill.tools else None
)
```

### 2. Agent使用配置

`backend/daoyoucode/agents/core/agent.py` (line 883):

```python
# 合并配置（Skill配置优先）
model = (llm_config or {}).get('model', self.config.model)
temperature = (llm_config or {}).get('temperature', self.config.temperature)
```

## 最佳实践

### ✅ 推荐做法

1. **在Skill配置文件中指定模型**
   ```yaml
   llm:
     model: qwen-coder-plus
     temperature: 0.3
   ```

2. **Agent代码中设置合理的默认值**
   ```python
   model="qwen-plus",  # 通用默认模型
   ```

3. **使用已配置的模型**
   - 检查 `backend/config/llm_config.yaml` 确保模型已配置
   - 常用模型：`qwen-plus`, `qwen-max`, `qwen-coder-plus`

### ❌ 避免的做法

1. **不要在Agent代码中硬编码特定模型**
   ```python
   # ❌ 不好
   model="deepseek-coder",  # 如果未配置会报错
   ```

2. **不要使用未配置的模型**
   - 先在 `llm_config.yaml` 中配置
   - 再在 `skill.yaml` 中使用

## 配置模型

### 查看已配置的模型

查看 `backend/config/llm_config.yaml`:

```yaml
providers:
  qwen:
    api_key: ${DASHSCOPE_API_KEY}
    models:
      - qwen-plus
      - qwen-max
      - qwen-coder-plus
```

### 添加新模型

1. 在 `llm_config.yaml` 中添加模型配置
2. 在 `skill.yaml` 中使用新模型

## 常见问题

### Q: 为什么会报"未配置提供商: deepseek"？

A: 因为Skill配置文件中使用了 `deepseek-coder`，但 `llm_config.yaml` 中没有配置deepseek提供商。

解决方案：
1. 修改Skill配置使用已配置的模型（如 `qwen-coder-plus`）
2. 或者在 `llm_config.yaml` 中添加deepseek配置

### Q: Agent代码中的model参数还有用吗？

A: 有用！它作为fallback默认值。如果Skill配置文件中没有指定模型，就会使用Agent的默认模型。

### Q: 同一个Agent可以在不同Skill中使用不同模型吗？

A: 可以！这正是这个架构的优势。例如：
- `skills/testing/skill.yaml` 使用 `qwen-coder-plus`
- `skills/advanced-testing/skill.yaml` 使用 `qwen-max`
- 都使用同一个 `test_expert` Agent

## 总结

模型配置的正确位置是 **Skill配置文件**，Agent代码中的模型只是默认值。这样设计让系统更灵活、更易配置。
